"[{\"Sorting algorithm\": \"In computer science, a sorting algorithm is an algorithm that puts elements of a list into an order. The most frequently used orders are numerical order and lexicographical order, and either ascending or descending. Efficient sorting is important for optimizing the efficiency of other algorithms (such as search and merge algorithms) that require input data to be in sorted lists. Sorting is also often useful for canonicalizing data and for producing human-readable output.\\nFormally, the output of any sorting algorithm must satisfy two conditions:\\n\\nThe output is in monotonic order (each element is no smaller/larger than the previous element, according to the required order).\\nThe output is a permutation (a reordering, yet retaining all of the original elements) of the input.For optimum efficiency, the input data should be stored in a data structure which allows random access rather than one that allows only sequential access.\\n\\nHistory and concepts\\nFrom the beginning of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. Among the authors of early sorting algorithms around 1951 was Betty Holberton, who worked on ENIAC and UNIVAC. Bubble sort was analyzed as early as 1956. Asymptotically optimal algorithms have been known since the mid-20th century \\u2013  new algorithms are still being invented, with the widely used Timsort dating to 2002, and the library sort being first published in 2006.\\nComparison sorting algorithms have a fundamental requirement of \\u03a9(n log n) comparisons (some input sequences will require a multiple of n log n comparisons, where n is the number of elements in the array to be sorted). Algorithms not based on comparisons, such as counting sort, can have better performance.\\nSorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide-and-conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time\\u2013space tradeoffs, and upper and lower bounds.\\nSorting small arrays optimally (in fewest comparisons and swaps) or fast (i.e. taking into account machine specific details) is still an open research problem, with solutions only known for very small arrays (<20 elements). Similarly optimal (by various definitions) sorting on a parallel machine is an open research topic.\\n\\nClassification\\nSorting algorithms can be classified by:\\n\\nComputational complexity\\nBest, worst and average case behavior in terms of the size of the list. For typical serial sorting algorithms, good behavior is O(n log n), with parallel sort in O(log2 n), and bad behavior is O(n2). Ideal behavior for a serial sort is O(n), but this is not possible in the average case. Optimal parallel sorting is O(log n).\\nSwaps for \\\"in-place\\\" algorithms.\\nMemory usage (and use of other computer resources). In particular, some sorting algorithms are \\\"in-place\\\". Strictly, an in-place sort needs only O(1) memory beyond the items being sorted; sometimes O(log n) additional memory is considered \\\"in-place\\\".\\nRecursion: Some algorithms are either recursive or non-recursive, while others may be both (e.g., merge sort).\\nStability: stable sorting algorithms maintain the relative order of records with equal keys (i.e., values).\\nWhether or not they are a comparison sort. A comparison sort examines the data only by comparing two elements with a comparison operator.\\nGeneral method: insertion, exchange, selection, merging, etc. Exchange sorts include bubble sort and quicksort. Selection sorts include cycle sort and heapsort.\\nWhether the algorithm is serial or parallel. The remainder of this discussion almost exclusively concentrates upon serial algorithms and assumes serial operation.\\nAdaptability: Whether or not the presortedness of the input affects the running time.  Algorithms that take this into account are known to be adaptive.\\nOnline: An algorithm such as Insertion Sort that is online can sort a constant stream of input.\\n\\nStability\\nStable sort algorithms sort equal elements in the same order that they appear in the input. For example, in the card sorting example to the right, the cards are being sorted by their rank, and their suit is being ignored. This allows the possibility of multiple different correctly sorted versions of the original list. Stable sorting algorithms choose one of these, according to the following rule: if two items compare as equal (like the two 5 cards), then their relative order will be preserved, i.e. if one comes before the other in the input, it will come before the other in the output.\\nStability is important to preserve order over multiple sorts on the same data set. For example, say that student records consisting of name and class section are sorted dynamically, first by name, then by class section. If a stable sorting algorithm is used in both cases, the sort-by-class-section operation will not change the name order; with an unstable sort, it could be that sorting by section shuffles the name order, resulting in a nonalphabetical list of students.\\nMore formally, the data being sorted can be represented as a record or tuple of values, and the part of the data that is used for sorting is called the key. In the card example, cards are represented as a record (rank, suit), and the key is the rank. A sorting algorithm is stable if whenever there are two records R and S with the same key, and R appears before S in the original list, then R will always appear before S in the sorted list.\\nWhen equal elements are indistinguishable, such as with integers, or more generally, any data where the entire element is the key, stability is not an issue. Stability is also not an issue if all keys are different.\\nUnstable sorting algorithms can be specially implemented to be stable. One way of doing this is to artificially extend the key comparison, so that comparisons between two objects with otherwise equal keys are decided using the order of the entries in the original input list as a tie-breaker. Remembering this order, however, may require additional time and space.\\nOne application for stable sorting algorithms is sorting a list using a primary and secondary key. For example, suppose we wish to sort a hand of cards such that the suits are in the order clubs (\\u2663), diamonds (\\u2666), hearts (\\u2665), spades (\\u2660), and within each suit, the cards are sorted by rank. This can be done by first sorting the cards by rank (using any sort), and then doing a stable sort by suit:\\n\\nWithin each suit, the stable sort preserves the ordering by rank that was already done. This idea can be extended to any number of keys and is utilised by radix sort. The same effect can be achieved with an unstable sort by using a lexicographic key comparison, which, e.g., compares first by suit, and then compares by rank if the suits are the same.\\n\\nComparison of algorithms\\nIn these tables, n is the number of records to be sorted. The columns \\\"Best\\\", \\\"Average\\\" and \\\"Worst\\\" give the time complexity in each case, under the assumption that the length of each key is constant, and therefore that all comparisons, swaps and other operations can proceed in constant time. \\\"Memory\\\" denotes the amount of extra storage needed additionally to that used by the list itself, under the same assumption. The run times and the memory requirements listed are inside big O notation, hence the base of the logarithms does not matter. The notation log2 n means (log n)2.\\n\\nComparison sorts\\nBelow is a table of comparison sorts. A comparison sort cannot perform better than O(n log n) on average.\\n\\nNon-comparison sorts\\nThe following table describes integer sorting algorithms and other sorting algorithms that are not comparison sorts. As such, they are not limited to \\u03a9(n log n). Complexities below assume n items to be sorted, with keys of size k, digit size d, and r the range of numbers to be sorted. Many of them are based on the assumption that the key size is large enough that all entries have unique key values, and hence that n \\u226a 2k, where \\u226a means \\\"much less than\\\". In the unit-cost random-access machine model, algorithms with running time of \\n  \\n    \\n      \\n        \\n          n\\n          \\u22c5\\n          \\n            \\n              k\\n              d\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle n\\\\cdot {\\\\frac {k}{d}}}\\n  , such as radix sort, still take time proportional to \\u0398(n log n), because n is limited to be not more than \\n  \\n    \\n      \\n        \\n          2\\n          \\n            \\n              k\\n              d\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2^{\\\\frac {k}{d}}}\\n  , and a larger number of elements to sort would require a bigger k in order to store them in the memory.\\nSamplesort can be used to parallelize any of the non-comparison sorts, by efficiently distributing data into several buckets and then passing down sorting to several processors, with no need to merge as buckets are already sorted between each other.\\n\\nOthers\\nSome algorithms are slow compared to those discussed above, such as the bogosort with unbounded run time and the stooge sort which has O(n2.7) run time. These sorts are usually described for educational purposes to demonstrate how the run time of algorithms is estimated. The following table describes some sorting algorithms that are impractical for real-life use in traditional software contexts due to extremely poor performance or specialized hardware requirements.\\n\\nTheoretical computer scientists have detailed other sorting algorithms that provide better than O(n log n) time complexity assuming additional constraints, including:\\n\\nThorup's algorithm, a randomized algorithm for sorting keys from a domain of finite size, taking O(n log log n) time and O(n) space.\\nA randomized integer sorting algorithm taking \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            n\\n            \\n              \\n                log\\n                \\u2061\\n                log\\n                \\u2061\\n                n\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left(n{\\\\sqrt {\\\\log \\\\log n}}\\\\right)}\\n   expected time and O(n) space.\\nOne of the authors of the previously mentioned algorithm also claims to have discovered an algorithm taking \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            n\\n            \\n              \\n                log\\n                \\u2061\\n                n\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left(n{\\\\sqrt {\\\\log n}}\\\\right)}\\n   time and O(n) space, sorting real numbers. Further claiming that, without any added assumptions on the input, it can be modified to achieve \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            n\\n            log\\n            \\u2061\\n            n\\n            \\n              /\\n            \\n            \\n              \\n                log\\n                \\u2061\\n                log\\n                \\u2061\\n                n\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left(n\\\\log n/{\\\\sqrt {\\\\log \\\\log n}}\\\\right)}\\n   time and O(n) space.\\n\\nPopular sorting algorithms\\nWhile there are a large number of sorting algorithms, in practical implementations a few algorithms predominate. Insertion sort is widely used for small data sets, while for large data sets an asymptotically efficient sort is used, primarily heapsort, merge sort, or quicksort. Efficient implementations generally use a hybrid algorithm, combining an asymptotically efficient algorithm for the overall sort with insertion sort for small lists at the bottom of a recursion. Highly tuned implementations use more sophisticated variants, such as Timsort (merge sort, insertion sort, and additional logic), used in Android, Java, and Python, and introsort (quicksort and heapsort), used (in variant forms) in some C++ sort implementations and in .NET.\\nFor more restricted data, such as numbers in a fixed interval, distribution sorts such as counting sort or radix sort are widely used. Bubble sort and variants are rarely used in practice, but are commonly found in teaching and theoretical discussions.\\nWhen physically sorting objects (such as alphabetizing papers, tests or books) people intuitively generally use insertion sorts for small sets. For larger sets, people often first bucket, such as by initial letter, and multiple bucketing allows practical sorting of very large sets. Often space is relatively cheap, such as by spreading objects out on the floor or over a large area, but operations are expensive, particularly moving an object a large distance \\u2013 locality of reference is important. Merge sorts are also practical for physical objects, particularly as two hands can be used, one for each list to merge, while other algorithms, such as heapsort or quicksort, are poorly suited for human use. Other algorithms, such as library sort, a variant of insertion sort that leaves spaces, are also practical for physical use.\\n\\nSimple sorts\\nTwo of the simplest sorts are insertion sort and selection sort, both of which are efficient on small data, due to low overhead, but not efficient on large data. Insertion sort is generally faster than selection sort in practice, due to fewer comparisons and good performance on almost-sorted data, and thus is preferred in practice, but selection sort uses fewer writes, and thus is used when write performance is a limiting factor.\\n\\nInsertion sort\\nInsertion sort is a simple sorting algorithm that is relatively efficient for small lists and mostly sorted lists, and is often used as part of more sophisticated algorithms. It works by taking elements from the list one by one and inserting them in their correct position into a new sorted list similar to how we put money in our wallet. In arrays, the new list and the remaining elements can share the array's space, but insertion is expensive, requiring shifting all following elements over by one. Shellsort is a variant of insertion sort that is more efficient for larger lists.\\n\\nSelection sort\\nSelection sort is an in-place comparison sort. It has O(n2) complexity, making it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity, and also has performance advantages over more complicated algorithms in certain situations.\\nThe algorithm finds the minimum value, swaps it with the value in the first position, and repeats these steps for the remainder of the list. It does no more than n swaps, and thus is useful where swapping is very expensive.\\n\\nEfficient sorts\\nPractical general sorting algorithms are almost always based on an algorithm with average time complexity (and generally worst-case complexity) O(n log n), of which the most common are heapsort, merge sort, and quicksort. Each has advantages and drawbacks, with the most significant being that simple implementation of merge sort uses O(n) additional space, and simple implementation of quicksort has O(n2) worst-case complexity. These problems can be solved or ameliorated at the cost of a more complex algorithm.\\nWhile these algorithms are asymptotically efficient on random data, for practical efficiency on real-world data various modifications are used. First, the overhead of these algorithms becomes significant on smaller data, so often a hybrid algorithm is used, commonly switching to insertion sort once the data is small enough. Second, the algorithms often perform poorly on already sorted data or almost sorted data \\u2013 these are common in real-world data, and can be sorted in O(n) time by appropriate algorithms. Finally, they may also be unstable, and stability is often a desirable property in a sort. Thus more sophisticated algorithms are often employed, such as Timsort (based on merge sort) or introsort (based on quicksort, falling back to heapsort).\\n\\nMerge sort\\nMerge sort takes advantage of the ease of merging already sorted lists into a new sorted list. It starts by comparing every two elements (i.e., 1 with 2, then 3 with 4...) and swapping them if the first should come after the second. It then merges each of the resulting lists of two into lists of four, then merges those lists of four, and so on; until at last two lists are merged into the final sorted list. Of the algorithms described here, this is the first that scales well to very large lists, because its worst-case running time is O(n log n). It is also easily applied to lists, not only arrays, as it only requires sequential access, not random access. However, it has additional O(n) space complexity, and involves a large number of copies in simple implementations.\\nMerge sort has seen a relatively recent surge in popularity for practical implementations, due to its use in the sophisticated algorithm Timsort, which is used for the standard sort routine in the programming languages Python and Java (as of JDK7). Merge sort itself is the standard routine in Perl, among others, and has been used in Java at least since 2000 in JDK1.3.\\n\\nHeapsort\\nHeapsort is a much more efficient version of selection sort. It also works by determining the largest (or smallest) element of the list, placing that at the end (or beginning) of the list, then continuing with the rest of the list, but accomplishes this task efficiently by using a data structure called a heap, a special type of binary tree. Once the data list has been made into a heap, the root node is guaranteed to be the largest (or smallest) element. When it is removed and placed at the end of the list, the heap is rearranged so the largest element remaining moves to the root. Using the heap, finding the next largest element takes O(log n) time, instead of O(n) for a linear scan as in simple selection sort. This allows Heapsort to run in O(n log n) time, and this is also the worst case complexity.\\n\\nQuicksort\\nQuicksort is a divide-and-conquer algorithm which relies on a partition operation: to partition an array, an element called a pivot is selected. All elements smaller than the pivot are moved before it and all greater elements are moved after it. This can be done efficiently in linear time and in-place. The lesser and greater sublists are then recursively sorted. This yields average time complexity of O(n log n), with low overhead, and thus this is a popular algorithm. Efficient implementations of quicksort (with in-place partitioning) are typically unstable sorts and somewhat complex, but are among the fastest sorting algorithms in practice. Together with its modest O(log n) space usage, quicksort is one of the most popular sorting algorithms and is available in many standard programming libraries.\\nThe important caveat about quicksort is that its worst-case performance is O(n2); while this is rare, in naive implementations (choosing the first or last element as pivot) this occurs for sorted data, which is a common case. The most complex issue in quicksort is thus choosing a good pivot element, as consistently poor choices of pivots can result in drastically slower O(n2) performance, but good choice of pivots yields O(n log n) performance, which is asymptotically optimal. For example, if at each step the median is chosen as the pivot then the algorithm works in O(n log n). Finding the median, such as by the median of medians selection algorithm is however an O(n) operation on unsorted lists and therefore exacts significant overhead with sorting. In practice choosing a random pivot almost certainly yields O(n log n) performance.\\nIf a guarantee of O(n log n) performance is important, there is a simple modification to achieve that. The idea, due to Musser, is to set a limit on the maximum depth of recursion. If that limit is exceeded, then sorting is continued using the heapsort algorithm. Musser proposed that the limit should be \\n  \\n    \\n      \\n        1\\n        +\\n        2\\n        \\u230a\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        )\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle 1+2\\\\lfloor \\\\log _{2}(n)\\\\rfloor }\\n  , which is approximately twice the maximum recursion depth one would expect on average with a randomly ordered array.\\n\\nShellsort\\nShellsort was invented by Donald Shell in 1959. It improves upon insertion sort by moving out of order elements more than one position at a time. The concept behind Shellsort is that insertion sort performs in \\n  \\n    \\n      \\n        O\\n        (\\n        k\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(kn)}\\n   time, where k is the greatest distance between two out-of-place elements. This means that generally, they perform in O(n2), but for data that is mostly sorted, with only a few elements out of place, they perform faster. So, by first sorting elements far away, and progressively shrinking the gap between the elements to sort, the final sort computes much faster. One implementation can be described as arranging the data sequence in a two-dimensional array and then sorting the columns of the array using insertion sort.\\nThe worst-case time complexity of Shellsort is an open problem and depends on the gap sequence used, with known complexities ranging from O(n2) to O(n4/3) and \\u0398(n log2 n). This, combined with the fact that Shellsort is in-place, only needs a relatively small amount of code, and does not require use of the call stack, makes it is useful in situations where memory is at a premium, such as in embedded systems and operating system kernels.\\n\\nBubble sort and variants\\nBubble sort, and variants such as the Comb sort and cocktail sort, are simple, highly inefficient sorting algorithms. They are frequently seen in introductory texts due to ease of analysis, but they are rarely used in practice.\\n\\nBubble sort\\nBubble sort is a simple sorting algorithm. The algorithm starts at the beginning of the data set. It compares the first two elements, and if the first is greater than the second, it swaps them. It continues doing this for each pair of adjacent elements to the end of the data set. It then starts again with the first two elements, repeating until no swaps have occurred on the last pass. This algorithm's average time and worst-case performance is O(n2), so it is rarely used to sort large, unordered data sets. Bubble sort can be used to sort a small number of items (where its asymptotic inefficiency is not a high penalty). Bubble sort can also be used efficiently on a list of any length that is nearly sorted (that is, the elements are not significantly out of place). For example, if any number of elements are out of place by only one position (e.g. 0123546789 and 1032547698), bubble sort's exchange will get them in order on the first pass, the second pass will find all elements in order, so the sort will take only 2n time.\\n\\nComb sort\\nComb sort is a relatively simple sorting algorithm based on bubble sort and originally designed by W\\u0142odzimierz Dobosiewicz in 1980. It was later rediscovered and popularized by Stephen Lacey and Richard Box with a Byte Magazine article published in April 1991. The basic idea is to eliminate turtles, or small values near the end of the list, since in a bubble sort these slow the sorting down tremendously. (Rabbits, large values around the beginning of the list, do not pose a problem in bubble sort) It accomplishes this by initially swapping elements that are a certain distance from one another in the array, rather than only swapping elements if they are adjacent to one another, and then shrinking the chosen distance until it is operating as a normal bubble sort. Thus, if Shellsort can be thought of as a generalized version of insertion sort that swaps elements spaced a certain distance away from one another, comb sort can be thought of as the same generalization applied to bubble sort.\\n\\nExchange sort\\nExchange sort is sometimes confused with bubble sort, although the algorithms are in fact distinct. Exchange sort works by comparing the first element with all elements above it, swapping where needed, thereby guaranteeing that the first element is correct for the final sort order; it then proceeds to do the same for the second element, and so on. It lacks the advantage which bubble sort has of detecting in one pass if the list is already sorted, but it can be faster than bubble sort by a constant factor (one less pass over the data to be sorted; half as many total comparisons) in worst case situations. Like any simple O(n2) sort it can be reasonably fast over very small data sets, though in general insertion sort will be faster.\\n\\nDistribution sorts\\nDistribution sort refers to any sorting algorithm where data is distributed from their input to multiple intermediate structures which are then gathered and placed on the output. For example, both bucket sort and flashsort are distribution based sorting algorithms. Distribution sorting algorithms can be used on a single processor, or they can be a distributed algorithm, where individual subsets are separately sorted on different processors, then combined. This allows external sorting of data too large to fit into a single computer's memory.\\n\\nCounting sort\\nCounting sort is applicable when each input is known to belong to a particular set, S, of possibilities.  The algorithm runs in O(|S| + n) time and O(|S|) memory where n is the length of the input.  It works by creating an integer array of size |S| and using the ith bin to count the occurrences of the ith member of S in the input.  Each input is then counted by incrementing the value of its corresponding bin.  Afterward, the counting array is looped through to arrange all of the inputs in order.  This sorting algorithm often cannot be used because S needs to be reasonably small for the algorithm to be efficient, but it is extremely fast and demonstrates great asymptotic behavior as n increases.  It also can be modified to provide stable behavior.\\n\\nBucket sort\\nBucket sort is a divide-and-conquer sorting algorithm that generalizes counting sort by partitioning an array into a finite number of buckets.  Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm.\\nA bucket sort works best when the elements of the data set are evenly distributed across all buckets.\\n\\nRadix sort\\nRadix sort is an algorithm that sorts numbers by processing individual digits. n numbers consisting of k digits each are sorted in O(n \\u00b7 k) time.  Radix sort can process digits of each number either starting from the least significant digit (LSD) or starting from the most significant digit (MSD).  The LSD algorithm first sorts the list by the least significant digit while preserving their relative order using a stable sort. Then it sorts them by the next digit, and so on from the least significant to the most significant, ending up with a sorted list. While the LSD radix sort requires the use of a stable sort, the MSD radix sort algorithm does not (unless stable sorting is desired).  In-place MSD radix sort is not stable.  It is common for the counting sort algorithm to be used internally by the radix sort.  A hybrid sorting approach, such as using insertion sort for small bins, improves performance of radix sort significantly.\\n\\nMemory usage patterns and index sorting\\nWhen the size of the array to be sorted approaches or exceeds the available primary memory, so that (much slower) disk or swap space must be employed, the memory usage pattern of a sorting algorithm becomes important, and an algorithm that might have been fairly efficient when the array fit easily in RAM may become impractical. In this scenario, the total number of comparisons becomes (relatively) less important, and the number of times sections of memory must be copied or swapped to and from the disk can dominate the performance characteristics of an algorithm. Thus, the number of passes and the localization of comparisons can be more important than the raw number of comparisons, since comparisons of nearby elements to one another happen at system bus speed (or, with caching, even at CPU speed), which, compared to disk speed, is virtually instantaneous.\\nFor example, the popular recursive quicksort algorithm provides quite reasonable performance with adequate RAM, but due to the recursive way that it copies portions of the array it becomes much less practical when the array does not fit in RAM, because it may cause a number of slow copy or move operations to and from disk. In that scenario, another algorithm may be preferable even if it requires more total comparisons.\\nOne way to work around this problem, which works well when complex records (such as in a relational database) are being sorted by a relatively small key field, is to create an index into the array and then sort the index, rather than the entire array. (A sorted version of the entire array can then be produced with one pass, reading from the index, but often even that is unnecessary, as having the sorted index is adequate.)  Because the index is much smaller than the entire array, it may fit easily in memory where the entire array would not, effectively eliminating the disk-swapping problem. This procedure is sometimes called \\\"tag sort\\\".Another technique for overcoming the memory-size problem is using external sorting, for example one of the ways is to combine two algorithms in a way that takes advantage of the strength of each to improve overall performance. For instance, the array might be subdivided into chunks of a size that will fit in RAM, the contents of each chunk sorted using an efficient algorithm (such as quicksort), and the results merged using a k-way merge similar to that used in merge sort. This is faster than performing either merge sort or quicksort over the entire list.Techniques can also be combined. For sorting very large sets of data that vastly exceed system memory, even the index may need to be sorted using an algorithm or combination of algorithms designed to perform reasonably with virtual memory, i.e., to reduce the amount of swapping required.\\n\\nRelated algorithms\\nRelated problems include approximate sorting (sorting a sequence to within a certain amount of the correct order), partial sorting (sorting only the k smallest elements of a list, or finding the k smallest elements, but unordered) and selection (computing the kth smallest element). These can be solved inefficiently by a total sort, but more efficient algorithms exist, often derived by generalizing a sorting algorithm. The most notable example is quickselect, which is related to quicksort. Conversely, some sorting algorithms can be derived by repeated application of a selection algorithm; quicksort and quickselect can be seen as the same pivoting move, differing only in whether one recurses on both sides (quicksort, divide-and-conquer) or one side (quickselect, decrease-and-conquer).\\nA kind of opposite of a sorting algorithm is a shuffling algorithm. These are fundamentally different because they require a source of random numbers. Shuffling can also be implemented by a sorting algorithm, namely by a random sort: assigning a random number to each element of the list and then sorting based on the random numbers. This is generally not done in practice, however, and there is a well-known simple and efficient algorithm for shuffling: the Fisher\\u2013Yates shuffle.\\nSorting algorithms are ineffective for finding an order in many situations. Usually when elements have no reliable comparison function (crowdsourced preferences like voting systems), comparisons are very costly (sports), or when it would be impossible to pairwise compare all elements for all criteria (search engines). In these cases, the problem is usually referred to as ranking and the goal is to find the \\\"best\\\" result for some criteria according to probabilities inferred from comparisons or rankings. A common example is in chess, where players are ranked with the Elo rating system, and rankings are determined by a tournament system instead of a sorting algorithm.\\n\\nSee also\\nCollation \\u2013 Assembly of written information into a standard order\\nK-sorted sequence\\nPairwise comparison\\nSchwartzian transform \\u2013 Programming idiom for efficiently sorting a list by a computed key\\nSearch algorithm \\u2013 Any algorithm which solves the search problem\\nQuantum sort \\u2013 Sorting algorithms for quantum computers\\n\\nReferences\\nFurther reading\\nKnuth, Donald E. (1998), Sorting and Searching, The Art of Computer Programming, vol. 3 (2nd ed.), Boston: Addison-Wesley, ISBN 0-201-89685-0\\nSedgewick, Robert (1980), \\\"Efficient Sorting by Computer: An Introduction\\\", Computational Probability, New York: Academic Press, pp. 101\\u2013130, ISBN 0-12-394680-8\\n\\nExternal links\\n\\nSorting Algorithm Animations at the Wayback Machine (archived 3 March 2015).\\nSequential and parallel sorting algorithms \\u2013 Explanations and analyses of many sorting algorithms.\\nDictionary of Algorithms, Data Structures, and Problems \\u2013 Dictionary of algorithms, techniques, common functions, and problems.\\nSlightly Skeptical View on Sorting Algorithms \\u2013 Discusses several classic algorithms and promotes alternatives to the quicksort algorithm.\\n15 Sorting Algorithms in 6 Minutes (Youtube) \\u2013 Visualization and \\\"audibilization\\\" of 15 Sorting Algorithms in 6 Minutes.\\nA036604 sequence in OEIS database titled \\\"Sorting numbers: minimal number of comparisons needed to sort n elements\\\" \\u2013 Performed by Ford\\u2013Johnson algorithm.\\nSorting Algorithms Used on Famous Paintings (Youtube) \\u2013 Visualization of Sorting Algorithms on Many Famous Paintings.\\nA Comparison of Sorting Algorithms \\u2013 Runs a series of tests of 9 of the main sorting algorithms using Python timeit and Google Colab.\"}, {\"Adaptive heap sort\": \"In computer science, adaptive heap sort is a comparison-based sorting algorithm of the adaptive sort family. It is a variant of heap sort that performs better when the data contains existing order. Published by Christos Levcopoulos and Ola Petersson in 1992, the algorithm utilizes a new measure of presortedness, Osc,  as the number of oscillations. Instead of putting all the data into the heap as the traditional heap sort did, adaptive heap sort only take part of the data into the heap so that the run time will reduce significantly when the presortedness of the data is high.\\n\\nHeapsort\\nHeap sort is a sorting algorithm that utilizes binary heap data structure. The method treats an array as a complete binary tree and builds up a Max-Heap/Min-Heap to achieve sorting. It usually involves the following four steps.\\n\\nBuild a Max-Heap(Min-Heap): put all the data into the heap so that all nodes are either greater than or equal  (less than or equal to for Min-Heap) to each of its child nodes.\\nSwap the first element of the heap with the last element of the heap.\\nRemove the last element from the heap and put it at the end of the list. Adjust the heap so that the first element ends up at the right place in the heap.\\nRepeat Step 2 and 3 until the heap has only one element. Put this last element at the end of the list and output the list. The data in the list will be sorted.Below is a C/C++ implementation that builds up a Max-Heap and sorts the array after the heap is built.\\n\\nMeasures of presortedness\\nMeasures of presortedness measures the existing order in a given sequence. These measures of presortedness decides the number of data that will be put in to the heap during the sorting process  as well as the lower bound of running time.\\n\\nOscillations (Osc)\\nFor sequence \\n  \\n    \\n      \\n        X\\n        =\\n        \\u27e8\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle X=\\\\langle x_{1},x_{2},x_{3},\\\\dots ,x_{n}\\\\rangle }\\n  , Cross(xi) is defined as the number edges of the line plot of X that are intersected by a horizontal line through the point (i, xi). Mathematically, it is defined as \\n  \\n    \\n      \\n        \\n          \\n            C\\n            r\\n            o\\n            s\\n            s\\n          \\n        \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        {\\n        j\\n        \\u2223\\n        min\\n        {\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            j\\n            +\\n            1\\n          \\n        \\n        }\\n        <\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        <\\n        max\\n        {\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            j\\n            +\\n            1\\n          \\n        \\n        }\\n        \\n           for \\n        \\n        1\\n        \\u2264\\n        j\\n        <\\n        n\\n        }\\n        \\n          , for \\n        \\n        1\\n        \\u2264\\n        i\\n        \\u2264\\n        n\\n      \\n    \\n    {\\\\displaystyle {\\\\mathit {Cross}}(x_{i})=\\\\{j\\\\mid \\\\min\\\\{x_{j},x_{j+1}\\\\}<x_{i}<\\\\max\\\\{x_{j},x_{j+1}\\\\}{\\\\text{ for }}1\\\\leq j<n\\\\}{\\\\text{, for }}1\\\\leq i\\\\leq n}\\n  . The oscillation(Osc) of X is just the total number of intersections, defined as \\n  \\n    \\n      \\n        \\n          \\n            O\\n            s\\n            c\\n          \\n        \\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            \\u2211\\n            \\n              i\\n              =\\n              1\\n            \\n            \\n              n\\n            \\n          \\n          \\n            \\u2016\\n            \\n              \\n                C\\n                r\\n                o\\n                s\\n                s\\n              \\n            \\n            (\\n            \\n              x\\n              \\n                i\\n              \\n            \\n            )\\n            \\u2016\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathit {Osc}}(x)=\\\\textstyle \\\\sum _{i=1}^{n}\\\\displaystyle \\\\lVert {\\\\mathit {Cross}}(x_{i})\\\\rVert }\\n  .\\n\\nOther measures\\nBesides the original Osc measurement, other known measures include the number of inversions Inv, the number of runs Runs, the number of blocks Block, and the measures Max, Exc and Rem. Most of these different measurements are related for adaptive heap sort. Some measures dominate the others: every Osc-optimal algorithm is Inv optimal and Runs optimal; every Inv-optimal algorithm is Max optimal; and every Block-optimal algorithm is Exc optimal and Rem optimal.\\n\\nAlgorithm\\nAdaptive heap sort is a variant of heap sort that seeks optimality (asymptotically optimal) with respect to the lower bound derived with the measure of presortedness by taking advantage of the existing order in the data. In heap sort, for a data \\n  \\n    \\n      \\n        X\\n        =\\n        \\u27e8\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle X=\\\\langle x_{1},x_{2},x_{3},\\\\dots ,x_{n}\\\\rangle }\\n   , we put all n elements into the heap and then keep extracting the maximum (or minimum) for n times. Since the time of each max-extraction action is the logarithmic in the size of the heap, the total running time of standard heap sort is \\n  \\n    \\n      \\n        \\n          O\\n          (\\n          n\\n          log\\n          \\u2061\\n          n\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\color {Blue}O(n\\\\log n)}\\n  . For adaptive heap sort, instead of putting all the elements into the heap, only the possible maximums of the data (max-candidates) will be put into the heap so that fewer runs are required when each time we try to locate the maximum (or minimum).\\nFirst, a Cartesian tree is built from the input in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time by putting the data into a binary tree and making each node in the tree is greater(or smaller) than all its children nodes, and the root of the Cartesian tree is inserted into an empty binary heap. Then repeatedly extract the maximum from the binary heap, retrieve the maximum in the Cartesian tree, and add its left and right children (if any) which are themselves Cartesian trees, to the binary heap. If the input is already nearly sorted, the Cartesian trees will be very unbalanced, with few nodes having left and right children, resulting in the binary heap remaining small, and allowing the algorithm to sort more quickly than \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   for inputs that are already nearly sorted.Below is an implementation in pseudo-code:\\nInput: an array of n elements that need to be sorted\\n\\nConstruct the Cartesian tree l(x)\\nInsert the root of l(x) into a heap\\n\\nfor i = from 1 to n\\n{\\n    Perform ExtractMax on the heap \\n    if the max element extracted has any children in l(x)\\n    {\\n        retrieve the children in l(x)\\n        insert the children element into the heap\\n    }\\n}\\n\\nDrawbacks\\nDespite decades of research, there's still a gap between the theory of adaptive heap sort and its practical use. Because the algorithm makes use of Cartesian trees and pointer manipulation, it has low cache-efficiency and high memory requirements, both of which deteriorate the performance of implementations.\\n\\nSee also\\nAdaptive sort\\nHeapsort\\nCartesian tree\\n\\n\\n== References ==\"}, {\"Adaptive sort\": \"A sorting algorithm falls into the adaptive sort family if it takes advantage of existing order in its input.  It benefits from the presortedness in the input sequence \\u2013 or a limited amount of disorder for various definitions of measures of disorder \\u2013 and sorts faster.  Adaptive sorting is usually performed by modifying existing sorting algorithms.\\n\\nMotivation\\nComparison-based sorting algorithms have traditionally dealt with achieving an optimal bound of O(n log n) when dealing with time complexity.  Adaptive sort takes advantage of the existing order of the input to try to achieve better times, so that the time taken by the algorithm to sort is a smoothly growing function of the size of the sequence and the disorder in the sequence.  In other words, the more presorted the input is, the faster it should be sorted.\\nThis is an attractive feature for a sorting algorithm because nearly sorted sequences are common in practice.  Thus, the performance of existing sort algorithms can be improved by taking into account the existing order in the input.\\nNote that most worst-case sorting algorithms that do optimally well in the worst-case, notably heap sort and merge sort, do not take existing order within their input into account, although this deficiency is easily rectified in the case of merge sort by checking if the last element of the lefthand group is less than (or equal) to the first element of the righthand group, in which case a merge operation may be replaced by simple concatenation \\u2013 a modification that is well within the scope of making an algorithm adaptive.\\n\\nExamples\\nA classic example of an adaptive sorting algorithm is Straight Insertion Sort.  In this sorting algorithm, we scan the input from left to right, repeatedly finding the position of the current item, and insert it into an array of previously sorted items.\\nIn pseudo-code form, the Straight Insertion Sort algorithm could look something like this (array X is zero-based):\\n\\nprocedure Straight Insertion Sort (X):\\n    for j := 1 to length(X) - 1 do\\n        t := X[j]\\n        i := j\\n        while i > 0 and X[i - 1] > t do\\n            X[i] := X[i - 1]\\n            i := i - 1\\n        end\\n        X[i] := t\\n    end\\n\\nThe performance of this algorithm can be described in terms of the number of inversions in the input, and then \\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle T(n)}\\n   will be roughly equal to \\n  \\n    \\n      \\n        I\\n        (\\n        A\\n        )\\n        +\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle I(A)+(n-1)}\\n  , where \\n  \\n    \\n      \\n        I\\n        (\\n        A\\n        )\\n      \\n    \\n    {\\\\displaystyle I(A)}\\n   is the number of Inversions.  Using this measure of presortedness \\u2013 being relative to the number of inversions \\u2013 Straight Insertion Sort takes less time to sort the closer it is to being sorted.\\nOther examples of adaptive sorting algorithms are adaptive heap sort, adaptive merge sort, patience sort, Shellsort, smoothsort, splaysort, Timsort, and Cartesian tree sorting.\\n\\nSee also\\nSorting algorithms\\nSmoothsort\\n\\nReferences\\nHagerup, Torben; Jyrki Katjainen (2004). Algorithm Theory \\u2013 SWAT 2004. Berlin Heidelberg: Springer-Verlag. pp. 221\\u2013222. ISBN 3-540-22339-8.\\nMehta, Dinesh P.; Sartaj Sahni (2005). Data Structures and Applications. USA: Chapman & Hall/CRC. pp. 11\\u20118\\u201311\\u20119. ISBN 1-58488-435-5.\\nPetersson, Ola; Alistair Moffat (1992). A framework for adaptive sorting. Lecture Notes in Computer Science. Vol. 621. Berlin: Springer Berlin / Heidelberg. pp. 422\\u2013433. doi:10.1007/3-540-55706-7_38. ISBN 978-3-540-55706-7. ISSN 1611-3349.\"}, {\"Batcher odd\\u2013even mergesort\": \"Batcher's odd\\u2013even mergesort\\nis a generic construction devised by Ken Batcher for sorting networks of size O(n (log n)2) and depth O((log n)2), where n is the number of items to be sorted. Although it is not asymptotically optimal, Knuth concluded in 1998, with respect to the AKS network that \\\"Batcher's method is much better, unless n exceeds the total memory capacity of all computers on earth!\\\"It is popularized by the second GPU Gems book, as an easy way of doing reasonably efficient sorts on graphics-processing hardware.\\n\\nPseudocode\\nVarious recursive and iterative schemes are possible to calculate the indices of the elements to be compared and sorted. This is one iterative technique to generate the indices for sorting n elements:\\n\\nNon-recursive calculation of the partner node index is also possible.\\n\\nSee also\\nBitonic sorter\\nPairwise sorting network\\n\\nReferences\\nExternal links\\nOdd\\u2013even mergesort at fh-flensburg.de\\nOdd-even mergesort network generator Interactive Batcher's Odd-Even merge-based sorting network generator.\"}, {\"Bead sort\": \"Bead sort, also called gravity sort, is a natural sorting algorithm, developed by Joshua J. Arulanandham, Cristian S. Calude and Michael J. Dinneen in 2002, and published in The Bulletin of the European Association for Theoretical Computer Science.  Both digital and analog hardware implementations of bead sort can achieve a sorting time of O(n); however, the implementation of this algorithm tends to be significantly slower in software and can only be used to sort lists of positive integers.  Also, it would seem that even in the best case, the algorithm requires O(n2) space.\\n\\nAlgorithm overview\\nThe bead sort operation can be compared to the manner in which beads slide on parallel poles, such as on an abacus.  However, each pole may have a distinct number of beads.  Initially, it may be helpful to imagine the beads suspended on vertical poles.  In Step 1, such an arrangement is displayed using n=5 rows of beads on m=4 vertical poles.  The numbers to the right of each row indicate the number that the row in question represents; rows 1 and 2 are representing the positive integer 3 (because they each contain three beads) while the top row represents the positive integer 2 (as it only contains two beads).If we then allow the beads to fall, the rows now represent the same integers in sorted order.  Row 1 contains the largest number in the set, while row n contains the smallest.  If the above-mentioned convention of rows containing a series of beads on poles 1..k and leaving poles k+1..m empty has been followed, it will continue to be the case here.\\nThe action of allowing the beads to \\\"fall\\\" in our physical example has allowed the larger values from the higher rows to propagate to the lower rows.  If the value represented by row a is smaller than the value contained in row a+1, some of the beads from row a+1 will fall into row a; this is certain to happen, as row a does not contain beads in those positions to stop the beads from row a+1 from falling.\\nThe mechanism underlying bead sort is similar to that behind counting sort; the number of beads on each pole corresponds to the number of elements with value equal or greater than the index of that pole.\\n\\nComplexity\\nBead sort can be implemented with four general levels of complexity, among others:\\n\\nO(1): The beads are all moved simultaneously in the same time unit, as would be the case with the simple physical example above. This is an abstract complexity, and cannot be implemented in practice.\\nO(\\n  \\n    \\n      \\n        \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {n}}}\\n  ): In a realistic physical model that uses gravity, the time it takes to let the beads fall is proportional to the square root of the maximum height, which is proportional to n.\\nO(n): The beads are moved one row at a time.  This is the case used in the analog and digital hardware solutions.\\nO(S), where S is the sum of the integers in the input set: Each bead is moved individually.  This is the case when bead sort is implemented without a mechanism to assist in finding empty spaces below the beads, such as in software implementations.Like the Pigeonhole sort, bead sort is unusual in that in worst case it can perform faster than O(n log n), the fastest performance possible for a comparison sort in worst case. This is possible because the key for a bead sort is always a positive integer and bead sort exploits its structure.\\n\\nImplementation\\nThis implementation is written in the Python; it is assumed that the input_list will be a sequence of integers. The function returns a new list rather than mutating the one passed in, but it can be trivially modified to operate in place efficiently.\\n\\nWe can also implement the algorithm using Java.\\n\\nNotes\\nReferences\\nExternal links\\n\\\"Bead-Sort: A Natural Sorting Algorithm\\\" (PDF). Archived from the original (PDF) on 2017-08-09. Retrieved 2005-01-01. (114 KiB)\\nBead Sort in MGS, a visualization of a bead sort implemented in the MGS programming language\\nBead Sort on MathWorld\\nBead Sort interactive visualization\"}, {\"Binary prioritization\": \"Binary prioritization is a sorting algorithm which prioritizes to-do tasks.\\nUnlike other Binary Sort methods (e.g. binary search) this method assumes that the deferred work will be prioritized in a later process, but their order is not relevant in the first iteration. The faster processing of classified and important tasks is achieved by reducing the cost of sorting by not sorting the subset of the less important tasks. In each iteration, the cost is reduced by the sorted elements.\\n\\nApplication requirements\\nFor the application of the binary prioritization it is assumed that the elements to be prioritized are present in an unsorted heap.\\n\\nAlgorithm\\nThe algorithm of the binary prioritization is as follows:\\n\\nOne element is taken from the stack (or list).\\nThe element (the object) is analyzed regarding its priority (importance) relative to the other elements.\\nIf the item is judged as important, it is sorted on a new batch (to a new list) of important elements; if not, it is sorted on a different stack (in a different list) of unimportant elements. This is repeated in the first iteration until all items have been evaluated into two new stacks (or lists).\\nThe two stacks (lists) are then reunited by the stacking the important elements on the elements that were considered to be unimportant. The last element of the first stack is memorized, e.g. by a separator.\\nThe algorithm is then applied in an additional iteration on the united stack up to the separator element.\\nIf there was only one element in the last step, the algorithm is complete.\\n\\nApplication\\nAn example of the application of Binary sorting is the following scenario: A mail inbox has to be sorted and important mails have to be answered first, no matter the relative importance of rather unimportant mails.\"}, {\"Bitonic sorter\": \"Bitonic mergesort is a parallel algorithm for sorting. It is also used as a construction method for building a sorting network. The algorithm was devised by Ken Batcher. The resulting sorting networks consist of \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log ^{2}(n))}\\n   comparators and have a delay of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle O(\\\\log ^{2}(n))}\\n  , where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of items to be sorted.A sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. A bitonic sequence is a sequence with \\n  \\n    \\n      \\n        \\n          x\\n          \\n            0\\n          \\n        \\n        \\u2264\\n        \\u22ef\\n        \\u2264\\n        \\n          x\\n          \\n            k\\n          \\n        \\n        \\u2265\\n        \\u22ef\\n        \\u2265\\n        \\n          x\\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{0}\\\\leq \\\\cdots \\\\leq x_{k}\\\\geq \\\\cdots \\\\geq x_{n-1}}\\n   for some \\n  \\n    \\n      \\n        k\\n        ,\\n        0\\n        \\u2264\\n        k\\n        <\\n        n\\n      \\n    \\n    {\\\\displaystyle k,0\\\\leq k<n}\\n  , or a circular shift of such a sequence.\\n\\nComplexity\\nLet \\n  \\n    \\n      \\n        p\\n        =\\n        \\u230a\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle p=\\\\lfloor \\\\log _{2}n\\\\rfloor }\\n   and \\n  \\n    \\n      \\n        q\\n        =\\n        \\u2308\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2309\\n      \\n    \\n    {\\\\displaystyle q=\\\\lceil \\\\log _{2}n\\\\rceil }\\n  .\\nIt is obvious from the construction algorithm that the number of rounds of parallel comparisons is given by \\n  \\n    \\n      \\n        q\\n        (\\n        q\\n        +\\n        1\\n        )\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle q(q+1)/2}\\n  .\\nIt follows that the number of comparators \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   is bounded \\n  \\n    \\n      \\n        \\n          2\\n          \\n            p\\n            \\u2212\\n            1\\n          \\n        \\n        \\u22c5\\n        p\\n        (\\n        p\\n        +\\n        1\\n        )\\n        \\n          /\\n        \\n        2\\n        \\u2264\\n        c\\n        \\u2264\\n        \\u230a\\n        \\n          n\\n          \\n            /\\n          \\n          2\\n        \\n        \\u230b\\n        \\u22c5\\n        q\\n        (\\n        q\\n        +\\n        1\\n        )\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle 2^{p-1}\\\\cdot p(p+1)/2\\\\leq c\\\\leq \\\\lfloor {n/2}\\\\rfloor \\\\cdot q(q+1)/2}\\n   (which establishes an exact value for \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   when \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is a power of 2).\\n\\nHow the algorithm works\\nThe following is a bitonic sorting network with 16 inputs:\\n\\nThe 16 numbers enter as the inputs at the left end, slide along each of the 16 horizontal wires, and exit at the outputs at the right end. The network is designed to sort the elements, with the largest number at the bottom.\\nThe arrows are comparators. Whenever two numbers reach the two ends of an arrow, they are compared to ensure that the arrow points toward the larger number. If they are out of order, they are swapped. The colored boxes are just for illustration and have no effect on the algorithm.\\nEvery red box has the same structure: each input in the top half is compared to the corresponding input in the bottom half, with all arrows pointing down (dark red) or all up (light red). If the inputs happen to form a bitonic sequence (a single nondecreasing sequence followed by a single nonincreasing one or vice versa), then the output will form two bitonic sequences. The top half of the output will be bitonic, and the bottom half will be bitonic, with every element of the top half less than or equal to every element of the bottom half (for dark red) or vice versa (for light red). This theorem is not obvious, but can be verified by carefully considering all the cases of how the various inputs might compare, using the zero-one principle, where a bitonic sequence is a sequence of 0s and 1s that contains no more than two \\\"10\\\" or \\\"01\\\" subsequences.\\nThe red boxes combine to form blue and green boxes. Every such box has the same structure: a red box is applied to the entire input sequence, then to each half of the result, then to each half of each of those results, and so on. All arrows point down (blue) or all point up (green). This structure is known as a butterfly network. If the input to this box happens to be bitonic, then the output will be completely sorted in increasing order (blue) or decreasing order (green). If a number enters the blue or green box, then the first red box will sort it into the correct half of the list. It will then pass through a smaller red box that sorts it into the correct quarter of the list within that half. This continues until it is sorted into exactly the correct position. Therefore, the output of the green or blue box will be completely sorted.\\nThe green and blue boxes combine to form the entire sorting network. For any arbitrary sequence of inputs, it will sort them correctly, with the largest at the bottom. The output of each green or blue box will be a sorted sequence, so the output of each pair of adjacent lists will be bitonic, because the top one is blue and the bottom one is green. Each column of blue and green boxes takes N sorted sequences and concatenates them in pairs to form N/2 bitonic sequences, which are then sorted by the boxes in that column to form N/2 sorted sequences. This process starts with each input considered to be a sorted list of one element, and continues through all the columns until the last merges them into a single, sorted list. Because the last stage was blue, this final list will have the largest element at the bottom.\\n\\nAlternative representation\\nEach green box, in the diagram above,  performs the same operation as a blue box, but with the sort in the opposite direction. So, each green box could be replaced by a blue box followed by a crossover where all the wires move to the opposite position. This would allow all the arrows to point the same direction, but would prevent the horizontal lines from being straight. However, a similar crossover could be placed to the right of the bottom half of the outputs from any red block, and the sort would still work correctly, because the reverse of a bitonic sequence is still bitonic. If a red box then has a crossover before and after it, it can be rearranged internally so the two crossovers cancel, so the wires become straight again. Therefore, the following diagram is equivalent to the one above, where each green box has become a blue plus a crossover, and each orange box is a red box that absorbed two such crossovers:\\n\\nThe arrowheads are not drawn, because every comparator sorts in the same direction. The blue and red blocks perform the same operations as before. The orange blocks are equivalent to red blocks where the sequence order is reversed for the bottom half of its inputs and the bottom half of its outputs. This is the most common representation of a bitonic sorting network\\n\\nExample code\\nThe following is a recursion-free implementation of the bitonic mergesort when the array length is a power of two:\\n\\nSee also\\nBatcher odd\\u2013even mergesort\\nBitonicNull sort\\nPairwise sorting network\\n\\nReferences\\nExternal links\\nA discussion of this algorithm\\nReference code at NIST\\nTutorial with animated pictures and working code\"}, {\"Block sort\": \"Block sort, or block merge sort, is a sorting algorithm combining at least two merge operations with an insertion sort to arrive at O(n log n) in-place stable sorting. It gets its name from the observation that merging two sorted lists, A and B, is equivalent to breaking A into evenly sized blocks, inserting each A block into B under special rules, and merging AB pairs.\\nOne practical algorithm for O(log n) in place merging was proposed by Pok-Son Kim and Arne Kutzner in 2008.\\n\\nOverview\\nThe outer loop of block sort is identical to a bottom-up merge sort, where each level of the sort merges pairs of subarrays, A and B, in sizes of 1, then 2, then 4, 8, 16, and so on, until both subarrays combined are the array itself.\\nRather than merging A and B directly as with traditional methods, a block-based merge algorithm divides A into discrete blocks of size \\u221aA (resulting in \\u221aA number of blocks as well), inserts each A block into B such that the first value of each A block is less than or equal (\\u2264) to the B value immediately after it, then locally merges each A block with any B values between it and the next A block.\\nAs merges still require a separate buffer large enough to hold the A block to be merged, two areas within the array are reserved for this purpose (known as internal buffers). The first two A blocks are thus modified to contain the first instance of each value within A, with the original contents of those blocks shifted over if necessary. The remaining A blocks are then inserted into B and merged using one of the two buffers as swap space. This process causes the values in that buffer to be rearranged.\\nOnce every A and B block of every A and B subarray have been merged for that level of the merge sort, the values in that buffer must be sorted to restore their original order, so an insertion sort must be applied. The values in the buffers are then redistributed to their first sorted position within the array. This process repeats for each level of the outer bottom-up merge sort, at which point the array will have been stably sorted.\\n\\nAlgorithm\\nThe following operators are used in the code examples:\\n\\nAdditionally, block sort relies on the following operations as part of its overall algorithm:\\n\\nSwap: exchange the positions of two values in an array.\\nBlock swap: exchange a range of values within an array with values in a different range of the array.\\nBinary search: assuming the array is sorted, check the middle value of the current search range, then if the value is lesser check the lower range, and if the value is greater check the upper range. Block sort uses two variants: one which finds the first position to insert a value in the sorted array, and one which finds the last position.\\nLinear search: find a particular value in an array by checking every single element in order, until it is found.\\nInsertion sort: for each item in the array, loop backward and find where it needs to be inserted, then insert it at that position.\\nArray rotation: move the items in an array to the left or right by some number of spaces, with values on the edges wrapping around to the other side. Rotations can be implemented as three reversals.Rotate(array, amount, range)\\n    Reverse(array, range)\\n    Reverse(array, [range.start, range.start + amount))\\n    Reverse(array, [range.start + amount, range.end))\\n\\nFloor power of two: floor a value to the next power of two. Thus 63 becomes 32, 64 stays 64, and so forth.FloorPowerOfTwo(x)\\n    x = x | (x >> 1)\\n    x = x | (x >> 2)\\n    x = x | (x >> 4)\\n    x = x | (x >> 8)\\n    x = x | (x >> 16)\\n    if (this is a 64-bit system)\\n        x = x | (x >> 32)\\n    return x - (x >> 1)\\n\\nOuter loop\\nAs previously stated, the outer loop of a block sort is identical to a bottom-up merge sort. However, it benefits from the variant that ensures each A and B subarray are the same size to within one item:\\n\\n   BlockSort(array)\\n       power_of_two = FloorPowerOfTwo(array.size)\\n       scale = array.size/power_of_two // 1.0 \\u2264 scale < 2.0\\n      \\n       // insertion sort 16\\u201331 items at a time\\n       for (merge = 0; merge < power_of_two; merge += 16)\\n           start = merge * scale\\n           end = start + 16 * scale\\n           InsertionSort(array, [start, end))\\n      \\n       for (length = 16; length < power_of_two; length += length)\\n           for (merge = 0; merge < power_of_two; merge += length * 2)\\n               start = merge * scale\\n               mid = (merge + length) * scale\\n               end = (merge + length * 2) * scale\\n              \\n               if (array[end \\u2212 1] < array[start])\\n                   // the two ranges are in reverse order, so a rotation is enough to merge them\\n                   Rotate(array, mid \\u2212 start, [start, end))\\n               else if (array[mid \\u2212 1] > array[mid])\\n                   Merge(array, A = [start, mid), B = [mid, end))\\n               // else the ranges are already correctly ordered\\n\\nFixed-point math may also be used, by representing the scale factor as a fraction integer_part + numerator/denominator:\\n\\n   power_of_two = FloorPowerOfTwo(array.size)\\n   denominator = power_of_two/16\\n   numerator_step = array.size % denominator\\n   integer_step = floor(array.size/denominator)\\n  \\n   // insertion sort 16\\u201331 items at a time\\n  \\n   while (integer_step < array.size)\\n       integer_part = numerator = 0\\n       while (integer_part < array.size)\\n           // get the ranges for A and B\\n           start = integer_part\\n          \\n           integer_part += integer_step\\n           numerator += numerator_step\\n           if (numerator \\u2265 denominator)\\n               numerator \\u2212= denominator\\n               integer_part++\\n          \\n           mid = integer_part\\n          \\n           integer_part += integer_step\\n           numerator += numerator_step\\n           if (numerator \\u2265 denominator)\\n               numerator \\u2212= denominator\\n               integer_part++\\n          \\n           end = integer_part\\n          \\n           if (array[end \\u2212 1] < array[start])\\n               Rotate(array, mid \\u2212 start, [start, end))\\n           else if (array[mid \\u2212 1] > array[mid])\\n               Merge(array, A = [start, mid), B = [mid, end))\\n      \\n       integer_step += integer_step\\n       numerator_step += numerator_step\\n       if (numerator_step \\u2265 denominator)\\n           numerator_step \\u2212= denominator\\n           integer_step++\\n\\nExtract buffers\\nThe two internal buffers needed for each level of the merge step are created by moving the first 2\\u221aA first instances of their values within an A subarray to the start of A. First it iterates over the elements in A and counts off the unique values it needs, then it applies array rotations to move those unique values to the start. If A did not contain enough unique values to fill the two buffers (of size \\u221aA each), B can be used just as well. In this case it moves the last instance of each value to the end of B, with that part of B not being included during the merges.\\n\\nwhile (integer_step < array.size)\\n    block_size = \\u221ainteger_step\\n    buffer_size = integer_step/block_size + 1\\n    [extract two buffers of size 'buffer_size' each]\\n\\nIf B does not contain enough unique values either, it pulls out the largest number of unique values it could find, then adjusts the size of the A and B blocks such that the number of resulting A blocks is less than or equal to the number of unique items pulled out for the buffer. Only one buffer will be used in this case \\u2013 the second buffer won't exist.\\n\\nbuffer_size = [number of unique values found]\\nblock_size = integer_step/buffer_size + 1\\n  \\ninteger_part = numerator = 0\\nwhile (integer_part < array.size)\\n    [get the ranges for A and B]\\n    [adjust A and B to not include the ranges used by the buffers]\\n\\nTag A blocks\\nOnce the one or two internal buffers have been created, it begins merging each A and B subarray for this level of the merge sort. To do so, it divides each A and B subarray into evenly sized blocks of the size calculated in the previous step, where the first A block and last B block are unevenly sized if needed. It then loops over each of the evenly sized A blocks and swaps the second value with a corresponding value from the first of the two internal buffers. This is known as tagging the blocks.\\n\\n// blockA is the range of the remaining A blocks,\\n// and firstA is the unevenly sized first A block\\nblockA = [A.start, A.end)\\nfirstA = [A.start, A.start + |blockA| % block_size)\\n  \\n// swap the second value of each A block with the value in buffer1\\nfor (index = 0, indexA = firstA.end + 1; indexA < blockA.end; indexA += block_size)\\n    Swap(array[buffer1.start + index], array[indexA])\\n    index++\\n  \\nlastA = firstA\\nblockB = [B.start, B.start + minimum(block_size, |B|))\\nblockA.start += |firstA|\\n\\nRoll and drop\\nAfter defining and tagging the A blocks in this manner, the A blocks are rolled through the B blocks by block swapping the first evenly sized A block with the next B block. This process repeats until the first value of the A block with the smallest tag value is less than or equal to the last value of the B block that was just swapped with an A block.\\nAt that point, the minimum A block (the A block with the smallest tag value) is swapped to the start of the rolling A blocks and the tagged value is restored with its original value from the first buffer. This is known as dropping a block behind, as it will no longer be rolled along with the remaining A blocks. That A block is then inserted into the previous B block, first by using a binary search on B to find the index where the first value of A is less than or equal to the value at that index of B, and then by rotating A into B at that index.\\n\\n   minA = blockA.start\\n   indexA = 0\\n  \\n   while (true)\\n       // if there's a previous B block and the first value of the minimum A block is \\u2264\\n       // the last value of the previous B block, then drop that minimum A block behind.\\n       // or if there are no B blocks left then keep dropping the remaining A blocks.\\n       if ((|lastB| > 0 and array[lastB.end - 1] \\u2265 array[minA]) or |blockB| = 0)\\n           // figure out where to split the previous B block, and rotate it at the split\\n           B_split = BinaryFirst(array, array[minA], lastB)\\n           B_remaining = lastB.end - B_split\\n          \\n           // swap the minimum A block to the beginning of the rolling A blocks\\n           BlockSwap(array, blockA.start, minA, block_size)\\n          \\n           // restore the second value for the A block\\n           Swap(array[blockA.start + 1], array[buffer1.start + indexA])\\n           indexA++\\n          \\n           // rotate the A block into the previous B block\\n           Rotate(array, blockA.start - B_split, [B_split, blockA.start + block_size))\\n          \\n           // locally merge the previous A block with the B values that follow it,\\n           // using the second internal buffer as swap space (if it exists)\\n           if (|buffer2| > 0)\\n               MergeInternal(array, lastA, [lastA.end, B_split), buffer2)\\n           else\\n               MergeInPlace(array, lastA, [lastA.end, B_split))\\n          \\n           // update the range for the remaining A blocks,\\n           // and the range remaining from the B block after it was split\\n           lastA = [blockA.start - B_remaining, blockA.start - B_remaining + block_size)\\n           lastB = [lastA.end, lastA.end + B_remaining)\\n          \\n           // if there are no more A blocks remaining, this step is finished\\n           blockA.start = blockA.start + block_size\\n           if (|blockA| = 0)\\n               break\\n          \\n           minA = [new minimum A block] (see below)\\n       else if (|blockB| < block_size)\\n           // move the last B block, which is unevenly sized,\\n           // to before the remaining A blocks, by using a rotation\\n           Rotate(array, blockB.start - blockA.start, [blockA.start, blockB.end))\\n          \\n           lastB = [blockA.start, blockA.start + |blockB|)\\n           blockA.start += |blockB|\\n           blockA.end += |blockB|\\n           minA += |blockB|\\n           blockB.end = blockB.start\\n       else\\n           // roll the leftmost A block to the end by swapping it with the next B block\\n           BlockSwap(array, blockA.start, blockB.start, block_size)\\n           lastB = [blockA.start, blockA.start + block_size)\\n           if (minA = blockA.start)\\n               minA = blockA.end\\n          \\n           blockA.start += block_size\\n           blockA.end += block_size\\n           blockB.start += block_size\\n          \\n           // this is equivalent to minimum(blockB.end + block_size, B.end),\\n           // but that has the potential to overflow\\n           if (blockB.end > B.end - block_size)\\n               blockB.end = B.end\\n           else\\n               blockB.end += block_size\\n  \\n   // merge the last A block with the remaining B values\\n   if (|buffer2| > 0)\\n       MergeInternal(array, lastA, [lastA.end, B.end), buffer2)\\n   else\\n       MergeInPlace(array, lastA, [lastA.end, B.end))\\n\\nOne optimization that can be applied during this step is the floating-hole technique. When the minimum A block is dropped behind and needs to be rotated into the previous B block, after which its contents are swapped into the second internal buffer for the local merges, it would be faster to swap the A block to the buffer beforehand, and to take advantage of the fact that the contents of that buffer do not need to retain any order. So rather than rotating the second buffer (which used to be the A block before the block swap) into the previous B block at position index, the values in the B block after index can simply be block swapped with the last items of the buffer.\\nThe floating hole in this case refers to the contents of the second internal buffer floating around the array, and acting as a hole in the sense that the items do not need to retain their order.\\n\\nLocal merges\\nOnce the A block has been rotated into the B block, the previous A block is then merged with the B values that follow it, using the second buffer as swap space. When the first A block is dropped behind this refers to the unevenly sized A block at the start, when the second A block is dropped behind it means the first A block, and so forth.\\n\\nMergeInternal(array, A, B, buffer)\\n    // block swap the values in A with those in 'buffer'\\n    BlockSwap(array, A.start, buffer.start, |A|)\\n\\n    A_count = 0, B_count = 0, insert = 0\\n    while (A_count < |A| and B_count < |B|)\\n        if (array[buffer.start + A_count] \\u2264 array[B.start + B_count])\\n            Swap(array[A.start + insert], array[buffer.start + A_count])\\n            A_count++\\n        else\\n            Swap(array[A.start + insert], array[B.start + B_count])\\n            B_count++\\n        insert++\\n\\n    // block swap the remaining part of the buffer with the remaining part of the array\\n    BlockSwap(array, buffer.start + A_count, A.start + insert, |A| - A_count)\\n\\nIf the second buffer does not exist, a strictly in-place merge operation must be performed, such as a rotation-based version of the Hwang and Lin algorithm, the Dudzinski and Dydek algorithm, or a repeated binary search and rotate.\\n\\nMergeInPlace(array, A, B)\\n    while (|A| > 0 and |B| > 0)\\n        // find the first place in B where the first item in A needs to be inserted\\n        mid = BinaryFirst(array, array[A.start], B)\\n\\n        // rotate A into place\\n        amount = mid - A.end\\n        Rotate(array, amount, [A.start, mid))\\n\\n        // calculate the new A and B ranges\\n        B = [mid, B.end)\\n        A = [A.start + amount, mid)\\n        A.start = BinaryLast(array, array[A.start], A)\\n\\nAfter dropping the minimum A block and merging the previous A block with the B values that follow it, the new minimum A block must be found within the blocks that are still being rolled through the array. This is handled by running a linear search through those A blocks and comparing the tag values to find the smallest one.\\n\\nminA = blockA.start\\nfor (findA = minA + block_size; findA < blockA.end - 1; findA += block_size)\\n    if (array[findA + 1] < array[minA + 1])\\n        minA = findA\\n\\nThese remaining A blocks then continue rolling through the array and being dropped and inserted where they belong. This process repeats until all of the A blocks have been dropped and rotated into the previous B block.\\nOnce the last remaining A block has been dropped behind and inserted into B where it belongs, it should be merged with the remaining B values that follow it. This completes the merge process for that particular pair of A and B subarrays. However, it must then repeat the process for the remaining A and B subarrays for the current level of the merge sort.\\nNote that the internal buffers can be reused for every set of A and B subarrays for this level of the merge sort, and do not need to be re-extracted or modified in any way.\\n\\nRedistribute\\nAfter all of the A and B subarrays have been merged, the one or two internal buffers are still left over. The first internal buffer was used for tagging the A blocks, and its contents are still in the same order as before, but the second internal buffer may have had its contents rearranged when it was used as swap space for the merges. This means the contents of the second buffer will need to be sorted using a different algorithm, such as insertion sort. The two buffers must then be redistributed back into the array using the opposite process that was used to create them.\\nAfter repeating these steps for every level of the bottom-up merge sort, the block sort is completed.\\n\\nVariants\\nBlock sort works by extracting two internal buffers, breaking A and B subarrays into evenly sized blocks, rolling and dropping the A blocks into B (using the first buffer to track the order of the A blocks), locally merging using the second buffer as swap space, sorting the second buffer, and redistributing both buffers. While the steps do not change, these subsystems can vary in their actual implementation.\\nOne variant of block sort allows it to use any amount of additional memory provided to it, by using this external buffer for merging an A subarray or A block with B whenever A fits into it. In this situation it would be identical to a merge sort.\\nGood choices for the buffer size include:\\n\\nRather than tagging the A blocks using the contents of one of the internal buffers, an indirect movement-imitation buffer can be used instead. This is an internal buffer defined as s1 t s2, where s1 and s2 are each as large as the number of A and B blocks, and t contains any values immediately following s1 that are equal to the last value of s1 (thus ensuring that no value in s2 appears in s1). A second internal buffer containing \\u221aA unique values is still used. The first \\u221aA values of s1 and s2 are then swapped with each other to encode information into the buffer about which blocks are A blocks and which are B blocks. When an A block at index i is swapped with a B block at index j (where the first evenly sized A block is initially at index 0), s1[i] and s1[j] are swapped with s2[i] and s2[j], respectively. This imitates the movements of the A blocks through B. The unique values in the second buffer are used to determine the original order of the A blocks as they are rolled through the B blocks. Once all of the A blocks have been dropped, the movement-imitation buffer is used to decode whether a given block in the array is an A block or a B block, each A block is rotated into B, and the second internal buffer is used as swap space for the local merges.\\nThe second value of each A block doesn't necessarily need to be tagged \\u2013 the first, last, or any other element could be used instead. However, if the first value is tagged, the values will need to be read from the first internal buffer (where they were swapped) when deciding where to drop the minimum A block.\\nMany sorting algorithms can be used to sort the contents of the second internal buffer, including unstable sorts like quicksort, since the contents of the buffer are guaranteed to unique. Insertion sort is still recommended, though, for its situational performance and lack of recursion.\\n\\nAnalysis\\nBlock sort is a well-defined and testable class of algorithms, with working implementations available as a merge and as a sort. This allows its characteristics to be measured and considered.\\n\\nComplexity\\nBlock sort begins by performing insertion sort on groups of 16\\u201331 items in the array. Insertion sort is an \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   operation, so this leads to anywhere from \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          16\\n          \\n            2\\n          \\n        \\n        \\u00d7\\n        n\\n        \\n          /\\n        \\n        16\\n        )\\n      \\n    \\n    {\\\\displaystyle O(16^{2}\\\\times n/16)}\\n   to \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          31\\n          \\n            2\\n          \\n        \\n        \\u00d7\\n        n\\n        \\n          /\\n        \\n        31\\n        )\\n      \\n    \\n    {\\\\displaystyle O(31^{2}\\\\times n/31)}\\n  , which is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   once the constant factors are omitted. It must also apply an insertion sort on the second internal buffer after each level of merging is completed. However, as this buffer was limited to \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   in size, the \\n  \\n    \\n      \\n        O\\n        \\n          \\n            \\n              n\\n            \\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle O{\\\\sqrt {n}}^{2}}\\n   operation also ends up being \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\nNext it must extract two internal buffers for each level of the merge sort. It does so by iterating over the items in the A and B subarrays and incrementing a counter whenever the value changes, and upon finding enough values it rotates them to the start of A or the end of B. In the worst case this will end up searching the entire array before finding \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   non-contiguous unique values, which requires \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   comparisons and \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   rotations for \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   values. This resolves to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        +\\n        \\n          \\n            n\\n          \\n        \\n        \\u00d7\\n        \\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n+{\\\\sqrt {n}}\\\\times {\\\\sqrt {n}})}\\n  , or \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\nWhen none of the A or B subarrays contained \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   unique values to create the internal buffers, a normally suboptimal in-place merge operation is performed where it repeatedly binary searches and rotates A into B. However, the known lack of unique values within any of the subarrays places a hard limit on the number of binary searches and rotations that will be performed during this step, which is again \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   items rotated up to \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times, or \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  . The size of each block is also adjusted to be smaller in the case where it found \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   unique values but not 2\\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n  , which further limits the number of unique values contained within any A or B block.\\nTagging the A blocks is performed \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times for each A subarray, then the A blocks are rolled through and inserted into the B blocks up to \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times. The local merges retain the same \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   complexity of a standard merge, albeit with more assignments since the values must be swapped rather than copied. The linear search for finding the new minimum A block iterates over \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   blocks \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times. And the buffer redistribution process is identical to the buffer extraction but in reverse, and therefore has the same \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   complexity.\\nAfter omitting all but the highest complexity and considering that there are \\n  \\n    \\n      \\n        log\\n        \\u2061\\n        \\n          n\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\log {n}}\\n   levels in the outer merge loop, this leads to a final asymptotic complexity of \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        \\n          n\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log {n})}\\n   for the worst and average cases. For the best case, where the data is already in order, the merge step performs  \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        16\\n      \\n    \\n    {\\\\displaystyle n/16}\\n   comparisons for the first level, then \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        32\\n      \\n    \\n    {\\\\displaystyle n/32}\\n  , \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        64\\n      \\n    \\n    {\\\\displaystyle n/64}\\n  , \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        128\\n      \\n    \\n    {\\\\displaystyle n/128}\\n  , etc. This is a well-known mathematical series which resolves to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\n\\nMemory\\nAs block sort is non-recursive and does not require the use of dynamic allocations, this leads to constant stack and heap space. It uses O(1) auxiliary memory in a transdichotomous model, which accepts that the O(log n) bits needed to keep track of the ranges for A and B cannot be any greater than 32 or 64 on 32-bit or 64-bit computing systems, respectively, and therefore simplifies to O(1) space for any array that can feasibly be allocated.\\n\\nStability\\nAlthough items in the array are moved out of order during a block sort, each operation is fully reversible and will have restored the original order of equivalent items by its completion.\\nStability requires the first instance of each value in an array before sorting to still be the first instance of that value after sorting. Block sort moves these first instances to the start of the array to create the two internal buffers, but when all of the merges are completed for the current level of the block sort, those values are distributed back to the first sorted position within the array. This maintains stability.\\nBefore rolling the A blocks through the B blocks, each A block has its second value swapped with a value from the first buffer. At that point the A blocks are moved out of order to roll through the B blocks. However, once it finds where it should insert the smallest A block into the previous B block, that smallest A block is moved back to the start of the A blocks and its second value is restored. By the time all of the A blocks have been inserted, the A blocks will be in order again and the first buffer will contain its original values in the original order.\\nUsing the second buffer as swap space when merging an A block with some B values causes the contents of that buffer to be rearranged. However, as the algorithm already ensured the buffer only contains unique values, sorting the contents of the buffer is sufficient to restore their original stable order.\\n\\nAdaptivity\\nBlock sort is an adaptive sort on two levels: first, it skips merging A and B subarrays that are already in order. Next, when A and B need to be merged and are broken into evenly sized blocks, the A blocks are only rolled through B as far as is necessary, and each block is only merged with the B values immediately following it. The more ordered the data originally was, the fewer B values there will be that need to be merged into A.\\n\\nAdvantages\\nBlock sort is a stable sort that does not require additional memory, which is useful in cases where there is not enough free memory to allocate the O(n) buffer. When using the external buffer variant of block sort, it can scale from using O(n) memory to progressively smaller buffers as needed, and will still work efficiently within those constraints.\\n\\nDisadvantages\\nBlock sort does not exploit sorted ranges of data on as fine a level as some other algorithms, such as Timsort. It only checks for these sorted ranges at the two predefined levels: the A and B subarrays, and the A and B blocks. It is also harder to implement and parallelize compared to a merge sort.\\n\\n\\n== References ==\"}, {\"Block swap algorithms\": \"In computer algorithms, Block swap algorithms swap two regions of elements of an array. It is simple to swap two non-overlapping regions of an array of equal size. However, it is not simple to swap two non-overlapping regions of an array in-place that are next to each other, but are of unequal sizes (such swapping is equivalent to Array Rotation). Three algorithms are known to accomplish this: Bentley's Juggling (also known as Dolphin Algorithm ), Gries-Mills, and Reversal. All three algorithms are linear time \\nO(n), (see Time complexity).\\n\\nReversal algorithm\\nThe reversal algorithm is the simplest to explain, using rotations. A rotation is an in-place reversal of array elements. This method swaps two elements of an array from outside in within a range. The rotation works for an even or odd number of array elements. The reversal algorithm uses three in-place rotations to accomplish an in-place block swap:\\n\\nRotate region A\\nRotate region B\\nRotate region ABGries-Mills and Reversal algorithms perform better than Bentley's Juggling, because of their cache-friendly memory access pattern behavior.\\nThe Reversal algorithm parallelizes well, because rotations can be split into sub-regions, which can be rotated independently of others.\\n\\n\\n== References ==\"}, {\"Bogosort\": \"In computer science, bogosort (also known as permutation sort, stupid sort, slowsort or bozosort) is a sorting algorithm based on the generate and test paradigm. The function successively generates permutations of its input until it finds one that is sorted. It is not considered useful for sorting, but may be used for educational purposes, to contrast it with more efficient algorithms.\\nTwo versions of this algorithm exist: a deterministic version that enumerates all permutations until it hits a sorted one, and a randomized version that randomly permutes its input. An analogy for the working of the latter version is to sort a deck of cards by throwing the deck into the air, picking the cards up at random, and repeating the process until the deck is sorted. Its name is a portmanteau of the words bogus and sort.\\n\\nDescription of the algorithm\\nThe following is a description of the randomized algorithm in pseudocode:\\n\\nwhile not sorted(deck):\\n    shuffle(deck)\\n\\nHere is the above pseudocode rewritten in Python 3:\\n\\nThis code assumes that data is a simple, mutable, array-like data structure\\u2014like Python's built-in list\\u2014whose elements can be compared without issue.\\n\\nRunning time and termination\\nIf all elements to be sorted are distinct, the expected number of comparisons performed in the average case by randomized bogosort is asymptotically equivalent to (e \\u2212 1)n!, and the expected number of swaps in the average case equals (n \\u2212 1)n!. The expected number of swaps grows faster than the expected number of comparisons, because if the elements are not in order, this will usually be discovered after only a few comparisons, no matter how many elements there are; but the work of shuffling the collection is proportional to its size. In the worst case, the number of comparisons and swaps are both unbounded, for the same reason that a tossed coin might turn up heads any number of times in a row.\\nThe best case occurs if the list as given is already sorted; in this case the expected number of comparisons is n \\u2212 1, and no swaps at all are carried out.For any collection of fixed size, the expected running time of the algorithm is finite for much the same reason that the infinite monkey theorem holds: there is some probability of getting the right permutation, so given an unbounded number of tries it will almost surely eventually be chosen.\\n\\nRelated algorithms\\nGorosort\\nis a sorting algorithm introduced in the 2011 Google Code Jam. As long as the list is not in order, a subset of all elements is randomly permuted. If this subset is optimally chosen each time this is performed, the expected value of the total number of times this operation needs to be done is equal to the number of misplaced elements.\\nBogobogosort\\nis an algorithm that was designed not to succeed before the heat death of the universe on any sizable list. It works by recursively calling itself with smaller and smaller copies of the beginning of the list to see if they are sorted.  The base case is a single element, which is always sorted.  For other cases, it compares the last element to the maximum element from the previous elements in the list.  If the last element is greater or equal, it checks if the order of the copy matches the previous version, and if so returns.  Otherwise, it reshuffles the current copy of the list and restarts its recursive check.\\nBozosort\\nis another sorting algorithm based on random numbers. If the list is not in order, it picks two items at random and swaps them, then checks to see if the list is sorted. The running time analysis of a bozosort is more difficult, but some estimates are found in H. Gruber's analysis of \\\"perversely awful\\\" randomized sorting algorithms. O(n!) is found to be the expected average case.\\nWorstsort\\nis a pessimal sorting algorithm that is guaranteed to complete in finite time; however, its efficiency can be arbitrarily bad, depending on its configuration. The worstsort algorithm is based on a bad sorting algorithm, badsort. The badsort algorithm accepts two parameters: L, which is the list to be sorted, and k, which is a recursion depth. At recursion level k = 0, badsort merely uses a common sorting algorithm, such as bubblesort, to sort its inputs and return the sorted list. That is to say, badsort(L, 0) = bubblesort(L). Therefore, badsort's time complexity is O(n2) if k = 0. However, for any k > 0, badsort(L, k) first generates P, the list of all permutations of L. Then, badsort calculates badsort(P, k \\u2212 1), and returns the first element of the sorted P. To make worstsort truly pessimal, k may be assigned to the value of a computable increasing function such as \\n  \\n    \\n      \\n        f\\n        :\\n        \\n          N\\n        \\n        \\u2192\\n        \\n          N\\n        \\n      \\n    \\n    {\\\\displaystyle f\\\\colon \\\\mathbb {N} \\\\to \\\\mathbb {N} }\\n   (e.g. f(n) = A(n, n), where A is Ackermann's function).  Ergo, to sort a list arbitrarily badly, you would execute worstsort(L, f) = badsort(L, f(length(L))), where length(L) is the number of elements in L. The resulting algorithm has complexity \\n  \\n    \\n      \\n        \\u03a9\\n        \\n          (\\n          \\n            \\n              (\\n              \\n                n\\n                \\n                  !\\n                  \\n                    (\\n                    f\\n                    (\\n                    n\\n                    )\\n                    )\\n                  \\n                \\n              \\n              )\\n            \\n            \\n              2\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\textstyle \\\\Omega \\\\left(\\\\left(n!^{(f(n))}\\\\right)^{2}\\\\right)}\\n  , where \\n  \\n    \\n      \\n        n\\n        \\n          !\\n          \\n            (\\n            m\\n            )\\n          \\n        \\n        =\\n        (\\n        \\u2026\\n        (\\n        (\\n        n\\n        !\\n        )\\n        !\\n        )\\n        !\\n        \\u2026\\n        )\\n        !\\n      \\n    \\n    {\\\\displaystyle n!^{(m)}=(\\\\dotso ((n!)!)!\\\\dotso )!}\\n   = factorial of n iterated m times. This algorithm can be made as inefficient as one wishes by picking a fast enough growing function f.\\nSlowsort\\nis a different humorous sorting algorithm that employs a misguided divide-and-conquer strategy to achieve massive complexity.\\nQuantum Bogosort\\nis a hypothetical sorting algorithm based on bogosort, created as an in-joke among computer scientists. The algorithm generates a random permutation of its input using a quantum source of entropy, checks if the list is sorted, and, if it is not, destroys the universe. Assuming that the many-worlds interpretation holds, the use of this algorithm will result in at least one surviving universe where the input was successfully sorted in O(n) time.\\nMiracle sort\\nis a sorting algorithm that checks if the array is sorted until a miracle occurs. It continually checks the array until it is sorted, never changing the order of the array. Because the order is never altered, the algorithm has a hypothetical time complexity of O(\\u221e), but it can still sort through events such as miracles or single-event upsets. Particular care must be taken in the implementation of this algorithm as optimizing compilers may simply transform it into a while(true) loop.\\n\\nSee also\\nLas Vegas algorithm\\nStooge sort\\n\\nNotes\\nReferences\\nExternal links\\n\\nBogoSort on WikiWikiWeb\\nInefficient sort algorithms\\nBogosort: an implementation that runs on Unix-like systems, similar to the standard sort program.\\nBogosort and jmmcg::bogosort: Simple, yet perverse, C++ implementations of the bogosort algorithm.\\nBogosort NPM package: bogosort implementation for Node.js ecosystem.\\nMax Sherman Bogo-sort is Sort of Slow, June 2013\"}, {\"Bubble sort\": \"Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the input list element by element, comparing the current element with the one after it, swapping their values if needed. These passes through the list are repeated until no swaps had to be performed during a pass, meaning that the list has become fully sorted. The algorithm, which is a comparison sort, is named for the way the larger elements \\\"bubble\\\" up to the top of the list. \\nThis simple algorithm performs poorly in real world use and is used primarily as an educational tool. More efficient algorithms such as quicksort, timsort, or merge sort are used by the sorting libraries built into popular programming languages such as Python and Java.\\n\\nAnalysis\\nPerformance\\nBubble sort has a worst-case and average complexity of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  , where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of items being sorted. Most practical sorting algorithms have substantially better worst-case or average complexity, often \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n  . Even other \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   sorting algorithms, such as insertion sort, generally run faster than bubble sort, and are no more complex. For this reason, bubble sort is rarely used in practice.\\nLike insertion sort, bubble sort is adaptive, giving it an advantage over algorithms like quicksort. This means that it may outperform those algorithms in cases where the list is already mostly sorted (having a small number of inversions), despite the fact that it has worse average-case time complexity. For example, bubble sort is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   on a list that is already sorted, while quicksort would still perform its entire \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   sorting process.\\nWhile any sorting algorithm can be made \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   on a presorted list simply by checking the list before the algorithm runs, improved performance on almost-sorted lists is harder to replicate.\\n\\nRabbits and turtles\\nThe distance and direction that elements must move during the sort determine bubble sort's performance because elements move in different directions at different speeds. An element that must move toward the end of the list can move quickly because it can take part in successive swaps. For example, the largest element in the list will win every swap, so it moves to its sorted position on the first pass even if it starts near the beginning. On the other hand, an element that must move toward the beginning of the list cannot move faster than one step per pass, so elements move toward the beginning very slowly. If the smallest element is at the end of the list, it will take \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   passes to move it to the beginning. This has led to these types of elements being named rabbits and turtles, respectively, after the characters in Aesop's fable of The Tortoise and the Hare.\\nVarious efforts have been made to eliminate turtles to improve upon the speed of bubble sort. Cocktail sort is a bi-directional bubble sort that goes from beginning to end, and then reverses itself, going end to beginning. It can move turtles fairly well, but it retains \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   worst-case complexity. Comb sort compares elements separated by large gaps, and can move turtles extremely quickly before proceeding to smaller and smaller gaps to smooth out the list. Its average speed is comparable to faster algorithms like quicksort.\\n\\nStep-by-step example\\nTake an array of numbers \\\"5 1 4 2 8\\\", and sort the array from lowest number to greatest number using bubble sort. In each step, elements written in bold are being compared. Three passes will be required;\\n\\nFirst Pass\\n( 5 1 4 2 8 ) \\u2192 ( 1 5 4 2 8 ), Here, algorithm compares the first two elements, and swaps since 5 > 1.\\n( 1 5 4 2 8 ) \\u2192 ( 1 4 5 2 8 ), Swap since 5 > 4\\n( 1 4 5 2 8 ) \\u2192 ( 1 4 2 5 8 ), Swap since 5 > 2\\n( 1 4 2 5 8 ) \\u2192 ( 1 4 2 5 8 ), Now, since these elements are already in order (8 > 5), algorithm does not swap them.\\nSecond Pass\\n( 1 4 2 5 8 ) \\u2192 ( 1 4 2 5 8 )\\n( 1 4 2 5 8 ) \\u2192 ( 1 2 4 5 8 ), Swap since 4 > 2\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )Now, the array is already sorted, but the algorithm does not know if it is completed. The algorithm needs one additional whole pass without any swap to know it is sorted.\\n\\nThird Pass\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n\\nImplementation\\nPseudocode implementation\\nIn pseudocode the algorithm can be expressed as (0-based array):\\n\\nOptimizing bubble sort\\nThe bubble sort algorithm can be optimized by observing that the n-th pass finds the n-th largest element and puts it into its final place. So, the inner loop can avoid looking at the last n \\u2212 1 items when running for the n-th time:\\n\\nMore generally, it can happen that more than one element is placed in their final position on a single pass. In particular, after every pass, all elements after the last swap are sorted, and do not need to be checked again. This allows to skip over many elements, resulting in about a worst case 50% improvement in comparison count (though no improvement in swap counts), and adds very little complexity because the new code subsumes the \\\"swapped\\\" variable:\\nTo accomplish this in pseudocode, the following can be written:\\n\\nAlternate modifications, such as the cocktail shaker sort attempt to improve on the bubble sort performance while keeping the same idea of repeatedly comparing and swapping adjacent items.\\n\\nUse\\nAlthough bubble sort is one of the simplest sorting algorithms to understand and implement, its O(n2) complexity means that its efficiency decreases dramatically on lists of more than a small number of elements. Even among simple O(n2) sorting algorithms, algorithms like insertion sort are usually considerably more efficient.\\nDue to its simplicity, bubble sort is often used to introduce the concept of an algorithm, or a sorting algorithm, to introductory computer science students. However, some researchers such as Owen Astrachan have gone to great lengths to disparage bubble sort and its continued popularity in computer science education, recommending that it no longer even be taught.The Jargon File, which famously calls bogosort \\\"the archetypical [sic] perversely awful algorithm\\\", also calls bubble sort \\\"the generic bad algorithm\\\". Donald Knuth, in The Art of Computer Programming, concluded that \\\"the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems\\\", some of which he then discusses.Bubble sort is asymptotically equivalent in running time to insertion sort in the worst case, but the two algorithms differ greatly in the number of swaps necessary. Experimental results such as those of Astrachan have also shown that insertion sort performs considerably better even on random lists. For these reasons many modern algorithm textbooks avoid using the bubble sort algorithm in favor of insertion sort.\\nBubble sort also interacts poorly with modern CPU hardware. It produces at least twice as many writes as insertion sort, twice as many cache misses, and asymptotically more branch mispredictions. Experiments by Astrachan sorting strings in Java show bubble sort to be roughly one-fifth as fast as an insertion sort and 70% as fast as a selection sort.In computer graphics bubble sort is popular for its capability to detect a very small error (like swap of just two elements) in almost-sorted arrays and fix it with just linear complexity (2n). For example, it is used in a polygon filling algorithm, where bounding lines are sorted by their x coordinate at a specific scan line (a line parallel to the x axis) and with incrementing y their order changes (two elements are swapped) only at intersections of two lines. Bubble sort is a stable sort algorithm, like insertion sort.\\n\\nVariations\\nOdd\\u2013even sort is a parallel version of bubble sort, for message passing systems.\\nPasses can be from right to left, rather than left to right. This is more efficient for lists with unsorted items added to the end.\\nCocktail shaker sort alternates leftwards and rightwards passes.\\nI can't believe it can sort is a sorting algorithm that appears to be an incorrect version of bubble sort, but can be formally proven to work in a way more akin to insertion sort.\\n\\nDebate over name\\nBubble sort has been occasionally referred to as a \\\"sinking sort\\\".For example, Donald Knuth describes the insertion of values at or towards their desired location as letting \\\"[the value] settle to its proper level\\\", and that \\\"this method of sorting has sometimes been called the sifting or sinking technique.This debate is perpetuated by the ease with which one may consider this algorithm from two different but equally valid perspectives:\\n\\nThe larger values might be regarded as heavier and therefore be seen to progressively sink to the bottom of the list\\nThe smaller values might be regarded as lighter and therefore be seen to progressively bubble up to the top of the list.\\n\\nIn popular culture\\nIn 2007, former Google CEO Eric Schmidt asked then-presidential candidate Barack Obama during an interview about the best way to sort one million integers; Obama paused for a moment and replied: \\\"I think the bubble sort would be the wrong way to go.\\\"\\n\\nNotes\\nReferences\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Problem 2-2, pg.40.\\nSorting in the Presence of Branch Prediction and Caches\\nFundamentals of Data Structures by Ellis Horowitz, Sartaj Sahni and Susan Anderson-Freed ISBN 81-7371-605-6\\nOwen Astrachan. Bubble Sort: An Archaeological Algorithmic Analysis\\nComputer Integrated Manufacturing by Spasic PhD, Srdic MSc, Open Source, 1987.[1]\\n\\nExternal links\\n\\nMartin, David R. (2007). \\\"Animated Sorting Algorithms: Bubble Sort\\\". Archived from the original on 2015-03-03. \\u2013 graphical demonstration\\n\\\"Lafore's Bubble Sort\\\". (Java applet animation)\\nOEIS sequence A008302 (Table (statistics) of the number of permutations of [n] that need k pair-swaps during the sorting)\"}, {\"Bucket sort\": \"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort,  a generalization of pigeonhole sort that allows multiple keys per bucket, and is a cousin of radix sort in the most-to-least significant digit flavor. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity depends on the algorithm used to sort each bucket, the number of buckets to use, and whether the input is uniformly distributed.\\nBucket sort works as follows:\\n\\nSet up an array of initially empty \\\"buckets\\\".\\nScatter: Go over the original array, putting each object in its bucket.\\nSort each non-empty bucket.\\nGather: Visit the buckets in order and put all elements back into the original array.\\n\\nPseudocode\\nfunction bucketSort(array, k) is\\n    buckets \\u2190 new array of k empty lists\\n    M \\u2190 1 + the maximum key value in the array\\n    for i = 0 to length(array) do\\n        insert array[i] into buckets[floor(k \\u00d7 array[i] / M)]\\n    for i = 0 to k do \\n        nextSort(buckets[i])\\n    return the concatenation of buckets[0], ...., buckets[k]\\n\\nLet array denote the array to be sorted and k denote the number of buckets to use. One can compute the maximum key value in linear time by iterating over all the keys once. The floor function must be used to convert a floating number to an integer ( and possibly casting of datatypes too ). The function nextSort is a sorting function used to sort each bucket. Conventionally, insertion sort is used, but other algorithms could be used as well, such as selection sort or merge sort. Using bucketSort itself as nextSort produces a relative of radix sort; in particular, the case n = 2 corresponds to quicksort (although potentially with poor pivot choices).\\n\\nAnalysis\\nWorst-case analysis\\nWhen the input contains several keys that are close to each other (clustering), those elements are likely to be placed in the same bucket, which results in some buckets containing more elements than average. The worst-case scenario occurs when all the elements are placed in a single bucket. The overall performance would then be dominated by the algorithm used to sort each bucket, for example \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   insertion sort or \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        (\\n        n\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log(n))}\\n   comparison sort algorithms, such as merge sort.\\n\\nAverage-case analysis\\nConsider the case that the input is uniformly distributed. The first step, which is initialize the buckets and find the maximum key value in the array, can be done in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time. If division and multiplication can be done in constant time, then scattering each element to its bucket also costs \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  . Assume insertion sort is used to sort each bucket, then the third step costs \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          \\n            \\u2211\\n            \\n              i\\n              =\\n              1\\n            \\n            \\n              k\\n            \\n          \\n          \\n            \\n              n\\n              \\n                i\\n              \\n              \\n                2\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle O(\\\\textstyle \\\\sum _{i=1}^{k}\\\\displaystyle n_{i}^{2})}\\n  , where \\n  \\n    \\n      \\n        \\n          n\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n_{i}}\\n   is the length of the bucket indexed \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  . Since we are concerning the average time, the expectation \\n  \\n    \\n      \\n        E\\n        (\\n        \\n          n\\n          \\n            i\\n          \\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle E(n_{i}^{2})}\\n   has to be evaluated instead. Let \\n  \\n    \\n      \\n        \\n          X\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X_{ij}}\\n   be the random variable that is \\n  \\n    \\n      \\n        1\\n      \\n    \\n    {\\\\displaystyle 1}\\n   if element \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n   is placed in bucket \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , and \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n   otherwise. We have \\n  \\n    \\n      \\n        \\n          n\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          \\u2211\\n          \\n            j\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          X\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n_{i}=\\\\sum _{j=1}^{n}X_{ij}}\\n  . Therefore,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                E\\n                (\\n                \\n                  n\\n                  \\n                    i\\n                  \\n                  \\n                    2\\n                  \\n                \\n                )\\n              \\n              \\n                \\n                =\\n                E\\n                \\n                  (\\n                  \\n                    \\n                      \\u2211\\n                      \\n                        j\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        j\\n                      \\n                    \\n                    \\n                      \\u2211\\n                      \\n                        k\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        k\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                E\\n                \\n                  (\\n                  \\n                    \\n                      \\u2211\\n                      \\n                        j\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      \\u2211\\n                      \\n                        k\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        j\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        k\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                E\\n                \\n                  (\\n                  \\n                    \\n                      \\u2211\\n                      \\n                        j\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        j\\n                      \\n                      \\n                        2\\n                      \\n                    \\n                  \\n                  )\\n                \\n                +\\n                E\\n                \\n                  (\\n                  \\n                    \\n                      \\u2211\\n                      \\n                        1\\n                        \\u2264\\n                        j\\n                        ,\\n                        k\\n                        \\u2264\\n                        n\\n                      \\n                    \\n                    \\n                      \\u2211\\n                      \\n                        j\\n                        \\u2260\\n                        k\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        j\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        k\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}E(n_{i}^{2})&=E\\\\left(\\\\sum _{j=1}^{n}X_{ij}\\\\sum _{k=1}^{n}X_{ik}\\\\right)\\\\\\\\&=E\\\\left(\\\\sum _{j=1}^{n}\\\\sum _{k=1}^{n}X_{ij}X_{ik}\\\\right)\\\\\\\\&=E\\\\left(\\\\sum _{j=1}^{n}X_{ij}^{2}\\\\right)+E\\\\left(\\\\sum _{1\\\\leq j,k\\\\leq n}\\\\sum _{j\\\\neq k}X_{ij}X_{ik}\\\\right)\\\\end{aligned}}}\\n  The last line separates the summation into the case \\n  \\n    \\n      \\n        j\\n        =\\n        k\\n      \\n    \\n    {\\\\displaystyle j=k}\\n   and the case \\n  \\n    \\n      \\n        j\\n        \\u2260\\n        k\\n      \\n    \\n    {\\\\displaystyle j\\\\neq k}\\n  . Since the chance of an object distributed to bucket \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        k\\n      \\n    \\n    {\\\\displaystyle 1/k}\\n  , \\n  \\n    \\n      \\n        \\n          X\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X_{ij}}\\n   is 1 with probability \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        k\\n      \\n    \\n    {\\\\displaystyle 1/k}\\n   and 0 otherwise. \\n\\n  \\n    \\n      \\n        E\\n        (\\n        \\n          X\\n          \\n            i\\n            j\\n          \\n          \\n            2\\n          \\n        \\n        )\\n        =\\n        \\n          1\\n          \\n            2\\n          \\n        \\n        \\u22c5\\n        \\n          (\\n          \\n            \\n              1\\n              k\\n            \\n          \\n          )\\n        \\n        +\\n        \\n          0\\n          \\n            2\\n          \\n        \\n        \\u22c5\\n        \\n          (\\n          \\n            1\\n            \\u2212\\n            \\n              \\n                1\\n                k\\n              \\n            \\n          \\n          )\\n        \\n        =\\n        \\n          \\n            1\\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E(X_{ij}^{2})=1^{2}\\\\cdot \\\\left({\\\\frac {1}{k}}\\\\right)+0^{2}\\\\cdot \\\\left(1-{\\\\frac {1}{k}}\\\\right)={\\\\frac {1}{k}}}\\n  \\n\\n  \\n    \\n      \\n        E\\n        (\\n        \\n          X\\n          \\n            i\\n            j\\n          \\n        \\n        \\n          X\\n          \\n            i\\n            k\\n          \\n        \\n        )\\n        =\\n        1\\n        \\u22c5\\n        \\n          (\\n          \\n            \\n              1\\n              k\\n            \\n          \\n          )\\n        \\n        \\n          (\\n          \\n            \\n              1\\n              k\\n            \\n          \\n          )\\n        \\n        =\\n        \\n          \\n            1\\n            \\n              k\\n              \\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E(X_{ij}X_{ik})=1\\\\cdot \\\\left({\\\\frac {1}{k}}\\\\right)\\\\left({\\\\frac {1}{k}}\\\\right)={\\\\frac {1}{k^{2}}}}\\n  With the summation, it would be\\n\\n  \\n    \\n      \\n        E\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                j\\n                =\\n                1\\n              \\n              \\n                n\\n              \\n            \\n            \\n              X\\n              \\n                i\\n                j\\n              \\n              \\n                2\\n              \\n            \\n          \\n          )\\n        \\n        +\\n        E\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                1\\n                \\u2264\\n                j\\n                ,\\n                k\\n                \\u2264\\n                n\\n              \\n            \\n            \\n              \\u2211\\n              \\n                j\\n                \\u2260\\n                k\\n              \\n            \\n            \\n              X\\n              \\n                i\\n                j\\n              \\n            \\n            \\n              X\\n              \\n                i\\n                k\\n              \\n            \\n          \\n          )\\n        \\n        =\\n        n\\n        \\u22c5\\n        \\n          \\n            1\\n            k\\n          \\n        \\n        +\\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\u22c5\\n        \\n          \\n            1\\n            \\n              k\\n              \\n                2\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                n\\n                \\n                  2\\n                \\n              \\n              +\\n              n\\n              k\\n              \\u2212\\n              n\\n            \\n            \\n              k\\n              \\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E\\\\left(\\\\sum _{j=1}^{n}X_{ij}^{2}\\\\right)+E\\\\left(\\\\sum _{1\\\\leq j,k\\\\leq n}\\\\sum _{j\\\\neq k}X_{ij}X_{ik}\\\\right)=n\\\\cdot {\\\\frac {1}{k}}+n(n-1)\\\\cdot {\\\\frac {1}{k^{2}}}={\\\\frac {n^{2}+nk-n}{k^{2}}}}\\n  Finally, the complexity would be \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                1\\n              \\n              \\n                k\\n              \\n            \\n            E\\n            (\\n            \\n              n\\n              \\n                i\\n              \\n              \\n                2\\n              \\n            \\n            )\\n          \\n          )\\n        \\n        =\\n        O\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                1\\n              \\n              \\n                k\\n              \\n            \\n            \\n              \\n                \\n                  \\n                    n\\n                    \\n                      2\\n                    \\n                  \\n                  +\\n                  n\\n                  k\\n                  \\u2212\\n                  n\\n                \\n                \\n                  k\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n          \\n          )\\n        \\n        =\\n        O\\n        \\n          (\\n          \\n            \\n              \\n                \\n                  n\\n                  \\n                    2\\n                  \\n                \\n                k\\n              \\n            \\n            +\\n            n\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left(\\\\sum _{i=1}^{k}E(n_{i}^{2})\\\\right)=O\\\\left(\\\\sum _{i=1}^{k}{\\\\frac {n^{2}+nk-n}{k^{2}}}\\\\right)=O\\\\left({\\\\frac {n^{2}}{k}}+n\\\\right)}\\n  .\\nThe last step of bucket sort, which is concatenating all the sorted objects in each buckets, requires \\n  \\n    \\n      \\n        O\\n        (\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(k)}\\n   time. Therefore, the total complexity is \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            n\\n            +\\n            \\n              \\n                \\n                  n\\n                  \\n                    2\\n                  \\n                \\n                k\\n              \\n            \\n            +\\n            k\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left(n+{\\\\frac {n^{2}}{k}}+k\\\\right)}\\n  . Note that if k is chosen to be \\n  \\n    \\n      \\n        k\\n        =\\n        \\u0398\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle k=\\\\Theta (n)}\\n  , then bucket sort runs in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   average time, given a uniformly distributed input.\\n\\nOptimizations\\nA common optimization is to put the unsorted elements of the buckets back in the original array first, then run insertion sort over the complete array; because insertion sort's runtime is based on how far each element is from its final position, the number of comparisons remains relatively small, and the memory hierarchy is better exploited by storing the list contiguously in memory.If the input distribution is known or can be estimated, buckets can often be chosen which contain constant density (rather than merely having constant size). This allows \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   average time complexity even without uniformly distributed input.\\n\\nVariants\\nGeneric bucket sort\\nThe most common variant of bucket sort operates on a list of n numeric inputs between zero and some maximum value M and divides the value range into n buckets each of size M/n. If each bucket is sorted using insertion sort, the sort can be shown to run in expected linear time (where the average is taken over all possible inputs). However, the performance of this sort degrades with clustering; if many values occur close together, they will all fall into a single bucket and be sorted slowly.  This performance degradation is avoided in the original bucket sort algorithm by assuming that the input is generated by a random process that distributes elements uniformly over the interval [0,1).\\n\\nProxmapSort\\nSimilar to generic bucket sort as described above, ProxmapSort works by dividing an array of keys into subarrays via the use of a \\\"map key\\\" function that preserves a partial ordering on the keys; as each key is added to its subarray, insertion sort is used to keep that subarray sorted, resulting in the entire array being in sorted order when ProxmapSort completes. ProxmapSort differs from bucket sorts in its use of the map key to place the data approximately where it belongs in sorted order, producing a \\\"proxmap\\\" \\u2014 a proximity mapping \\u2014 of the keys.\\n\\nHistogram sort\\nAnother variant of bucket sort known as histogram sort or counting sort adds an initial pass that counts the number of elements that will fall into each bucket using a count array. Using this information, the array values can be arranged into a sequence of buckets in-place by a sequence of exchanges, leaving no space overhead for bucket storage.\\n\\nPostman's sort\\nThe Postman's sort is a variant of bucket sort that takes advantage of a hierarchical structure of elements, typically described by a set of attributes. This is the algorithm used by letter-sorting machines in post offices: mail is sorted first between domestic and international; then by state, province or territory; then by destination post office; then by routes, etc. Since keys are not compared against each other, sorting time is O(cn), where c depends on the size of the key and number of buckets. This is similar to a radix sort that works \\\"top down,\\\" or \\\"most significant digit first.\\\"\\n\\nShuffle sort\\nThe shuffle sort is a variant of bucket sort that begins by removing the first 1/8 of the n items to be sorted, sorts them recursively, and puts them in an array. This creates n/8 \\\"buckets\\\" to which the remaining 7/8 of the items are distributed. Each \\\"bucket\\\" is then sorted, and the \\\"buckets\\\" are concatenated into a sorted array.\\n\\nComparison with other sorting algorithms\\nBucket sort can be seen as a generalization of counting sort; in fact, if each bucket has size 1 then bucket sort degenerates to counting sort. The variable bucket size of bucket sort allows it to use O(n) memory instead of O(M) memory, where M is the number of distinct values; in exchange, it gives up counting sort's O(n + M) worst-case behavior.\\nBucket sort with two buckets is effectively a version of quicksort where the pivot value is always selected to be the middle value of the value range. While this choice is effective for uniformly distributed inputs, other means of choosing the pivot in quicksort such as randomly selected pivots make it more resistant to clustering in the input distribution.\\nThe n-way mergesort algorithm also begins by distributing the list into n sublists and sorting each one; however, the sublists created by mergesort have overlapping value ranges and so cannot be recombined by simple concatenation as in bucket sort. Instead, they must be interleaved by a merge algorithm. However, this added expense is counterbalanced by the simpler scatter phase and the ability to ensure that each sublist is the same size, providing a good worst-case time bound.\\nTop-down radix sort can be seen as a special case of bucket sort where both the range of values and the number of buckets is constrained to be a power of two. Consequently, each bucket's size is also a power of two, and the procedure can be applied recursively. This approach can accelerate the scatter phase, since we only need to examine a prefix of the bit representation of each element to determine its bucket.\\n\\nReferences\\nPaul E. Black \\\"Postman's Sort\\\" from Dictionary of Algorithms and Data Structures at NIST.\\nRobert Ramey '\\\"The Postman's Sort\\\" C Users Journal Aug. 1992\\nNIST's Dictionary of Algorithms and Data Structures: bucket sort\\n\\nExternal links\\nBucket Sort Code for Ansi C\\nVariant of Bucket Sort with Demo\"}, {\"Cache-oblivious distribution sort\": \"The cache-oblivious distribution sort is a comparison-based sorting algorithm. It is similar to quicksort, but it is a cache-oblivious algorithm, designed for a setting where the number of elements to sort is too large to fit in a cache where operations are done. In the external memory model, the number of memory transfers it needs to perform a sort of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   items on a machine with cache of size \\n  \\n    \\n      \\n        Z\\n      \\n    \\n    {\\\\displaystyle Z}\\n   and cache lines of length \\n  \\n    \\n      \\n        L\\n      \\n    \\n    {\\\\displaystyle L}\\n   is \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          \\n            N\\n            L\\n          \\n        \\n        \\n          log\\n          \\n            Z\\n          \\n        \\n        \\u2061\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle O({\\\\frac {N}{L}}\\\\log _{Z}N)}\\n  , under the tall cache assumption that \\n  \\n    \\n      \\n        Z\\n        =\\n        \\u03a9\\n        (\\n        \\n          L\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle Z=\\\\Omega (L^{2})}\\n  . This number of memory transfers has been shown to be asymptotically optimal for comparison sorts. This distribution sort also achieves the asymptotically optimal runtime complexity of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        N\\n        log\\n        \\u2061\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (N\\\\log N)}\\n  .\\n\\nAlgorithm\\nOverview\\nDistribution sort operates on a contiguous array of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   elements. To sort the elements, it performs the following:\\n\\nPartition the array into \\n  \\n    \\n      \\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {N}}}\\n   contiguous subarrays of size \\n  \\n    \\n      \\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {N}}}\\n  , and recursively sort each subarray.\\nDistribute the elements of the sorted subarrays into \\n  \\n    \\n      \\n        q\\n        \\u2264\\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle q\\\\leq {\\\\sqrt {N}}}\\n   buckets \\n  \\n    \\n      \\n        \\n          B\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          B\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          B\\n          \\n            q\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B_{1},B_{2},\\\\ldots ,B_{q}}\\n   each of size at most \\n  \\n    \\n      \\n        2\\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2{\\\\sqrt {N}}}\\n   such that for every i from 1 to q-1,  every element of bucket \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B_{i}}\\n   is not larger than any element in \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n            +\\n            1\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle B_{i+1}.}\\n   This distribution step is the main step of this algorithm, and is covered in more detail below.\\nRecursively sort each bucket.\\nOutput the concatenation of the buckets.\\n\\nDistribution step\\nAs mentioned in step 2 above, the goal of the distribution step is to distribute the sorted subarrays into q buckets \\n  \\n    \\n      \\n        \\n          B\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          B\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          B\\n          \\n            q\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle B_{1},B_{2},\\\\ldots ,B_{q}.}\\n   The distribution step algorithm maintains two invariants. The first is that each bucket has size at most \\n  \\n    \\n      \\n        2\\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2{\\\\sqrt {N}}}\\n   at any time, and any element in bucket \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B_{i}}\\n   is no larger than any element in bucket \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n            +\\n            1\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle B_{i+1}.}\\n   The second is that every bucket has an associated pivot, a value which is greater than all elements in the bucket.\\nInitially, the algorithm starts with one empty bucket with pivot \\n  \\n    \\n      \\n        \\u221e\\n      \\n    \\n    {\\\\displaystyle \\\\infty }\\n  . As it fills buckets, it creates new buckets by splitting a bucket into two when it would be made overfull (by having at least \\n  \\n    \\n      \\n        (\\n        2\\n        \\n          \\n            N\\n          \\n        \\n        +\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (2{\\\\sqrt {N}}+1)}\\n   elements placed into it). The split is done by performing the linear time median finding algorithm, and partitioning based on this median. The pivot of the lower bucket will be set to the median found, and the pivot of the higher bucket will be set to the same as the bucket before the split. At the end of the distribution step, all elements are in the buckets, and the two invariants will still hold.\\nTo accomplish this, each subarray and bucket will have a state associated with it. The state of a subarray consists of an index next of the next element to be read from the subarray, and a bucket number bnum indicating which bucket index the element should be copied to. By convention, \\n  \\n    \\n      \\n        b\\n        n\\n        u\\n        m\\n        =\\n        \\u221e\\n      \\n    \\n    {\\\\displaystyle bnum=\\\\infty }\\n   if all elements in the subarray have been distributed. (Note that when we split a bucket, we have to increment all bnum values of all subarrays whose bnum value is greater than the index of the bucket that is split.) The state of a bucket consists of the value of the bucket's pivot, and the number of elements currently in the bucket.\\nConsider the follow basic strategy: iterate through each subarray, attempting to copy over its element at position next. If the element is smaller than the pivot of bucket bnum, then place it in that bucket, possibly incurring a bucket split. Otherwise, increment bnum until a bucket whose pivot is large enough is found. Though this correctly distributes all elements, it does not exhibit a good cache performance.\\nInstead, the distribution step is performed in a recursive divide-and-conquer. The step will be performed as a call to the function distribute, which takes three parameters i, j, and m. distribute(i, j, m) will distribute elements from the i-th through (i+m-1)-th subarrays into buckets, starting from \\n  \\n    \\n      \\n        \\n          B\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B_{j}}\\n  . It requires as a precondition that each subarray r in the range \\n  \\n    \\n      \\n        i\\n        ,\\n        \\u2026\\n        ,\\n        i\\n        +\\n        m\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle i,\\\\ldots ,i+m-1}\\n   has its \\n  \\n    \\n      \\n        b\\n        n\\n        u\\n        m\\n        [\\n        r\\n        ]\\n        \\u2265\\n        j\\n      \\n    \\n    {\\\\displaystyle bnum[r]\\\\geq j}\\n  . The execution of distribute(i, j, m) will guarantee that each \\n  \\n    \\n      \\n        b\\n        n\\n        u\\n        m\\n        [\\n        r\\n        ]\\n        \\u2265\\n        j\\n        +\\n        m\\n      \\n    \\n    {\\\\displaystyle bnum[r]\\\\geq j+m}\\n  . The whole distribution step is distribute\\n  \\n    \\n      \\n        (\\n        1\\n        ,\\n        1\\n        ,\\n        \\n          \\n            N\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (1,1,{\\\\sqrt {N}})}\\n  . Pseudocode for the implementation of distribute is shown below:\\n\\nThe base case, where m=1, has a call to the subroutine copy_elems. In this base case, all elements from subarray i that belong to bucket j are added at once. If this leads to bucket j having too many elements, it splits the bucket with the procedure described beforehand.\\n\\nSee also\\nCache-oblivious algorithm\\nFunnelsort\\nExternal sorting\\n\\nReferences\\nHarald Prokop. Cache-Oblivious Algorithms. Masters thesis, MIT. 1999.\"}, {\"Cartesian tree\": \"In computer science, a Cartesian tree is a binary tree derived from a sequence of numbers. The smallest number in the sequence is at the root of the tree; its left and right subtrees are constructed recursively from the subsequences to the left and right of this number. When all numbers are distinct, the Cartesian tree is uniquely defined from the properties that it is heap-ordered and that a symmetric (in-order) traversal of the tree returns the original sequence.\\nIntroduced by Vuillemin (1980) in the context of geometric range searching data structures, Cartesian trees have also been used in the definition of the treap and randomized binary search tree data structures for binary search problems, in comparison sort algorithms that perform efficiently on nearly-sorted inputs, and as the basis for pattern matching algorithms. A Cartesian tree for a sequence may be constructed in linear time.\\n\\nDefinition\\nThe Cartesian tree for a sequence of numbers is defined by the following properties:\\n\\nThe Cartesian tree for a sequence has one node for each number in the sequence. Each node is associated with a single sequence value.\\nA symmetric (in-order) traversal of the tree results in the original sequence. That is, the left subtree consists of the values earlier than the root in the sequence order, while the right subtree consists of the values later than the root, and a similar ordering constraint holds at each lower node of the tree.\\nThe tree has the heap property: the parent of any non-root node has a smaller value than the node itself.Based on the heap property, the root of the tree must be the smallest number in the sequence. From this, the tree itself may also be defined recursively: the root is the minimum value of the sequence, and the left and right subtrees are the Cartesian trees for the subsequences to the left and right of the root value.For a sequence of distinct numbers, the three properties above determine a unique Cartesian tree. If a sequence of numbers contains repetitions, the Cartesian tree may be determined by following a consistent tie-breaking rule before applying the above construction. For instance, the first of two equal elements may be treated as the smaller of the two.\\n\\nEfficient construction\\nA Cartesian tree may be constructed in linear time from its input sequence.\\nOne method is to simply process the sequence values in left-to-right order, maintaining the Cartesian tree of the nodes processed so far, in a structure that allows both upwards and downwards traversal of the tree. To process each new value \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  , start at the node representing the value prior to \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   in the sequence and follow the path from this node to the root of the tree until finding a value \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   smaller than \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  . The node \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   becomes the right child of \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  , and the previous right child of \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   becomes the new left child of \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  . The total time for this procedure is linear, because the time spent searching for the parent \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   of each new node \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   can be charged against the number of nodes that are removed from the rightmost path in the tree.An alternative linear-time construction algorithm is based on the all nearest smaller values problem. In the input sequence, one may define the left neighbor of a value \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   to be the value that occurs prior to \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  , is smaller than \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  , and is closer in position to \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   than any other smaller value. The right neighbor is defined symmetrically. The sequence of left neighbors may be found by an algorithm that maintains a stack containing a subsequence of the input. For each new sequence value \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  , the stack is popped until it is empty or its top element is smaller than \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  , and then \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   is pushed onto the stack. The left neighbor of \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   is the top element at the time \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   is pushed. The right neighbors may be found by applying the same stack algorithm to the reverse of the sequence. The parent of \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   in the Cartesian tree is either the left neighbor of \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   or the right neighbor of \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  , whichever exists and has a larger value. The left and right neighbors may also be constructed efficiently by parallel algorithms, so this formulation may be used to develop efficient parallel algorithms for Cartesian tree construction.Another linear-time algorithm for Cartesian tree construction is based on divide-and-conquer. The algorithm recursively constructs the tree on each half of the input, and then merges the two trees by taking the right spine of the left tree and left spine of the right tree (both of which are paths whose root-to-leaf order sorts their values from smallest to largest) and performs a standard merging operation, replacing these two paths in two trees by a single path that contains the same nodes. In the merged path, the successor in the sorted order of each node from the left tree is placed in its right child, and the successor of each node from the right tree is placed in its left child, the same position that was previously used for its successor in the spine. The left children of nodes from the left tree and right children of nodes from the right tree are left unchanged. The algorithm is also parallelizable since on each level of recursion, each of the two sub-problems can be computed in parallel, and the merging operation can be efficiently parallelized as well.It is possible to maintain the Cartesian tree of a dynamic input, subject to insertions of elements and lazy deletion of elements, in logarithmic amortized time per operation. Here, lazy deletion means that up to a constant fraction of the elements in the current tree may be marked as deleted, rather than actually removed. When too large a fraction of elements are marked, the tree is rebuilt.\\n\\nApplications\\nRange searching and lowest common ancestors\\nCartesian trees may be used as part of an efficient data structure for range minimum queries, a range searching problem involving queries that ask for the minimum value in a contiguous subsequence of the original sequence. In a Cartesian tree, this minimum value may be found at the lowest common ancestor of the leftmost and rightmost values in the subsequence. For instance, in the subsequence (12,10,20,15) of the sequence shown in the first illustration, the minimum value of the subsequence (10) forms the lowest common ancestor of the leftmost and rightmost values (12 and 15). Because lowest common ancestors may be found in constant time per query, using a data structure that takes linear space to store and that may be constructed in linear time, the same bounds hold for the range minimization problem.Bender & Farach-Colton (2000) reversed this relationship between the two data structure problems by showing that data structures for range minimization could also be used for finding lowest common ancestors. Their data structure associates with each node of the tree its distance from the root, and constructs a sequence of these distances in the order of an Euler tour of the (edge-doubled) tree. It then constructs a range minimization data structure for the resulting sequence. The lowest common ancestor of any two vertices in the given tree can be found as the minimum distance appearing in the interval between the initial positions of these two vertices in the sequence. Bender and Farach-Colton also provide a method for range minimization that can be used for the sequences resulting from this transformation, which have the special property that adjacent sequence values differ by \\u00b11. As they describe, for range minimization in sequences that do not have this form, it is possible to use Cartesian trees to reduce the range minimization problem to lowest common ancestors, and then to use Euler tours to reduce lowest common ancestors to a range minimization problem with this special form.The same range minimization problem may also be given an alternative interpretation in terms of two dimensional range searching. A collection of finitely many points in the Cartesian plane may be used to form a Cartesian tree, by sorting the points by their \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  -coordinates and using the \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  -coordinates in this order as the sequence of values from which this tree is formed. If \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   is the subset of the input points within some vertical slab defined by the inequalities \\n  \\n    \\n      \\n        L\\n        \\u2264\\n        x\\n        \\u2264\\n        R\\n      \\n    \\n    {\\\\displaystyle L\\\\leq x\\\\leq R}\\n  , \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   is the leftmost point in \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   (the one with minimum \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  -coordinate), and \\n  \\n    \\n      \\n        q\\n      \\n    \\n    {\\\\displaystyle q}\\n   is the rightmost point in \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   (the one with maximum \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  -coordinate) then the lowest common ancestor of \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   and \\n  \\n    \\n      \\n        q\\n      \\n    \\n    {\\\\displaystyle q}\\n   in the Cartesian tree is the bottommost point in the slab. A three-sided range query, in which the task is to list all points within a region bounded by the three inequalities \\n  \\n    \\n      \\n        L\\n        \\u2264\\n        x\\n        \\u2264\\n        R\\n      \\n    \\n    {\\\\displaystyle L\\\\leq x\\\\leq R}\\n   and \\n  \\n    \\n      \\n        y\\n        \\u2264\\n        T\\n      \\n    \\n    {\\\\displaystyle y\\\\leq T}\\n  , may be answered by finding this bottommost point \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  , comparing its \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  -coordinate to \\n  \\n    \\n      \\n        T\\n      \\n    \\n    {\\\\displaystyle T}\\n  , and (if the point lies within the three-sided region) continuing recursively in the two slabs bounded between \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   and \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   and between \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   and \\n  \\n    \\n      \\n        q\\n      \\n    \\n    {\\\\displaystyle q}\\n  . In this way, after the leftmost and rightmost points in the slab are identified, all points within the three-sided region may be listed in constant time per point.The same construction, of lowest common ancestors in a Cartesian tree, makes it possible to construct a data structure with linear space that allows the distances between pairs of points in any ultrametric space to be queried in constant time per query. The distance within an ultrametric is the same as the minimax path weight in the minimum spanning tree of the metric. From the minimum spanning tree, one can construct a Cartesian tree, the root node of which represents the heaviest edge of the minimum spanning tree. Removing this edge partitions the minimum spanning tree into two subtrees, and Cartesian trees recursively constructed for these two subtrees form the children of the root node of the Cartesian tree. The leaves of the Cartesian tree represent points of the metric space, and the lowest common ancestor of two leaves in the Cartesian tree is the heaviest edge between those two points in the minimum spanning tree, which has weight equal to the distance between the two points. Once the minimum spanning tree has been found and its edge weights sorted, the Cartesian tree may be constructed in linear time.\\n\\nAs a binary search tree\\nBecause a Cartesian tree is a binary tree, it is natural to use it as a binary search tree for an ordered sequence of values. However, defining a Cartesian tree based on the same values that form the search keys of a binary search tree does not work well: the Cartesian tree of a sorted sequence is just a path, rooted at its leftmost endpoint, and binary searching in this tree degenerates to sequential search in the path. However, it is possible to generate more-balanced search trees by generating priority values for each search key that are different than the key itself, sorting the inputs by their key values, and using the corresponding sequence of priorities to generate a Cartesian tree. This construction may equivalently be viewed in the geometric framework described above, in which the \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  -coordinates of a set of points are the search keys and the \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  -coordinates are the priorities.This idea was applied by Seidel & Aragon (1996), who suggested the use of random numbers as priorities. The data structure resulting from this random choice is called a treap, due to its combination of binary search tree and binary heap features. An insertion into a treap may be performed by inserting the new key as a leaf of an existing tree, choosing a priority for it, and then performing tree rotation operations along a path from the node to the root of the tree to repair any violations of the heap property caused by this insertion; a deletion may similarly be performed by a constant amount of change to the tree followed by a sequence of rotations along a single path in the tree. A variation on this data structure called a zip tree uses the same idea of random priorities, but simplifies the random generation of the priorities, and performs insertions and deletions in a different way, by splitting the sequence and its associated Cartesian tree into two subsequences and two trees and then recombining them.If the priorities of each key are chosen randomly and independently once whenever the key is inserted into the tree, the resulting Cartesian tree will have the same properties as a random binary search tree, a tree computed by inserting the keys in a randomly chosen permutation starting from an empty tree, with each insertion leaving the previous tree structure unchanged and inserting the new node as a leaf of the tree. Random binary search trees had been studied for much longer, and are known to behave well as search trees (they have logarithmic depth with high probability); the same good behavior carries over to treaps. It is also possible, as suggested by Aragon and Seidel, to reprioritize frequently-accessed nodes, causing them to move towards the root of the treap and speeding up future accesses for the same keys.\\n\\nIn sorting\\nLevcopoulos & Petersson (1989) describe a sorting algorithm based on Cartesian trees. They describe the algorithm as based on a tree with the maximum at the root, but it may be modified straightforwardly to support a Cartesian tree with the convention that the minimum value is at the root. For consistency, it is this modified version of the algorithm that is described below.\\nThe Levcopoulos\\u2013Petersson algorithm can be viewed as a version of selection sort or heap sort that maintains a priority queue of candidate minima, and that at each step finds and removes the minimum value in this queue, moving this value to the end of an output sequence. In their algorithm, the priority queue consists only of elements whose parent in the Cartesian tree has already been found and removed. Thus, the algorithm consists of the following steps:\\nConstruct a Cartesian tree for the input sequence\\nInitialize a priority queue, initially containing only the tree root\\nWhile the priority queue is non-empty:\\nFind and remove the minimum value in the priority queue\\nAdd this value to the output sequence\\nAdd the Cartesian tree children of the removed value to the priority queueAs Levcopoulos and Petersson show, for input sequences that are already nearly sorted, the size of the priority queue will remain small, allowing this method to take advantage of the nearly-sorted input and run more quickly. Specifically, the worst-case running time of this algorithm is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log k)}\\n  , where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the sequence length and \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   is the average, over all values in the sequence, of the number of consecutive pairs of sequence values that bracket the given value. They also prove a lower bound stating that, for any \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   and (non-constant) \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  , any comparison-based sorting algorithm must use \\n  \\n    \\n      \\n        \\u03a9\\n        (\\n        n\\n        log\\n        \\u2061\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Omega (n\\\\log k)}\\n   comparisons for some inputs.\\n\\nIn pattern matching\\nThe problem of Cartesian tree matching has been defined as a generalized form of string matching in which one seeks a substring (or in some cases, a subsequence) of a given string that has a Cartesian tree of the same form as a given pattern. Fast algorithms for variations of the problem with a single pattern or multiple patterns have been developed, as well as data structures analogous to the suffix tree and other text indexing structures.\\n\\nHistory\\nCartesian trees were introduced and named by Vuillemin (1980). The name is derived from the Cartesian coordinate system for the plane: in one version of this structure, as in the two-dimensional range searching application discussed above, a Cartesian tree for a point set has the sorted order of the points by their \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  -coordinates as its symmetric traversal order, and it has the heap property according to the \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  -coordinates of the points. Vuillemin (1980) described both this geometric version of the structure, and the definition here in which a Cartesian tree is defined from a sequence. Using sequences instead of point coordinates provides a more general setting that allows the Cartesian tree to be applied to non-geometric problems as well.\\n\\nNotes\\n\\n\\n== References ==\"}, {\"Cascade merge sort\": \"Cascade merge sort is similar to the polyphase merge sort but uses a simpler distribution.  The merge is slower than a polyphase merge when there are fewer than six files, but faster when there are more than six.\\n\\nReferences\\nBradley, James (1982), File and Data Base Techniques, Holt, Rinehart and Winston, ISBN 0-03-058673-9\\n\\nExternal links\\nhttp://www.minkhollow.ca/Courses/461/Notes/Cosequential/Cascade.html\"}, {\"Cocktail shaker sort\": \"Cocktail shaker sort, also known as bidirectional bubble sort, cocktail sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is an extension of bubble sort.  The algorithm extends bubble sort by operating in two directions.  While it improves on bubble sort by more quickly moving items to the beginning of the list, it provides only marginal performance improvements. \\nLike most variants of bubble sort, cocktail shaker sort is used primarily as an educational tool. More performant algorithms such as quicksort, merge sort, or timsort are used by the sorting libraries built into popular programming languages such as Python and Java.\\n\\nPseudocode\\nThe simplest form goes through the whole list each time:\\n\\nprocedure cocktailShakerSort(A : list of sortable items) is\\n    do\\n        swapped := false\\n        for each i in 0 to length(A) \\u2212 1 do:\\n            if A[i] > A[i + 1] then // test whether the two elements are in the wrong order\\n                swap(A[i], A[i + 1]) // let the two elements change places\\n                swapped := true\\n            end if\\n        end for\\n        if not swapped then\\n            // we can exit the outer loop here if no swaps occurred.\\n            break do-while loop\\n        end if\\n        swapped := false\\n        for each i in length(A) \\u2212 1 to 0 do:\\n            if A[i] > A[i + 1] then\\n                swap(A[i], A[i + 1])\\n                swapped := true\\n            end if\\n        end for\\n    while swapped // if no elements have been swapped, then the list is sorted\\nend procedure\\n\\nThe first rightward pass will shift the largest element to its correct place at the end, and the following leftward pass will shift the smallest element to its correct place at the beginning. The second complete pass will shift the second largest and second smallest elements to their correct places, and so on. After i passes, the first i and the last i elements in the list are in their correct positions, and do not need to be checked. By shortening the part of the list that is sorted each time, the number of operations can be halved (see bubble sort).\\nThis is an example of the algorithm in MATLAB/OCTAVE with the optimization of remembering the last swap index and updating the bounds.\\n\\nDifferences from bubble sort\\nCocktail shaker sort is a slight variation of bubble sort. It differs in that instead of repeatedly passing through the list from bottom to top, it passes alternately from bottom to top and then from top to bottom. It can achieve slightly better performance than a standard bubble sort. The reason for this is that bubble sort only passes through the list in one direction and therefore can only move items backward one step each iteration.\\nAn example of a list that proves this point is the list (2,3,4,5,1), which would only need to go through one pass of cocktail sort to become sorted, but if using an ascending bubble sort would take four passes. However one cocktail sort pass should be counted as two bubble sort passes. Typically cocktail sort is less than two times faster than bubble sort.\\nAnother optimization can be that the algorithm remembers where the last actual swap has been done. In the next iteration, there will be no swaps beyond this limit and the algorithm has shorter passes. As the cocktail shaker sort goes bidirectionally, the range of possible swaps, which is the range to be tested, will reduce per pass, thus reducing the overall running time slightly.\\n\\nComplexity\\nThe complexity of the cocktail shaker sort in big O notation is \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   for both the worst case and the average case, but it becomes closer to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   if the list is mostly ordered before applying the sorting algorithm. For example, if every element is at a position that differs by at most k (k \\u2265 1) from the position it is going to end up in, the complexity of cocktail shaker sort becomes \\n  \\n    \\n      \\n        O\\n        (\\n        k\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle O(kn).}\\n  \\nThe cocktail shaker sort is also briefly discussed in the book The Art of Computer Programming, along with similar refinements of bubble sort. In conclusion, Knuth states about bubble sort and its improvements:\\n\\nBut none of these refinements leads to an algorithm better than straight insertion [that is, insertion sort]; and we already know that straight insertion isn't suitable for large N. [...] In short, the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems.\\n\\nReferences\\nSources\\nHartenstein, R. (July 2010). \\\"A new World Model of Computing\\\" (PDF). The Grand Challenge to Reinvent Computing. Belo Horizonte, Brazil: CSBC. Archived from the original (PDF) on 2013-08-07. Retrieved 2011-01-14.\\n\\nExternal links\\n\\nInteractive demo of cocktail sort\\nJava source code and an animated demo of cocktail sort (called bi-directional bubble sort) and several other algorithms\\n\\\".NET Implementation of cocktail sort and several other algorithms\\\". Archived from the original on 2012-02-12.\"}, {\"Comb sort\": \"Comb sort is a relatively simple sorting algorithm originally designed by W\\u0142odzimierz Dobosiewicz and Artur Borowy in 1980, later rediscovered (and given the name \\\"Combsort\\\") by Stephen Lacey and Richard Box in 1991. Comb sort improves on bubble sort in the same way that Shellsort improves on insertion sort.nist.gov's \\\"diminishing increment sort\\\" definition mentions the term 'comb sort' as visualizing iterative passes of the data, \\\"where the teeth of a comb touch;\\\" the former term is linked to Don Knuth.\\n\\nAlgorithm\\nThe basic idea is to eliminate turtles, or small values near the end of the list, since in a bubble sort these slow the sorting down tremendously. Rabbits, large values around the beginning of the list, do not pose a problem in bubble sort.\\nIn bubble sort, when any two elements are compared, they always have a gap (distance from each other) of 1. The basic idea of comb sort is that the gap can be much more than 1. The inner loop of bubble sort, which does the actual swap, is modified such that the gap between swapped elements goes down (for each iteration of outer loop) in steps of a \\\"shrink factor\\\" k: [n/k, n/k2, n/k3, ..., 1].\\nThe gap starts out as the length of the list n being sorted divided by the shrink factor k (generally 1.3; see below) and one pass of the aforementioned modified bubble sort is applied with that gap. Then the gap is divided by the shrink factor again, the list is sorted with this new gap, and the process repeats until the gap is 1. At this point, comb sort continues using a gap of 1 until the list is fully sorted. The final stage of the sort is thus equivalent to a bubble sort, but by this time most turtles have been dealt with, so a bubble sort will be efficient.\\nThe shrink factor has a great effect on the efficiency of comb sort. k = 1.3 has been suggested as an ideal shrink factor by the authors of the original article after empirical testing on over 200,000 random lists. A value too small slows the algorithm down by making unnecessarily many comparisons, whereas a value too large fails to effectively deal with turtles, making it require many passes with 1 gap size.\\nThe pattern of repeated sorting passes with decreasing gaps is similar to Shellsort, but in Shellsort the array is sorted completely each pass before going on to the next-smallest gap.  Comb sort's passes do not completely sort the elements.  This is the reason that Shellsort gap sequences have a larger optimal shrink factor of about 2.2.\\n\\nPseudocode\\nfunction combsort(array input) is\\n\\n    gap := input.size // Initialize gap size\\n    shrink := 1.3 // Set the gap shrink factor\\n    sorted := false\\n\\n    loop while sorted = false\\n        // Update the gap value for a next comb\\n        gap := floor(gap / shrink)\\n        if gap \\u2264 1 then\\n            gap := 1\\n            sorted := true // If there are no swaps this pass, we are done\\n        end if\\n\\n        // A single \\\"comb\\\" over the input list\\n        i := 0\\n        loop while i + gap < input.size // See Shell sort for a similar idea\\n            if input[i] > input[i+gap] then\\n                swap(input[i], input[i+gap])\\n                sorted := false\\n                // If this assignment never happens within the loop,\\n                // then there have been no swaps and the list is sorted.\\n             end if\\n    \\n             i := i + 1\\n         end loop\\n     end loop\\nend function\\n\\nPython code\\nPlus, two quick Python implementations: one works on the list (or array, or other mutable type where the operations used on it make sense to the language) in-place, the other makes a list with the same values as the given data and returns that after sorting it (similar to the builtin sorted function).\\n\\nSee also\\n\\nBubble sort, a generally slower algorithm, is the basis of comb sort.\\nCocktail sort, or bidirectional bubble sort, is a variation of bubble sort that also addresses the problem of turtles, albeit less effectively.\\n\\n\\n== References ==\"}, {\"Comparison sort\": \"A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a \\\"less than or equal to\\\" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list. The only requirement is that the operator forms a total preorder over the data, with:\\n\\nif a \\u2264 b and b \\u2264 c then a \\u2264 c (transitivity)\\nfor all a and b, a \\u2264 b or b \\u2264 a (connexity).It is possible that both a \\u2264 b and b \\u2264 a; in this case either may come first in the sorted list. In a stable sort, the input order determines the sorted order in this case.\\nA metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a balance scale. Their goal is to line up the weights in order by their weight without any information except that obtained by placing two weights on the scale and seeing which one is heavier (or if they weigh the same).\\n\\nExamples\\nSome of the most well-known comparison sorts include:\\n\\nQuicksort\\nHeapsort\\nShellsort\\nMerge sort\\nIntrosort\\nInsertion sort\\nSelection sort\\nBubble sort\\nOdd\\u2013even sort\\nCocktail shaker sort\\nCycle sort\\nMerge-insertion sort\\nSmoothsort\\nTimsort\\nBlock sort\\n\\nPerformance limits and advantages of different sorting techniques\\nThere are fundamental limits on the performance of comparison sorts. A comparison sort must have an average-case lower bound of \\u03a9(n log n) comparison operations, which is known as linearithmic time. This is a consequence of the limited information available through comparisons alone \\u2014 or, to put it differently, of the vague algebraic structure of totally ordered sets. In this sense, mergesort, heapsort, and introsort are asymptotically optimal in terms of the number of comparisons they must perform, although this metric neglects other operations. Non-comparison sorts (such as the examples discussed below) can achieve O(n) performance by using operations other than comparisons, allowing them to sidestep this lower bound (assuming elements are constant-sized).\\nComparison sorts may run faster on some lists; many adaptive sorts such as insertion sort run in O(n) time on an already-sorted or nearly-sorted list. The \\u03a9(n log n) lower bound applies only to the case in which the input list can be in any possible order.\\nReal-world measures of sorting speed may need to take into account the ability of some algorithms to optimally use relatively fast cached computer memory, or the application may benefit from sorting methods where sorted data begins to appear to the user quickly (and then user's speed of reading will be the limiting factor) as opposed to sorting methods where no output is available until the whole list is sorted.\\nDespite these limitations, comparison sorts offer the notable practical advantage that control over the comparison function allows sorting of many different datatypes and fine control over how the list is sorted. For example, reversing the result of the comparison function allows the list to be sorted in reverse; and one can sort a list of tuples in lexicographic order by just creating a comparison function that compares each part in sequence:\\n\\nfunction tupleCompare((lefta, leftb, leftc), (righta, rightb, rightc))\\n    if lefta \\u2260 righta\\n        return compare(lefta, righta)\\n    else if leftb \\u2260 rightb\\n        return compare(leftb, rightb)\\n    else\\n        return compare(leftc, rightc)\\n\\nComparison sorts generally adapt more easily to complex orders such as the order of floating-point numbers. Additionally, once a comparison function is written, any comparison sort can be used without modification; non-comparison sorts typically require specialized versions for each datatype.\\nThis flexibility, together with the efficiency of the above comparison sorting algorithms on modern computers, has led to widespread preference for comparison sorts in most practical work.\\n\\nAlternatives\\nSome sorting problems admit a strictly faster solution than the \\u03a9(n log n) bound for comparison sorting by using non-comparison sorts; an example is integer sorting, where all keys are integers. When the keys form a small (compared to n) range, counting sort is an example algorithm that runs in linear time. Other integer sorting algorithms, such as radix sort, are not asymptotically faster than comparison sorting, but can be faster in practice.\\nThe problem of sorting pairs of numbers by their sum is not subject to the \\u03a9(n\\u00b2 log n) bound either (the square resulting from the pairing up); the best known algorithm still takes O(n\\u00b2 log n) time, but only O(n\\u00b2) comparisons.\\n\\nNumber of comparisons required to sort a list\\nThe number of comparisons that a comparison sort algorithm requires increases in proportion to \\n  \\n    \\n      \\n        n\\n        log\\n        \\u2061\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle n\\\\log(n)}\\n  , where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of elements to sort.  This bound is asymptotically tight.\\nGiven a list of distinct numbers (we can assume this because this is a worst-case analysis), there are \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   factorial permutations exactly one of which is the list in sorted order. The sort algorithm must gain enough information from the comparisons to identify the correct permutation. If the algorithm always completes after at most \\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle f(n)}\\n   steps, it cannot distinguish more than \\n  \\n    \\n      \\n        \\n          2\\n          \\n            f\\n            (\\n            n\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2^{f(n)}}\\n   cases because the keys are distinct and each comparison has only two possible outcomes. Therefore,\\n\\n  \\n    \\n      \\n        \\n          2\\n          \\n            f\\n            (\\n            n\\n            )\\n          \\n        \\n        \\u2265\\n        n\\n        !\\n      \\n    \\n    {\\\\displaystyle 2^{f(n)}\\\\geq n!}\\n  , or equivalently \\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        )\\n        \\u2265\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        !\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle f(n)\\\\geq \\\\log _{2}(n!).}\\n  By looking at the first \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle n/2}\\n   factors of \\n  \\n    \\n      \\n        n\\n        !\\n        =\\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\u22ef\\n        1\\n      \\n    \\n    {\\\\displaystyle n!=n(n-1)\\\\cdots 1}\\n  , we obtain\\n\\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        !\\n        )\\n        \\u2265\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        \\n          (\\n          \\n            \\n              (\\n              \\n                \\n                  n\\n                  2\\n                \\n              \\n              )\\n            \\n            \\n              \\n                n\\n                2\\n              \\n            \\n          \\n          )\\n        \\n        =\\n        \\n          \\n            n\\n            2\\n          \\n        \\n        \\n          \\n            \\n              log\\n              \\u2061\\n              n\\n            \\n            \\n              log\\n              \\u2061\\n              2\\n            \\n          \\n        \\n        \\u2212\\n        \\n          \\n            n\\n            2\\n          \\n        \\n        =\\n        \\u0398\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}(n!)\\\\geq \\\\log _{2}\\\\left(\\\\left({\\\\frac {n}{2}}\\\\right)^{\\\\frac {n}{2}}\\\\right)={\\\\frac {n}{2}}{\\\\frac {\\\\log n}{\\\\log 2}}-{\\\\frac {n}{2}}=\\\\Theta (n\\\\log n).}\\n  \\n\\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        !\\n        )\\n        =\\n        \\u03a9\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}(n!)=\\\\Omega (n\\\\log n).}\\n  This provides the lower-bound part of the claim. A more precise bound can be given via Stirling's approximation. An upper bound of the same form, with the same leading term as the bound obtained from Stirling's approximation, follows from the existence of the algorithms that attain this bound in the worst case, like merge sort.\\nThe above argument provides an absolute, rather than only asymptotic lower bound on the number of comparisons, namely \\n  \\n    \\n      \\n        \\n          \\u2308\\n          \\n            \\n              log\\n              \\n                2\\n              \\n            \\n            \\u2061\\n            (\\n            n\\n            !\\n            )\\n          \\n          \\u2309\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left\\\\lceil \\\\log _{2}(n!)\\\\right\\\\rceil }\\n   comparisons. This lower bound is fairly good (it can be approached within a linear tolerance by a simple merge sort), but it is known to be inexact. For example, \\n  \\n    \\n      \\n        \\n          \\u2308\\n          \\n            \\n              log\\n              \\n                2\\n              \\n            \\n            \\u2061\\n            (\\n            13\\n            !\\n            )\\n          \\n          \\u2309\\n        \\n        =\\n        33\\n      \\n    \\n    {\\\\displaystyle \\\\left\\\\lceil \\\\log _{2}(13!)\\\\right\\\\rceil =33}\\n  , but the minimal number of comparisons to sort 13 elements has been proved to be 34.\\nDetermining the exact number of comparisons needed to sort a given number of entries is a computationally hard problem even for small n, and no simple formula for the solution is known. For some of the few concrete values that have been computed, see OEIS: A036604.\\n\\nLower bound for the average number of comparisons\\nA similar bound applies to the average number of comparisons. Assuming that\\n\\nall keys are distinct, i.e. every comparison will give either a>b or a<b, and\\nthe input is a random permutation, chosen uniformly from the set of all possible permutations of n elements,it is impossible to determine which order the input is in with fewer than log2(n!) comparisons on average.\\nThis can be most easily seen using concepts from information theory. The Shannon entropy of such a random permutation is log2(n!) bits. Since a comparison can give only two results, the maximum amount of information it provides is 1 bit. Therefore, after k comparisons the remaining entropy of the permutation, given the results of those comparisons, is at least log2(n!) \\u2212 k bits on average. To perform the sort, complete information is needed, so the remaining entropy must be 0. It follows that k must be at least log2(n!) on average. \\nThe lower bound derived via information theory is phrased as 'information-theoretic lower bound'. Information-theoretic lower bound is correct but is not necessarily the strongest lower bound. And in some cases, the information-theoretic lower bound of a problem may even be far from the true lower bound. For example, the information-theoretic lower bound of selection is \\n  \\n    \\n      \\n        \\n          \\u2308\\n          \\n            \\n              log\\n              \\n                2\\n              \\n            \\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          \\u2309\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left\\\\lceil \\\\log _{2}(n)\\\\right\\\\rceil }\\n   whereas \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   comparisons are needed by an adversarial argument. The interplay between information-theoretic lower bound and the true lower bound are much like a real-valued function lower-bounding an integer function. However, this is not exactly correct when the average case is considered.\\nTo unearth what happens while analyzing the average case, the key is that what does 'average' refer to? Averaging over what? With some knowledge of information theory, the information-theoretic lower bound averages over the set of all permutations as a whole. But any computer algorithms (under what are believed currently) must treat each permutation as an individual instance of the problem. Hence, the average lower bound we're searching for is averaged over all individual cases.\\nTo search for the lower bound relating to the non-achievability of computers, we adopt the Decision tree model. Let's rephrase a bit of what our objective is. In the Decision tree model, the lower bound to be shown is the lower bound of the average length of root-to-leaf paths of an \\n  \\n    \\n      \\n        n\\n        !\\n      \\n    \\n    {\\\\displaystyle n!}\\n  -leaf binary tree (in which each leaf corresponds to a permutation). It would be convinced to say that a balanced full binary tree achieves the minimum of the average length. With some careful calculations, for a balanced full binary tree with \\n  \\n    \\n      \\n        n\\n        !\\n      \\n    \\n    {\\\\displaystyle n!}\\n   leaves, the average length of root-to-leaf paths is given by \\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              (\\n              2\\n              n\\n              !\\n              \\u2212\\n              \\n                2\\n                \\n                  \\u230a\\n                  \\n                    log\\n                    \\n                      2\\n                    \\n                  \\n                  \\u2061\\n                  n\\n                  !\\n                  \\u230b\\n                  +\\n                  1\\n                \\n              \\n              )\\n              \\u22c5\\n              \\u2308\\n              \\n                log\\n                \\n                  2\\n                \\n              \\n              \\u2061\\n              n\\n              !\\n              \\u2309\\n              +\\n              (\\n              \\n                2\\n                \\n                  \\u230a\\n                  \\n                    log\\n                    \\n                      2\\n                    \\n                  \\n                  \\u2061\\n                  n\\n                  !\\n                  \\u230b\\n                  +\\n                  1\\n                \\n              \\n              \\u2212\\n              n\\n              !\\n              )\\n              \\u22c5\\n              \\u230a\\n              \\n                log\\n                \\n                  2\\n                \\n              \\n              \\u2061\\n              n\\n              !\\n              \\u230b\\n            \\n            \\n              n\\n              !\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {(2n!-2^{\\\\lfloor \\\\log _{2}n!\\\\rfloor +1})\\\\cdot \\\\lceil \\\\log _{2}n!\\\\rceil +(2^{\\\\lfloor \\\\log _{2}n!\\\\rfloor +1}-n!)\\\\cdot \\\\lfloor \\\\log _{2}n!\\\\rfloor }{n!}}}\\n  For example, for n = 3, the information-theoretic lower bound for the average case is approximately 2.58, while the average lower bound derived via Decision tree model is 8/3, approximately 2.67.\\nIn the case that multiple items may have the same key, there is no obvious statistical interpretation for the term \\\"average case\\\", so an argument like the above cannot be applied without making specific assumptions about the distribution of keys.\\n\\nn log n maximum number of comparisons for array-size in format 2^k\\nCan easy compute for real algorithm sorted-list-merging (array are sorted n-blocks with size 1, merge to 1-1 to 2, merge 2-2 to 4...).\\n\\n(1) = = = = = = = =\\n\\n(2) =   =   =   =     // max 1 compares (size1+size2-1), 4x repeats to concat 8 arrays with size 1 and 1\\n   === === === ===\\n\\n(3)   =       =       // max 7 compares, 2x repeats to concat 4 arrays with size 2 and 2\\n     ===     ===  \\n    =====   ===== \\n   ======= =======\\n\\n(4)                   // max 15 compares, 1x repeats to concat 2 arrays with size 4 and 4\\n\\nFormula extraction:\\nn = 256 = 2^8 (array size in format 2^k, for simplify)\\nOn = (n-1) + 2(n/2-1) + 4(n/4-1) + 8(n/8-1) + 16(n/16-1) + 32(n/32-1) + 64(n/64-1) + 128(n/128-1)\\nOn = (n-1) + (n-2) + (n-4) + (n-8) + (n-16) + (n-32) + (n-64) + (n-128)\\nOn = n+n+n+n+n+n+n+n - (1+2+4+8+16+32+64+128)   | 1+2+4... = formula for geometric sequence Sn = a1 * (q^i - 1) / (n - 1), n is number of items, a1 is first item\\nOn = 8*n - 1 * (2^8 - 1) / (2 - 1)\\nOn = 8*n - (2^8 - 1)   | 2^8 = n\\nOn = 8*n - (n - 1)\\nOn = (8-1)*n + 1   | 8 = ln(n)/ln(2) = ln(256)/ln(2)\\nOn = (ln(n)/ln(2) - 1) * n + 1\\n\\nExample:\\nn = 2^4 = 16, On ~= 3*n\\nn = 2^8 = 256, On ~= 7*n\\nn = 2^10 = 1.024, On ~= 9*n\\nn = 2^20 = 1.048.576, On ~= 19*n\\n\\nSorting a pre-sorted list\\nIf a list is already close to sorted, according to some measure of sortedness, the number of comparisons required to sort it can be smaller. An adaptive sort takes advantage of this \\\"presortedness\\\" and runs more quickly on nearly-sorted inputs, often while still maintaining an \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   worst case time bound. An example is adaptive heap sort, a sorting algorithm based on Cartesian trees. It takes time \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log k)}\\n  , where \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   is the average, over all values \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   in the sequence, of the number of times the sequence jumps from below \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   to above \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   or vice versa.\\n\\nNotes\\nReferences\\nDonald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Second Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Section 5.3.1: Minimum-Comparison Sorting, pp. 180\\u2013197.\"}, {\"K-sorted sequence\": \"In computer science, a nearly-sorted sequence, also known as roughly-sorted sequence and as \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted sequence is a sequence which is almost ordered. By almost ordered, it is meant that no element of the sequence is very far away from where it would be if the sequence were perfectly ordered. It is still possible that no element of the sequence is at the place where it should be if the sequence were perfectly ordered.\\nThe nearly-sorted sequences are particularly useful when the exact order of element has little importance. For example Twitter nearly sort the tweets, up to the second, as there is no need for more precision. Actually, given the impossibility to exactly synchronize all computers, an exact sorting of all tweets according to the time at which they are posted is impossible. This idea led to the creation of Snowflake IDs.\\n\\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorting is the operation of reordering the elements of a sequence so that it becomes \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted. \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorting is generally more efficient than sorting. Similarly, sorting a sequence is easier if it is known that the sequence is \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted.  So if a program needs only to consider \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted sequences as input or output, considering \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted sequences may save time.\\nThe radius of a sequence is a measure of presortedness, that is, its value indicate how much the elements in the list has to be moved to get a totally sorted value. In the above example of tweets which are sorted up to the second, the radius is bounded by the number of tweets in a second.\\n\\nDefinition\\nGiven a positive number \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  , a sequence \\n  \\n    \\n      \\n        [\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle [a_{1},\\\\dots ,a_{n}]}\\n   is said to be \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted if for each \\n  \\n    \\n      \\n        1\\n        \\u2264\\n        i\\n      \\n    \\n    {\\\\displaystyle 1\\\\leq i}\\n   and for each \\n  \\n    \\n      \\n        i\\n        +\\n        k\\n        \\u2264\\n        j\\n        \\u2264\\n        n\\n      \\n    \\n    {\\\\displaystyle i+k\\\\leq j\\\\leq n}\\n  , \\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n        \\u2264\\n        \\n          a\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{i}\\\\leq a_{j}}\\n  . That is, the sequence has to be ordered only for pairs of elements whose distance is at least \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  .\\nThe radius of the sequence \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  , denoted \\n  \\n    \\n      \\n        \\n          ROUGH\\n        \\n        (\\n        \\u03b1\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\text{ROUGH}}(\\\\alpha )}\\n   or \\n  \\n    \\n      \\n        \\n          Par\\n        \\n        (\\n        \\u03b1\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\text{Par}}(\\\\alpha )}\\n   is the smallest \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   such that the sequence is \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted. The radius is a measure of presortedness.\\nA sequence is said to be nearly-sorted or roughly-sorted if its radius is small compared to its length.\\n\\nEquivalent definition\\nA sequence \\n  \\n    \\n      \\n        [\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle [a_{1},\\\\dots ,a_{n}]}\\n   is \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted if and only if each range of length \\n  \\n    \\n      \\n        2\\n        k\\n        +\\n        2\\n      \\n    \\n    {\\\\displaystyle 2k+2}\\n  , \\n  \\n    \\n      \\n        [\\n        \\n          a\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          a\\n          \\n            i\\n            +\\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            i\\n            +\\n            2\\n            k\\n            +\\n            2\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle [a_{i},a_{i+1},\\\\dots ,a_{i+2k+2}]}\\n   is \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted.\\n\\nProperties\\nAll sequences of length \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   are \\n  \\n    \\n      \\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (n-1)}\\n  -sorted, that is, \\n  \\n    \\n      \\n        0\\n        \\u2264\\n        \\n          Par\\n        \\n        (\\n        [\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        ]\\n        )\\n        <\\n        n\\n      \\n    \\n    {\\\\displaystyle 0\\\\leq {\\\\text{Par}}([a_{1},\\\\dots ,a_{n}])<n}\\n  . A sequence is \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n  -sorted if and only if it is sorted. A \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted sequence is automatically \\n  \\n    \\n      \\n        (\\n        k\\n        +\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (k+1)}\\n  -sorted but not necessarily \\n  \\n    \\n      \\n        (\\n        k\\n        \\u2212\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (k-1)}\\n  -sorted.\\n\\nRelation with sorted sequences\\nGiven a sequence a \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted sequence \\n  \\n    \\n      \\n        [\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle [a_{1},\\\\dots ,a_{n}]}\\n   and its sorted permutation \\n  \\n    \\n      \\n        [\\n        \\n          a\\n          \\n            \\n              \\u03c3\\n              \\n                1\\n              \\n            \\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            \\n              \\u03c3\\n              \\n                n\\n              \\n            \\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle [a_{\\\\sigma _{1}},\\\\dots ,a_{\\\\sigma _{n}}]}\\n  , \\n  \\n    \\n      \\n        \\n          |\\n        \\n        i\\n        \\u2212\\n        \\n          \\u03c3\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle |i-\\\\sigma _{i}|}\\n   is at most \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  .\\n\\nAlgorithms\\nDeciding whether a sequence is\\nk\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted\\nDeciding whether a sequence \\n  \\n    \\n      \\n        [\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle [a_{1},\\\\dots ,a_{n}]}\\n    is \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted can be done in linear time and constant space by a streaming algorithm. It suffices, for each \\n  \\n    \\n      \\n        1\\n        \\u2264\\n        i\\n        <\\n        n\\n        \\u2212\\n        k\\n      \\n    \\n    {\\\\displaystyle 1\\\\leq i<n-k}\\n  , to keep track of \\n  \\n    \\n      \\n        max\\n        (\\n        \\n          a\\n          \\n            j\\n          \\n        \\n        \\u2223\\n        j\\n        \\u2264\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\max(a_{j}\\\\mid j\\\\leq i)}\\n   and to check that \\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n            +\\n            k\\n          \\n        \\n        \\u2265\\n        max\\n        (\\n        \\n          a\\n          \\n            j\\n          \\n        \\n        \\u2223\\n        j\\n        \\u2264\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle a_{i+k}\\\\geq \\\\max(a_{j}\\\\mid j\\\\leq i)}\\n  .\\n\\nComputing the radius of a sequence\\nComputing the radius of a sequence can be computed in linear time and space. This follows from the fact that it can be defined as \\n  \\n    \\n      \\n        max\\n        (\\n        i\\n        \\u2212\\n        j\\n        \\u2223\\n        min\\n        (\\n        \\n          a\\n          \\n            k\\n          \\n        \\n        \\u2223\\n        k\\n        \\u2265\\n        i\\n        )\\n        <\\n        max\\n        (\\n        \\n          a\\n          \\n            k\\n          \\n        \\n        \\u2223\\n        k\\n        \\u2264\\n        j\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\max(i-j\\\\mid \\\\min(a_{k}\\\\mid k\\\\geq i)<\\\\max(a_{k}\\\\mid k\\\\leq j))}\\n  .\\n\\nHalving the radius of a sequence\\nGiven a \\n  \\n    \\n      \\n        2\\n        k\\n      \\n    \\n    {\\\\displaystyle 2k}\\n  -sorted sequence \\n  \\n    \\n      \\n        \\u03b1\\n        =\\n        [\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\alpha =[a_{1},\\\\dots ,a_{n}]}\\n  , it is possible to compute a \\n  \\n    \\n      \\n        (\\n        k\\n        \\u2212\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (k-1)}\\n  -sorted permutation \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\u2032\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha '}\\n   of \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   in linear time and constant space.\\nFirst, given a sequence \\n  \\n    \\n      \\n        \\u03b2\\n        =\\n        [\\n        \\n          b\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          b\\n          \\n            2\\n            k\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\beta =[b_{1},\\\\dots ,b_{2k}]}\\n  , lets say that this sequence is partitioned if the \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -smaller elements are in \\n  \\n    \\n      \\n        [\\n        \\n          b\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          b\\n          \\n            k\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle [b_{1},\\\\dots ,b_{k}]}\\n   and the \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -greater elements are in \\n  \\n    \\n      \\n        [\\n        \\n          b\\n          \\n            k\\n            +\\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          b\\n          \\n            2\\n            k\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle [b_{k+1},\\\\dots ,b_{2k}]}\\n  . Lets call partitioning the action of reordering the sequence \\n  \\n    \\n      \\n        \\u03b2\\n      \\n    \\n    {\\\\displaystyle \\\\beta }\\n   into a partitioned permutation. This can be done in linear time by first finding the median of \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   and then moving elements to the first or second half depending on whether they are smaller or greater than the median.\\nThe sequence \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\u2032\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha '}\\n   can be obtained by partitioning the blocks of elements at position \\n  \\n    \\n      \\n        [\\n        2\\n        k\\n        (\\n        i\\n        \\u2212\\n        1\\n        )\\n        +\\n        1\\n        ,\\n        \\u2026\\n        ,\\n        2\\n        k\\n        i\\n        ]\\n      \\n    \\n    {\\\\displaystyle [2k(i-1)+1,\\\\dots ,2ki]}\\n  , then by partitioning the blocks of elements at position \\n  \\n    \\n      \\n        [\\n        2\\n        k\\n        (\\n        i\\n        \\u2212\\n        1\\n        )\\n        +\\n        k\\n        +\\n        1\\n        ,\\n        \\u2026\\n        ,\\n        2\\n        k\\n        i\\n        +\\n        k\\n        ]\\n      \\n    \\n    {\\\\displaystyle [2k(i-1)+k+1,\\\\dots ,2ki+k]}\\n  , and then again the elements at position  \\n  \\n    \\n      \\n        [\\n        2\\n        k\\n        (\\n        i\\n        \\u2212\\n        1\\n        )\\n        +\\n        1\\n        ,\\n        \\u2026\\n        ,\\n        2\\n        k\\n        i\\n        ]\\n      \\n    \\n    {\\\\displaystyle [2k(i-1)+1,\\\\dots ,2ki]}\\n   for each number \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   such that those sequences are defined.\\nUsing \\n  \\n    \\n      \\n        \\n          \\n            n\\n            \\n              2\\n              k\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{2k}}}\\n   processors, with no shared read nor write access to memory, the same algorithm can be applied in \\n  \\n    \\n      \\n        O\\n        (\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(k)}\\n   time, since each partition of a sequence can occur in parallel.\\n\\nMerging\\nk\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted sequences\\n\\nMerging two \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted sequences \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            1\\n          \\n        \\n        =\\n        [\\n        \\n          a\\n          \\n            1\\n          \\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n          \\n            1\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{1}=[a_{1}^{1},\\\\dots ,a_{n}^{1}]}\\n   and \\n  \\n    \\n      \\n        \\u03b1\\n        =\\n        [\\n        \\n          a\\n          \\n            1\\n          \\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            m\\n          \\n          \\n            2\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\alpha =[a_{1}^{2},\\\\dots ,a_{m}^{2}]}\\n   can be done in linear time and constant space.\\nFirst, using the preceding algorithm, both sequences should be transformed into \\n  \\n    \\n      \\n        k\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle k/2}\\n  -sorted sequences.\\nLet's construct iteratively an output sequence \\n  \\n    \\n      \\n        \\u03c9\\n      \\n    \\n    {\\\\displaystyle \\\\omega }\\n   by removing content from both \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{i}}\\n   and adding it in \\n  \\n    \\n      \\n        \\u03c9\\n      \\n    \\n    {\\\\displaystyle \\\\omega }\\n  .\\nIf both \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{i}}\\n  's are empty, then it suffices to return \\n  \\n    \\n      \\n        \\u03c9\\n      \\n    \\n    {\\\\displaystyle \\\\omega }\\n  . Otherwise, let us assume that \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{1}}\\n   is empty and not \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{2}}\\n  , it suffices to remove the content of \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{2}}\\n   and append it to \\n  \\n    \\n      \\n        \\u03c9\\n      \\n    \\n    {\\\\displaystyle \\\\omega }\\n  . The case where \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{2}}\\n   is empty and not \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{1}}\\n   is similar by symmetry.\\nLet us consider the case where neither \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{i}}\\n   is empty. Let us call \\n  \\n    \\n      \\n        \\n          A\\n          \\n            1\\n          \\n        \\n        =\\n        max\\n        (\\n        \\n          a\\n          \\n            i\\n          \\n          \\n            1\\n          \\n        \\n        \\u2223\\n        1\\n        \\u2264\\n        i\\n        \\u2264\\n        k\\n        \\n          /\\n        \\n        2\\n        )\\n      \\n    \\n    {\\\\displaystyle A^{1}=\\\\max(a_{i}^{1}\\\\mid 1\\\\leq i\\\\leq k/2)}\\n   and \\n  \\n    \\n      \\n        \\n          A\\n          \\n            2\\n          \\n        \\n        =\\n        max\\n        (\\n        \\n          a\\n          \\n            i\\n          \\n          \\n            2\\n          \\n        \\n        \\u2223\\n        1\\n        \\u2264\\n        i\\n        \\u2264\\n        k\\n        \\n          /\\n        \\n        2\\n        )\\n      \\n    \\n    {\\\\displaystyle A^{2}=\\\\max(a_{i}^{2}\\\\mid 1\\\\leq i\\\\leq k/2)}\\n  , they are the greatest of the \\n  \\n    \\n      \\n        k\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle k/2}\\n  -firsts\\nelements of each list. Let us assume that \\n  \\n    \\n      \\n        \\n          A\\n          \\n            1\\n          \\n        \\n        <\\n        \\n          A\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A^{1}<A^{2}}\\n  , the other case is similar by symmetry. Remove\\n\\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            k\\n            \\n              /\\n            \\n            2\\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{1}^{1},\\\\dots ,a_{k/2}^{1}}\\n   from \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{1}}\\n   and remove \\n  \\n    \\n      \\n        {\\n        \\n          a\\n          \\n            j\\n          \\n          \\n            2\\n          \\n        \\n        \\u2223\\n        1\\n        \\u2264\\n        j\\n        \\u2264\\n        k\\n        \\n          /\\n        \\n        2\\n        ,\\n        \\n          a\\n          \\n            j\\n          \\n          \\n            2\\n          \\n        \\n        \\u2264\\n        \\n          A\\n          \\n            1\\n          \\n        \\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{a_{j}^{2}\\\\mid 1\\\\leq j\\\\leq k/2,a_{j}^{2}\\\\leq A^{1}\\\\}}\\n   from \\n  \\n    \\n      \\n        \\n          \\u03b1\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha ^{2}}\\n   and add them to \\n  \\n    \\n      \\n        \\u03c9\\n      \\n    \\n    {\\\\displaystyle \\\\omega }\\n  .\\n\\nSorting a\\nk\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted sequence\\nA \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorted sequence can be sorted by applying the halving algorithm given above \\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}(k)}\\n   times. This takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log k)}\\n   time on a sequential machine, or \\n  \\n    \\n      \\n        O\\n        (\\n        k\\n        log\\n        \\u2061\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(k\\\\log k)}\\n   time using \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   processors.\\n\\n\\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorting a sequence\\nSince each sequence \\n  \\n    \\n      \\n        \\u03b1\\n        =\\n        [\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\alpha =[a_{1},\\\\dots ,a_{n}]}\\n   is necessarily \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  -sorted, it suffices to apply the halving algorithm \\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        \\n          (\\n          \\n            \\n              n\\n              k\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}\\\\left({\\\\frac {n}{k}}\\\\right)}\\n  -times. Thus, \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -sorting can be done in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        (\\n        n\\n        \\n          /\\n        \\n        k\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log(n/k))}\\n  -time. This algorithm is Par-optimal, that is, there exists no sequential algorithm with a better worst-case complexity.\\n\\n\\n== References ==\"}, {\"Counting sort\": \"In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small positive integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that possess distinct key values, and applying prefix sum on those counts to determine the positions of each key value in the output sequence. Its running time is linear in the number of items and the difference between the maximum key value and the minimum key value, so it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items. It is often used as a subroutine in radix sort, another sorting algorithm, which can handle larger keys more efficiently.Counting sort is not a comparison sort; it uses key values as indexes into an array and the \\u03a9(n log n) lower bound for comparison sorting will not apply. Bucket sort may be used in lieu of counting sort, and entails a similar time analysis. However, compared to counting sort, bucket sort requires linked lists, dynamic arrays, or a large amount of pre-allocated memory to hold the sets of items within each bucket, whereas counting sort stores a single number (the count of items) per bucket.\\n\\nInput and output assumptions\\nIn the most general case, the input to counting sort consists of a collection of n items, each of which has a non-negative integer key whose maximum value is at most k.\\nIn some descriptions of counting sort, the input to be sorted is assumed to be more simply a sequence of integers itself, but this simplification does not accommodate many applications of counting sort. For instance, when used as a subroutine in radix sort, the keys for each call to counting sort are individual digits of larger item keys; it would not suffice to return only a sorted list of the key digits, separated from the items.\\nIn applications such as in radix sort, a bound on the maximum key value k will be known in advance, and can be assumed to be part of the input to the algorithm. However, if the value of k is not already known then it may be computed, as a first step, by an additional loop over the data to determine the maximum key value.\\nThe output is an array of the elements ordered by their keys. Because of its application to radix sorting, counting sort must be a stable sort; that is, if two elements share the same key, their relative order in the output array and their relative order in the input array should match.\\n\\nPseudocode\\nIn pseudocode, the algorithm may be expressed as:\\n\\nfunction CountingSort(input, k)\\n    \\n    count \\u2190 array of k + 1 zeros\\n    output \\u2190 array of same length as input\\n    \\n    for i = 0 to length(input) - 1 do\\n        j = key(input[i])\\n        count[j] = count[j] + 1\\n\\n    for i = 1 to k do\\n        count[i] = count[i] + count[i - 1]\\n\\n    for i = length(input) - 1 down to 0 do\\n        j = key(input[i])\\n        count[j] = count[j] - 1\\n        output[count[j]] = input[i]\\n\\n    return output\\n\\nHere input is the input array to be sorted, key returns the numeric key of each item in the input array, count is an auxiliary array used first to store the numbers of items with each key, and then (after the second loop) to store the positions where items with each key should be placed,\\nk is the maximum value of the non-negative key values and output is the sorted output array.\\nIn summary, the algorithm loops over the items in the first loop, computing a histogram of the number of times each key occurs within the input collection. After that in the second loop, it performs a prefix sum computation on count in order to determine, for each key, the position range where the items having that key should be placed; i.e. items of key \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   should be placed starting in position count[\\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  ]. Finally, in the third loop, it loops over the items of input again, but in reverse order, moving each item into its sorted position in the output array.The relative order of items with equal keys is preserved here; i.e., this is a stable sort.\\n\\nComplexity analysis\\nBecause the algorithm uses only simple for loops, without recursion or subroutine calls, it is straightforward to analyze. The initialization of the count array, and the second for loop which performs a prefix sum on the count array, each iterate at most k + 1 times and therefore take O(k) time. The other two for loops, and the initialization of the output array, each take O(n) time. Therefore, the time for the whole algorithm is the sum of the times for these steps, O(n + k).Because it uses arrays of length k + 1 and n, the total space usage of the algorithm is also O(n + k). For problem instances in which the maximum key value is significantly smaller than the number of items, counting sort can be highly space-efficient, as the only storage it uses other than its input and output arrays is the Count array which uses space O(k).\\n\\nVariant algorithms\\nIf each item to be sorted is itself an integer, and used as key as well, then the second and third loops of counting sort can be combined; in the second loop, instead of computing the position where items with key i should be placed in the output, simply append Count[i] copies of the number i to the output.\\nThis algorithm may also be used to eliminate duplicate keys, by replacing the Count array with a bit vector that stores a one for a key that is present in the input and a zero for a key that is not present. If additionally the items are the integer keys themselves, both second and third loops can be omitted entirely and the bit vector will itself serve as output, representing the values as offsets of the non-zero entries, added to the range's lowest value. Thus the keys are sorted and the duplicates are eliminated in this variant just by being placed into the bit array.\\nFor data in which the maximum key size is significantly smaller than the number of data items, counting sort may be parallelized by splitting the input into subarrays of approximately equal size, processing each subarray in parallel to generate a separate count array for each subarray, and then merging the count arrays. When used as part of a parallel radix sort algorithm, the key size (base of the radix representation) should be chosen to match the size of the split subarrays. The simplicity of the counting sort algorithm and its use of the easily parallelizable prefix sum primitive also make it usable in more fine-grained parallel algorithms.As described, counting sort is not an in-place algorithm; even disregarding the count array, it needs separate input and output arrays. It is possible to modify the algorithm so that it places the items into sorted order within the same array that was given to it as the input, using only the count array as auxiliary storage; however, the modified in-place version of counting sort is not stable.\\n\\nHistory\\nAlthough radix sorting itself dates back far longer,\\ncounting sort, and its application to radix sorting, were both invented by Harold H. Seward in 1954.\\n\\nReferences\\nExternal links\\n\\nCounting Sort html5 visualization\\nDemonstration applet from Cardiff University Archived 2013-06-02 at the Wayback Machine\\nKagel, Art S. (2 June 2006), \\\"counting sort\\\",  in Black, Paul E. (ed.), Dictionary of Algorithms and Data Structures, U.S. National Institute of Standards and Technology, retrieved 2011-04-21.\"}, {\"Cubesort\": \"Cubesort is a parallel sorting algorithm that builds a self-balancing multi-dimensional array from the keys to be sorted. As the axes are of similar length the structure resembles a cube. After each key is inserted the cube can be rapidly converted to an array.A cubesort implementation written in C was published in 2014.\\n\\nOperation\\nCubesort's algorithm uses a specialized binary search on each axis to find the location to insert an element. When an axis grows too large it is split. Locality of reference is optimal as only four binary searches are performed on small arrays for each insertion. By using many small dynamic arrays the high cost for insertion on single large arrays is avoided.\\n\\nReferences\\nExternal links\\nCubesort description and implementation in C\\nNiedermeier, Rolf (1996). \\\"Recursively divisible problems\\\". Algorithms and Computation. Lecture Notes in Computer Science. Vol. 1178. Springer Berlin Heidelberg. pp. 187\\u2013188. doi:10.1007/BFb0009494. eISSN 1611-3349. ISBN 978-3-540-62048-8. ISSN 0302-9743. (passing mention)\"}, {\"Cycle sort\": \"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result.\\nUnlike nearly every other sort, items are never written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort.\\nMinimizing the number of writes is useful when making writes to some huge data set is very expensive, such as with EEPROMs like Flash memory where each write reduces the lifespan of the memory.\\n\\nAlgorithm\\nTo illustrate the idea of cycle sort, consider a list with distinct elements. Given an element \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  , we can find the index at which it will occur in the sorted list by simply counting the number of elements in the entire list that are smaller than \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  . Now\\n\\nIf the element is already at the correct position, do nothing.\\nIf it is not, we will write it to its intended position. That position is inhabited by a different element \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  , which we then have to move to its correct position. This process of displacing elements to their correct positions continues until an element is moved to the original position of \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  . This completes a cycle.\\nRepeating this process for every element sorts the list, with a single writing operation if and only if an element is not already at its correct position. While computing the correct positions takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time for every single element, thus resulting in a quadratic time algorithm, the number of writing operations is minimized.\\n\\nImplementation\\nTo create a working implementation from the above outline, two issues need to be addressed:\\n\\nWhen computing the correct positions, we have to make sure not to double-count the first element of the cycle.\\nIf there are duplicate elements present, when we try to move an element \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   to its correct position, that position might already be inhabited by an \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  . Simply swapping these would cause the algorithm to cycle indefinitely. Instead, we have to insert the element after any of its duplicates.The following Python implementation performs cycle sort on an array, counting the number of writes to that array that were needed to sort it.\\nPython\\n\\nThe next implementation written in C++ simply performs cyclic array sorting.\\n\\nSituation-specific optimizations\\nWhen the array contains only duplicates of a relatively small number of items, a constant-time perfect hash function can greatly speed up finding where to put an item1, turning the sort from \\u0398(n2) time to \\u0398(n + k) time, where k is the total number of hashes. The array ends up sorted in the order of the hashes, so choosing a hash function that gives you the right ordering is important.\\nBefore the sort, create a histogram, sorted by hash, counting the number of occurrences of each hash in the array. Then create a table with the cumulative sum of each entry in the histogram. The cumulative sum table will then contain the position in the array of each element. The proper place of elements can then be found by a constant-time hashing and cumulative sum table lookup rather than a linear search.\\n\\nReferences\\nExternal links\\n^  \\\"Cycle-Sort: A Linear Sorting Method\\\", The Computer Journal (1990) 33 (4): 365-367.\\n\\nOriginal source of unrestricted variant\\nCyclesort - a curious little sorting algorithm\"}, {\"Dutch national flag problem\": \"The Dutch national flag problem is a computational problem proposed by Edsger Dijkstra. The flag of the Netherlands consists of three colors: red, white, and blue. Given balls of these three colors arranged randomly in a line (it does not matter how many balls there are), the task is to arrange them such that all balls of the same color are together and their collective color groups are in the correct order.\\nThe solution to this problem is of interest for designing sorting algorithms; in particular, variants of the quicksort algorithm that must be robust to repeated elements may use a three-way partitioning function that groups items less than a given key (red), equal to the key (white) and greater than the key (blue). Several solutions exist that have varying performance characteristics, tailored to sorting arrays with either small or large numbers of repeated elements.\\n\\nThe array case\\nThis problem can also be viewed in terms of rearranging elements of an array.\\nSuppose each of the possible elements could be classified into exactly one of three categories (bottom, middle, and top).\\nFor example, if all the elements are in 0 ... 1, the bottom could be defined as elements in 0 ... 0.25 (not including 0.25), the middle as 0.25 ... 0.5 (not including 0.5)\\nand the top as 0.5 and greater.  (The choice of these values illustrates that the categories need not be equal ranges). The problem is then to produce an array such that all \\\"bottom\\\" elements come before (have an index less than the index of) all \\\"middle\\\" elements, which come before all \\\"top\\\" elements.\\nOne algorithm is to have the top group grow down from the top of the array, the bottom group grow up from the bottom, and keep the middle group just above the bottom. The algorithm indexes three locations, the bottom of the top group, the top of the bottom group, and the top of the middle group. Elements that are yet to be sorted fall between the middle and the top group. At each step, examine the element just above the middle. If it belongs to the top group, swap it with the element just below the top. If it belongs in the bottom, swap it with the element just above the bottom. If it is in the middle, leave it. Update the appropriate index. Complexity is \\u0398(n) moves and examinations.\\n\\nPseudocode\\nThe following pseudocode for three-way partitioning which assumes zero-based array indexing was proposed by Dijkstra himself. It uses three indices i, j and k, maintaining the invariant that i \\u2264 j \\u2264 k.\\n\\nEntries from 0 up to (but not including) i are values less than mid,\\nentries from i up to (but not including) j are values equal to mid,\\nentries from j up to (and including) k are values not yet sorted, and\\nentries from k + 1 to the end of the array are values greater than mid.procedure three-way-partition(A : array of values, mid : value):\\n    i \\u2190 0\\n    j \\u2190 0\\n    k \\u2190 size of A - 1\\n\\n    while j <= k:\\n        if A[j] < mid:\\n            swap A[i] and A[j]\\n            i \\u2190 i + 1\\n            j \\u2190 j + 1\\n        else if A[j] > mid:\\n            swap A[j] and A[k]\\n            k \\u2190 k - 1\\n        else:\\n            j \\u2190 j + 1\\n\\nSee also\\nAmerican flag sort\\n\\nReferences\\nExternal links\\nExplanation and interactive explanatory execution of the algorithm, sorting two or three colors\"}, {\"Elevator algorithm\": \"The elevator algorithm (also SCAN) is a disk-scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\\nThis algorithm is named after the behavior of a building elevator, where the elevator continues to travel in its current direction (up or down) until empty, stopping only to let individuals off or to pick up new individuals heading in the same direction.  \\nFrom an implementation perspective, the drive maintains a buffer of pending read/write requests, along with the associated cylinder number of the request, in which lower cylinder numbers generally indicate that the cylinder is closer to the spindle, and higher numbers indicate the cylinder is farther away.\\n\\nDescription\\nWhen a new request arrives while the drive is idle, the initial arm/head movement will be in the direction of the cylinder where the data is stored, either in or out.  As additional requests arrive, requests are serviced only in the current direction of arm movement until the arm reaches the edge of the disk. When this happens, the direction of the arm reverses, and the requests that were remaining in the opposite direction are serviced, and so on.\\n\\nVariations\\nOne variation of this method ensures all requests are serviced in only one direction, that is, once the head has arrived at the outer edge of the disk, it returns to the beginning and services the new requests in this one direction only (or vice versa).  This is known as the \\\"Circular Elevator Algorithm\\\" or C-SCAN.  Although the time of the return seek is wasted, this results in more equal performance for all head positions, as the expected distance from the head is always half the maximum distance, unlike in the standard elevator algorithm where cylinders in the middle will be serviced as much as twice as often as the innermost or outermost cylinders.\\nOther variations include:\\n\\nFSCAN\\nLOOK (and C-LOOK)\\nN-Step-SCAN\\n\\nExample\\nThe following is an example of how to calculate average disk seek times for both the SCAN and C-SCAN algorithms.\\n\\nExample list of pending disk requests (listed by track number): 100, 50, 10, 20, 75.\\nThe starting track number for the examples will be 35.\\nThe list will need to be sorted in ascending order: 10, 20, 50, 75, 100.Both SCAN and C-SCAN behave in the same manner until they reach the last track queued. For the sake of this example let us assume that the SCAN algorithm is currently going from a lower track number to a higher track number (like the C-SCAN is doing). For both methods, one takes the difference in magnitude (i.e. absolute value) between the next track request and the current track.\\n\\nSeek 1: 50 \\u2212 35 = 15\\nSeek 2: 75 \\u2212 50 = 25\\nSeek 3: 100 \\u2212 75 = 25At this point both have reached the highest (end) track request. SCAN will just reverse direction and service the next closest disk request (in this example, 20) and C-SCAN will always go back to track 0 and start going to higher track requests.\\n\\nSeek 4 (SCAN): 20 \\u2212 100 = 80\\nSeek 5 (SCAN): 10 \\u2212 20 = 10\\nTotal (SCAN): 155\\nAverage (SCAN): 155 \\u00f7 5 = 31\\nSeek 4 (C-SCAN): 0 \\u2212 100 = 0 head movement as cylinders are treated as a  circular list (C-SCAN always goes back to the first track)\\nSeek 5 (C-SCAN): 10 \\u2212 0 = 10\\nSeek 6 (C-SCAN): 20 \\u2212 10 = 10\\nTotal (C-SCAN): 85\\nAverage (C-SCAN): 85 \\u00f7 5 = 17Even though six seeks were performed using the C-SCAN algorithm, only five I/Os were actually done.\\n\\nAnalysis\\nFor both versions of the elevator algorithm, the arm movement is less than twice the number of total cylinders and produces a smaller variance in response time. The algorithm is also relatively simple. \\nThe elevator algorithm is not always better than shortest seek first, which is slightly closer to optimal, but can result in high variance in response time and even in starvation when new requests continually get serviced prior to existing requests. Anti-starvation techniques can be applied to the shortest seek time first algorithm to guarantee a maximum response time.\\n\\nSee also\\nFCFS\\nShortest seek time first\\n\\n\\n== References ==\"}, {\"External sorting\": \"External sorting is a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted do not fit into the main memory of a computing device (usually RAM) and instead they must reside in the slower external memory, usually a disk drive. Thus, external sorting algorithms are external memory algorithms and thus applicable in the external memory model of computation.\\nExternal sorting algorithms generally fall into two types, distribution sorting, which resembles quicksort, and external merge sort, which resembles merge sort. External merge sort typically uses a hybrid sort-merge strategy.  In the sorting phase, chunks of data small enough to fit in main memory are read, sorted, and written out to a temporary file.  In the merge phase, the sorted subfiles are combined into a single larger file.\\n\\nModel\\nExternal sorting algorithms can be analyzed in the external memory model. In this model, a cache or internal memory of size M and an unbounded external memory are divided into blocks of size B, and the running time of an algorithm is determined by the number of memory transfers between internal and external memory. Like their cache-oblivious counterparts, asymptotically optimal external sorting algorithms achieve a running time (in Big O notation) of \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            \\n              \\n                \\n                  N\\n                  B\\n                \\n              \\n            \\n            \\n              log\\n              \\n                \\n                  \\n                    M\\n                    B\\n                  \\n                \\n              \\n            \\n            \\u2061\\n            \\n              \\n                \\n                  N\\n                  B\\n                \\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left({\\\\tfrac {N}{B}}\\\\log _{\\\\tfrac {M}{B}}{\\\\tfrac {N}{B}}\\\\right)}\\n  .\\n\\nExternal merge sort\\nOne example of external sorting is the external merge sort algorithm, which is a K-way merge algorithm. It sorts chunks that each fit in RAM, then merges the sorted chunks together.The algorithm first sorts M items at a time and puts the sorted lists back into external memory. It then recursively does a \\n  \\n    \\n      \\n        \\n          \\n            \\n              M\\n              B\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {M}{B}}}\\n  -way merge on those sorted lists. To do this merge, B elements from each sorted list are loaded into internal memory, and the minimum is repeatedly outputted.\\nFor example, for sorting 900 megabytes of data using only 100 megabytes of RAM:\\n\\nRead 100 MB of the data in main memory and sort by some conventional method, like quicksort.\\nWrite the sorted data to disk.\\nRepeat steps 1 and 2 until all of the data is in sorted 100 MB chunks (there are 900MB / 100MB = 9 chunks), which now need to be merged into one single output file.\\nRead the first 10 MB (= 100MB / (9 chunks + 1)) of each sorted chunk into input buffers in main memory and allocate the remaining 10 MB for an output buffer.  (In practice, it might provide better performance to make the output buffer larger and the input buffers slightly smaller.)\\nPerform a 9-way merge and store the result in the output buffer. Whenever the output buffer fills, write it to the final sorted file and empty it. Whenever any of the 9 input buffers empties, fill it with the next 10 MB of its associated 100 MB sorted chunk until no more data from the chunk is available. This is the key step that makes external merge sort work externally\\u2014because the merge algorithm only makes one pass sequentially through each of the chunks, each chunk does not have to be loaded completely; rather, sequential parts of the chunk can be loaded as needed.Historically, instead of a sort, sometimes a replacement-selection algorithm was used to perform the initial distribution, to produce on average half as many output chunks of double the length.\\n\\nAdditional passes\\nThe previous example is a two-pass sort: first sort, then merge. The sort ends with a single k-way merge, rather than a series of two-way merge passes as in a typical in-memory merge sort. This is because each merge pass reads and writes every value from and to disk, so reducing the number of passes more than compensates for the additional cost of a k-way merge.\\nThe limitation to single-pass merging is that as the number of chunks increases, memory will be divided into more buffers, so each buffer is smaller. Eventually, the reads become so small that more time is spent on disk seeks than data transfer.  A typical magnetic hard disk drive might have a 10 ms access time and 100 MB/s data transfer rate, so each seek takes as much time as transferring 1 MB of data.\\nThus, for sorting, say, 50 GB in 100 MB of RAM, using a single 500-way merge pass isn't efficient: we can only read 100 MB / 501 \\u2248 200 KB from each chunk at once, so 5/6 of the disk's time is spent seeking.  Using two merge passes solves the problem.  Then the sorting process might look like this:\\n\\nRun the initial chunk-sorting pass as before to create 500\\u00d7100 MB sorted chunks.\\nRun a first merge pass combining 25\\u00d7100 MB chunks at a time, resulting in 20\\u00d72.5 GB sorted chunks.\\nRun a second merge pass to merge the 20\\u00d72.5 GB sorted chunks into a single 50 GB sorted resultAlthough this requires an additional pass over the data, each read is now 4 MB long, so only 1/5 of the disk's time is spent seeking.  The improvement in data transfer efficiency during the merge passes (16.6% to 80% is almost a 5\\u00d7 improvement) more than makes up for the doubled number of merge passes.\\nVariations include using an intermediate medium like solid-state disk for some stages; the fast temporary storage needn't be big enough to hold the whole dataset, just substantially larger than available main memory. Repeating the example above with 1 GB of temporary SSD storage, the first pass could merge 10\\u00d7100 MB sorted chunks read from that temporary space to write 50x1 GB sorted chunks to HDD. The high bandwidth and random-read throughput of SSDs help speed the first pass, and the HDD reads for the second pass can then be 2 MB, large enough that seeks will not take up most of the read time. SSDs can also be used as read buffers in a merge phase, allowing fewer larger reads (20MB reads in this example) from HDD storage. Given the lower cost of SSD capacity relative to RAM, SSDs can be an economical tool for sorting large inputs with very limited memory.\\nLike in-memory sorts, efficient external sorts require O(n log n) time: exponentially growing datasets require linearly increasing numbers of passes that each take O(n) time. Under reasonable assumptions at least 500 GB of data stored on a hard drive can be sorted using 1 GB of main memory before a third pass becomes advantageous, and many times that much data can be sorted before a fourth pass becomes useful.Main memory size is important. Doubling memory dedicated to sorting halves the number of chunks and the number of reads per chunk, reducing the number of seeks required by about three-quarters. The ratio of RAM to disk storage on servers often makes it convenient to do huge sorts on a cluster of machines rather than on one machine with multiple passes. Media with high random-read performance like solid-state drives (SSDs) also increase the amount that can be sorted before additional passes improve performance.\\n\\nExternal distribution sort\\nExternal distribution sort is analogous to quicksort. The algorithm finds approximately \\n  \\n    \\n      \\n        \\n          \\n            \\n              M\\n              B\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {M}{B}}}\\n   pivots and uses them to divide the N elements into approximately equally sized subarrays, each of whose elements are all smaller than the next, and then recurse until the sizes of the subarrays are less than the block size. When the subarrays are less than the block size, sorting can be done quickly because all reads and writes are done in the cache, and in the external memory model requires \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   operations.\\nHowever, finding exactly \\n  \\n    \\n      \\n        \\n          \\n            \\n              M\\n              B\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {M}{B}}}\\n   pivots would not be fast enough to make the external distribution sort asymptotically optimal. Instead, we find slightly fewer pivots. To find these pivots, the algorithm splits the N input elements into \\n  \\n    \\n      \\n        \\n          \\n            \\n              N\\n              M\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {N}{M}}}\\n   chunks, and takes every \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                M\\n                \\n                  16\\n                  B\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {\\\\tfrac {M}{16B}}}}\\n   elements, and recursively uses the median of medians algorithm to find \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                M\\n                B\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {\\\\tfrac {M}{B}}}}\\n   pivots.There is a duality, or fundamental similarity, between merge- and distribution-based algorithms.\\n\\nPerformance\\nThe Sort Benchmark, created by computer scientist Jim Gray, compares external sorting algorithms implemented using finely tuned hardware and software.  Winning implementations use several techniques:\\n\\nUsing parallelism\\nMultiple disk drives can be used in parallel in order to improve sequential read and write speed.  This can be a very cost-efficient improvement: a Sort Benchmark winner in the cost-centric Penny Sort category uses six hard drives in an otherwise midrange machine.\\nSorting software can use multiple threads, to speed up the process on modern multicore computers.\\nSoftware can use asynchronous I/O so that one run of data can be sorted or merged while other runs are being read from or written to disk.\\nMultiple machines connected by fast network links can each sort part of a huge dataset in parallel.\\nIncreasing hardware speed\\nUsing more RAM for sorting can reduce the number of disk seeks and avoid the need for more passes.\\nFast external memory like solid-state drives can speed sorts, either if the data is small enough to fit entirely on SSDs or, more rarely, to accelerate sorting SSD-sized chunks in a three-pass sort.\\nMany other factors can affect hardware's maximum sorting speed: CPU speed and number of cores, RAM access latency, input/output bandwidth, disk read/write speed, disk seek time, and others. \\\"Balancing\\\" the hardware to minimize bottlenecks is an important part of designing an efficient sorting system.\\nCost-efficiency as well as absolute speed can be critical, especially in cluster environments where lower node costs allow purchasing more nodes.\\nIncreasing software speed\\nSome Sort Benchmark entrants use a variation on radix sort for the first phase of sorting: they separate data into one of many \\\"bins\\\" based on the beginning of its value.  Sort Benchmark data is random and especially well-suited to this optimization.\\nCompacting the input, intermediate files, and output can reduce time spent on I/O, but is not allowed in the Sort Benchmark.\\nBecause the Sort Benchmark sorts long (100-byte) records using short (10-byte) keys, sorting software sometimes rearranges the keys separately from the values to reduce memory I/O volume.\\n\\nSee also\\nMainframe sort merge\\nExternal memory algorithm\\nFunnelsort\\nCache-oblivious distribution sort\\n\\nReferences\\nExternal links\\nSTXXL, an algorithm toolkit including external mergesort\\nAn external mergesort example\\nA K-Way Merge Implementation\\nExternal-Memory Sorting in Java\\nA sample pennysort implementation using Judy Arrays\\nSort Benchmark\"}, {\"Flashsort\": \"Flashsort is a distribution sorting algorithm showing linear computational complexity O(n) for uniformly distributed data sets and relatively little additional memory requirement. The original work was published in 1998 by Karl-Dietrich Neubert.\\n\\nConcept\\nFlashsort is an efficient in-place implementation of histogram sort, itself a type of bucket sort.  It assigns each of the n input elements to one of m buckets, efficiently rearranges the input to place the buckets in the correct order, then sorts each bucket.  The original algorithm sorts an input array A as follows:\\n\\nUsing a first pass over the input or a priori knowledge, find the minimum and maximum sort keys.\\nLinearly divide the range [Amin, Amax] into m buckets.\\nMake one pass over the input, counting the number of elements Ai which fall into each bucket.  (Neubert calls the buckets \\\"classes\\\" and the assignment of elements to their buckets \\\"classification\\\".)\\nConvert the counts of elements in each bucket to a prefix sum, where Lb is the number of elements Ai in bucket b or less.  (L0 = 0 and Lm = n.)\\nRearrange the input to all elements of each bucket b are stored in positions Ai where Lb\\u22121 < i \\u2264 Lb.\\nSort each bucket using insertion sort.Steps 1\\u20133 and 6 are common to any bucket sort, and can be improved using techniques generic to bucket sorts.  In particular, the goal is for the buckets to be of approximately equal size (n/m elements each),  with the ideal being division into m quantiles.  While the basic algorithm is a linear interpolation sort, if the input distribution is known to be non-uniform, a non-linear division will more closely approximate this ideal.  Likewise, the final sort can use any of a number of techniques, including a recursive flash sort.\\nWhat distinguishes flash sort is step 5: an efficient O(n) in-place algorithm for collecting the elements of each bucket together in the correct relative order using only m words of additional memory.\\n\\nMemory efficient implementation\\nThe Flashsort rearrangement phase operates in cycles.  Elements start out \\\"unclassified\\\", then are moved to the correct bucket and considered \\\"classified\\\".  The basic procedure is to choose an unclassified element, find its correct bucket, exchange it with an unclassified element there (which must exist, because we counted the size of each bucket ahead of time), mark it as classified, and then repeat with the just-exchanged unclassified element.  Eventually, the element is exchanged with itself and the cycle ends.\\nThe details are easy to understand using two (word-sized) variables per bucket.  The clever part is the elimination of one of those variables, allowing twice as many buckets to be used and therefore half as much time spent on the final O(n2) sorting.\\nTo understand it with two variables per bucket, assume there are two arrays of m additional words: Kb is the (fixed) upper limit of bucket b (and K0 = 0), while Lb is a (movable) index into bucket b, so Kb\\u22121 \\u2264 Lb \\u2264 Kb.\\nWe maintain the loop invariant that each bucket is divided by Lb into an unclassified prefix (Ai for Kb\\u22121 < i \\u2264 Lb have yet to be moved to their target buckets) and a classified suffix (Ai for Lb < i \\u2264 Kb are all in the correct bucket and will not be moved again).  Initially Lb = Kb and all elements are unclassified.  As sorting proceeds, the Lb are decremented until Lb = Kb\\u22121 for all b and all elements are classified into the correct bucket.\\nEach round begins by finding the first incompletely classified bucket c (which has Kc\\u22121 < Lc) and taking the first unclassified element in that bucket Ai where i = Kc\\u22121 + 1.  (Neubert calls this the \\\"cycle leader\\\".)  Copy Ai to a temporary variable t and repeat:\\n\\nCompute the bucket b to which t belongs.\\nLet j = Lb be the location where t will be stored.\\nExchange t with Aj, i.e. store t in Aj while fetching the previous value Aj thereby displaced.\\nDecrement Lb to reflect the fact that Aj is now correctly classified.\\nIf j \\u2260 i, restart this loop with the new t.\\nIf j = i, this round is over and find a new first unclassified element Ai.\\nWhen there are no more unclassified elements, the distribution into buckets is complete.When implemented with two variables per bucket in this way, the choice of each round's starting point i is in fact arbitrary; any unclassified element may be used as a cycle leader.  The only requirement is that the cycle leaders can be found efficiently.\\nAlthough the preceding description uses K to find the cycle leaders, it is in fact possible to do without it, allowing the entire m-word array to be eliminated.  (After the distribution is complete, the bucket boundaries can be found in L.)\\nSuppose that we have classified all elements up to i\\u22121, and are considering Ai as a potential new cycle leader.  It is easy to compute its target bucket b.  By the loop invariant, it is classified if Lb < i \\u2264 Kb, and unclassified if i is outside that range.  The first inequality is easy to test, but the second appears to require the value Kb.\\nIt turns out that the induction hypothesis that all elements up to i\\u22121 are classified implies that i \\u2264 Kb, so it is not necessary to test the second inequality.\\nConsider the bucket c which position i falls into.  That is, Kc\\u22121 < i \\u2264 Kc.  By the induction hypothesis, all elements below i, which includes all buckets up to Kc\\u22121 < i, are completely classified. I.e. no elements which belong in those buckets remain in the rest of the array.  Therefore, it is not possible that b < c.\\nThe only remaining case is b \\u2265 c, which implies Kb \\u2265 Kc \\u2265 i, Q.E.D.\\nIncorporating this, the flashsort distribution algorithm begins with L as described above and i = 1.  Then proceed:\\nIf i > n, the distribution is complete.\\nGiven Ai, compute the bucket b to which it belongs.\\nIf i \\u2264 Lb, then Ai is unclassified.  Copy it a temporary variable t and:\\nLet j = Lb be the location where t will be stored.\\nExchange t with Aj, i.e. store t in Aj while fetching the previous value Aj thereby displaced.\\nDecrement Lb to reflect the fact that Aj is now correctly classified.\\nIf j \\u2260 i, compute the bucket b to which t belongs and restart this (inner) loop with the new t.\\nAi is now correctly classified.  Increment i and restart the (outer) loop.While saving memory, Flashsort has the disadvantage that it recomputes the bucket for many already-classified elements.  This is already done twice per element (once during the bucket-counting phase and a second time when moving each element), but searching for the first unclassified element requires a third computation for most elements.  This could be expensive if buckets are assigned using a more complex formula than simple linear interpolation.  A variant reduces the number of computations from almost 3n to at most 2n + m \\u2212 1 by taking the last unclassified element in an unfinished bucket as cycle leader:\\n\\nMaintain a variable c identifying the first incompletely-classified bucket.  Let c = 1 to begin with, and when c > m, the distribution is complete.\\nLet i = Lc.  If i = Lc\\u22121, increment c and restart this loop.  (L0 = 0.)\\nCompute the bucket b to which Ai belongs.\\nIf b < c, then Lc = Kc\\u22121 and we are done with bucket c.  Increment c and restart this loop.\\nIf b = c, the classification is trivial.  Decrement Lc and restart this loop.\\nIf  b > c, then Ai is unclassified.  Perform the same classification loop as the previous case, then restart this loop.Most elements have their buckets computed only twice, except for the final element in each bucket, which is used to detect the completion of the following bucket.  A small further reduction can be achieved by maintaining a count of unclassified elements and stopping when it reaches zero.\\n\\nPerformance\\nThe only extra memory requirements are the auxiliary vector L for storing bucket bounds and the constant number of other variables used.  Further, each element is moved (via a temporary buffer, so two move operations) only once.  However, this memory efficiency comes with the disadvantage that the array is accessed randomly, so cannot take advantage of a data cache smaller than the whole array.\\nAs with all bucket sorts, performance depends critically on the balance of the buckets.  In the ideal case of a balanced data set, each bucket will be approximately the same size. If the number m of buckets is linear in the input size n, each bucket has a constant size, so sorting a single bucket with an O(n2) algorithm like insertion sort has complexity O(12) = O(1).  The running time of the final insertion sorts is therefore m \\u22c5 O(1) = O(m) = O(n).\\nChoosing a value for m, the number of buckets, trades off time spent classifying elements (high m) and time spent in the final insertion sort step (low m).  For example, if m is chosen proportional to \\u221an, then the running time of the final insertion sorts is therefore m \\u22c5 O(\\u221an2) = O(n3/2).\\nIn the worst-case scenarios where almost all the elements are in a few buckets, the complexity of the algorithm is limited by the performance of the final bucket-sorting method, so degrades to O(n2). Variations of the algorithm improve worst-case performance by using better-performing sorts such as quicksort or recursive flashsort on buckets which exceed a certain size limit.For m = 0.1 n with uniformly distributed random data, flashsort is faster than heapsort for all n and faster than quicksort for n > 80. It becomes about twice as fast as quicksort at n = 10000.  Note that these measurements were taken in the late 1990s, when memory hierarchies were much less dependent on cacheing.\\nDue to the in situ permutation that flashsort performs in its classification process, flashsort is not stable. If stability is required, it is possible to use a second array so elements can be classified sequentially. However, in this case, the algorithm will require O(n) additional memory.\\n\\nSee also\\nInterpolation search, using the distribution of items for searching rather than sorting\\n\\nReferences\\nExternal links\\nImplementations of Randomized Sorting on Large Parallel Machines (1992)\\nImplementation of Parallel Algorithms (1992)\\nVisualization of Flashsort\"}, {\"Funnelsort\": \"Funnelsort is a comparison-based sorting algorithm. It is similar to mergesort, but it is a cache-oblivious algorithm, designed for a setting where the number of elements to sort is too large to fit in a cache where operations are done. It was introduced by Matteo Frigo, Charles Leiserson, Harald Prokop, and Sridhar Ramachandran in 1999 in the context of the cache oblivious model.\\n\\nMathematical properties\\nIn the external memory model, the number of memory transfers it needs to perform a sort of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   items on a machine with cache of size \\n  \\n    \\n      \\n        Z\\n      \\n    \\n    {\\\\displaystyle Z}\\n   and cache lines of length \\n  \\n    \\n      \\n        L\\n      \\n    \\n    {\\\\displaystyle L}\\n   is \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            \\n              \\n                \\n                  N\\n                  L\\n                \\n              \\n            \\n            \\n              log\\n              \\n                Z\\n              \\n            \\n            \\u2061\\n            N\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left({\\\\tfrac {N}{L}}\\\\log _{Z}N\\\\right)}\\n  , under the tall cache assumption that \\n  \\n    \\n      \\n        Z\\n        =\\n        \\u03a9\\n        (\\n        \\n          L\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle Z=\\\\Omega (L^{2})}\\n  . This number of memory transfers has been shown to be asymptotically optimal for comparison sorts. Funnelsort also achieves the asymptotically optimal runtime complexity of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        N\\n        log\\n        \\u2061\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (N\\\\log N)}\\n  .\\n\\nAlgorithm\\nBasic overview\\nFunnelsort operates on a contiguous array of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   elements. To sort the elements, it performs the following:\\n\\nSplit the input into \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n   arrays of size \\n  \\n    \\n      \\n        \\n          N\\n          \\n            2\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{2/3}}\\n  , and sort the arrays recursively.\\nMerge the \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n   sorted sequences using a \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n  -merger. (This process will be described in more detail.)Funnelsort is similar to merge sort in that some number of subarrays are recursively sorted, after which a merging step combines the subarrays into one sorted array. Merging is performed by a device called a k-merger, which is described in the section below.\\n\\nk-mergers\\nA k-merger takes \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   sorted sequences. Upon one invocation of a k-merger, it outputs the first \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3}}\\n   elements of the sorted sequence obtained by merging the k input sequences.\\nAt the top level, funnelsort uses a \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n  -merger on \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n   sequences of length \\n  \\n    \\n      \\n        \\n          N\\n          \\n            2\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{2/3}}\\n  , and invokes this merger once.\\nThe k-merger is built recursively out of \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n  -mergers. It consists of \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   input \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n  -mergers \\n  \\n    \\n      \\n        \\n          I\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          I\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          I\\n          \\n            \\n              k\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle I_{1},I_{2},\\\\ldots ,I_{\\\\sqrt {k}}}\\n  , and a single output \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n  -merger \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n  .\\nThe k inputs are separated into \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   sets of \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   inputs each. Each of these sets is an input to one of the input mergers. The output of each input merger is connected to a buffer, a FIFO queue that can hold \\n  \\n    \\n      \\n        2\\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2k^{3/2}}\\n   elements. The buffers are implemented as circular queues.\\nThe outputs of the \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   buffers are connected to the inputs of the output merger \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n  . Finally, the output of \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n   is the output of the entire k-merger.\\nIn this construction, any input merger only outputs \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   items at once, but the buffer it outputs to has double the space. This is done so that an input merger can be called only when its buffer does not have enough items, but that when it is called, it outputs a lot of items at once (namely, \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   of them).\\nA k-merger works recursively in the following way. To output \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3}}\\n   elements, it recursively invokes its output merger \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   times. However, before it makes a call to \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n  , it checks all of its buffers, filling each of them that are less than half full. To fill the i-th buffer, it recursively invokes the corresponding input merger \\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle I_{i}}\\n   once. If this cannot be done (due to the merger running out of inputs), this step is skipped. Since this call outputs \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   elements, the buffer contains at least \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   elements. At the end of all these operations, the k-merger has output the first \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3}}\\n   of its input elements, in sorted order.\\n\\nAnalysis\\nMost of the analysis of this algorithm revolves around analyzing the space and cache miss complexity of the k-merger.\\nThe first important bound is that a k-merger can be fit in \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          k\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(k^{2})}\\n   space. To see this, we let \\n  \\n    \\n      \\n        S\\n        (\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle S(k)}\\n   denote the space needed for a k-merger. To fit the \\n  \\n    \\n      \\n        \\n          k\\n          \\n            1\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{1/2}}\\n   buffers of size \\n  \\n    \\n      \\n        2\\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2k^{3/2}}\\n   takes \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          k\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(k^{2})}\\n   space. To fit the \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}+1}\\n   smaller buffers takes \\n  \\n    \\n      \\n        (\\n        \\n          \\n            k\\n          \\n        \\n        +\\n        1\\n        )\\n        S\\n        (\\n        \\n          \\n            k\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle ({\\\\sqrt {k}}+1)S({\\\\sqrt {k}})}\\n   space. Thus, the space satisfies the recurrence \\n  \\n    \\n      \\n        S\\n        (\\n        k\\n        )\\n        =\\n        (\\n        \\n          \\n            k\\n          \\n        \\n        +\\n        1\\n        )\\n        S\\n        (\\n        \\n          \\n            k\\n          \\n        \\n        )\\n        +\\n        O\\n        (\\n        \\n          k\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle S(k)=({\\\\sqrt {k}}+1)S({\\\\sqrt {k}})+O(k^{2})}\\n  . This recurrence has solution \\n  \\n    \\n      \\n        S\\n        (\\n        k\\n        )\\n        =\\n        O\\n        (\\n        \\n          k\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle S(k)=O(k^{2})}\\n  .\\nIt follows that there is a positive constant \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   such that a problem of size at most \\n  \\n    \\n      \\n        \\u03b1\\n        \\n          \\n            Z\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha {\\\\sqrt {Z}}}\\n   fits entirely in cache, meaning that it incurs no additional cache misses.\\nLetting \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle Q_{M}(k)}\\n   denote the number of cache misses incurred by a call to a k-merger, one can show that \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        k\\n        )\\n        =\\n        O\\n        (\\n        (\\n        \\n          k\\n          \\n            3\\n          \\n        \\n        \\n          log\\n          \\n            Z\\n          \\n        \\n        \\u2061\\n        k\\n        )\\n        \\n          /\\n        \\n        L\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle Q_{M}(k)=O((k^{3}\\\\log _{Z}k)/L).}\\n   This is done by an induction argument. It has \\n  \\n    \\n      \\n        k\\n        \\u2264\\n        \\u03b1\\n        \\n          \\n            Z\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k\\\\leq \\\\alpha {\\\\sqrt {Z}}}\\n   as a base case. For larger k, we can bound the number of times a \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n  -merger is called. The output merger is called exactly \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   times. The total number of calls on input mergers is at most \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        2\\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}+2{\\\\sqrt {k}}}\\n  . This gives a total bound of \\n  \\n    \\n      \\n        2\\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        2\\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2k^{3/2}+2{\\\\sqrt {k}}}\\n   recursive calls. In addition, the algorithm checks every buffer to see if needs to be filled. This is done on \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   buffers every step for \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   steps, leading to a max of \\n  \\n    \\n      \\n        \\n          k\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{2}}\\n   cache misses for all the checks.\\nThis leads to the recurrence \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        k\\n        )\\n        \\u2264\\n        (\\n        2\\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        2\\n        \\n          \\n            k\\n          \\n        \\n        )\\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        \\n          \\n            k\\n          \\n        \\n        )\\n        +\\n        \\n          k\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{M}(k)\\\\leq (2k^{3/2}+2{\\\\sqrt {k}})Q_{M}({\\\\sqrt {k}})+k^{2}}\\n  , which can be shown to have the solution given above.\\nFinally, the total cache misses \\n  \\n    \\n      \\n        Q\\n        (\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle Q(N)}\\n   for the entire sort can be analyzed. It satisfies the recurrence \\n  \\n    \\n      \\n        Q\\n        (\\n        N\\n        )\\n        =\\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        Q\\n        (\\n        \\n          N\\n          \\n            2\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        )\\n        +\\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle Q(N)=N^{1/3}Q(N^{2/3})+Q_{M}(N^{1/3}).}\\n   This can be shown to have solution \\n  \\n    \\n      \\n        Q\\n        (\\n        N\\n        )\\n        =\\n        O\\n        (\\n        (\\n        N\\n        \\n          /\\n        \\n        L\\n        )\\n        \\n          log\\n          \\n            Z\\n          \\n        \\n        \\u2061\\n        N\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle Q(N)=O((N/L)\\\\log _{Z}N).}\\n\\nLazy funnelsort\\nLazy funnelsort is a modification of the funnelsort, introduced by Gerth St\\u00f8lting Brodal and Rolf Fagerberg in 2002.\\nThe modification is that when a merger is invoked, it does not have to fill each of its buffers. Instead, it lazily fills a buffer only when it is empty. This modification has the same asymptotic runtime and memory transfers as the original funnelsort, but has applications in cache-oblivious algorithms for problems in computational geometry in a method known as distribution sweeping.\\n\\nSee also\\nCache-oblivious algorithm\\nCache-oblivious distribution sort\\nExternal sorting\\n\\n\\n== References ==\"}, {\"Gnome sort\": \"Gnome sort (nicknamed stupid sort) is a variation of the insertion sort sorting algorithm that does not use nested loops. Gnome sort was originally proposed by Iranian computer scientist Hamid Sarbazi-Azad (professor of Computer Science and Engineering at Sharif University of Technology) in 2000. The sort was first called stupid sort (not to be confused with bogosort), and then later described by Dick Grune and named gnome sort.Gnome sort performs at least as many comparisons as insertion sort and has the same asymptotic run time characteristics. Gnome sort works by building a sorted list one element at a time, getting each item to the proper place in a series of swaps. The average running time is O(n2) but tends towards O(n) if the list is initially almost sorted.Dick Grune described the sorting method with the following story:\\n\\nGnome Sort is based on the technique used by the standard Dutch Garden Gnome (Du.: tuinkabouter). \\nHere is how a garden gnome sorts a line of flower pots. \\nBasically, he looks at the flower pot next to him and the previous one; if they are in the right order he steps one pot forward, otherwise, he swaps them and steps one pot backward. \\nBoundary conditions: if there is no previous pot, he steps forwards; if there is no pot next to him, he is done.\\n\\nPseudocode\\nHere is pseudocode for the gnome sort using a zero-based array:\\n\\nExample\\nGiven an unsorted array, a = [5, 3, 2, 4], the gnome sort takes the following steps during the while loop. The current position is highlighted in bold and indicated as a value of the variable pos.\\n\\nNotes\\nReferences\\nExternal links\\n\\nGnome sort\"}, {\"Heapsort\": \"In computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like selection sort, heapsort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step.Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime (and as such is used by Introsort as a fallback should it detect that quicksort is becoming degenerate). Heapsort is an in-place algorithm, but it is not a stable sort.\\nHeapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, Robert W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.\\n\\nOverview\\nThe heapsort algorithm can be divided into two parts.\\nIn the first step, a heap is built out of the data (see Binary heap \\u00a7 Building a heap). The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. For a zero-based array, the root node is stored at index 0; if i is the index of the current node, then\\n\\n  iParent(i)     = floor((i-1) / 2) where floor functions map a real number to the largest leading integer.\\n  iLeftChild(i)  = 2*i + 1\\n  iRightChild(i) = 2*i + 2\\n\\nIn the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.\\nHeapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed here. The heap's invariant is preserved after each extraction, so the only cost is that of extraction.\\n\\nAlgorithm\\nThe heapsort algorithm involves preparing the list by first turning it into a max heap. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.\\nThe steps are:\\n\\nCall the buildMaxHeap() function on the list. Also referred to as heapify(), this builds a heap from a list in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   operations.\\nSwap the first element of the list with the final element. Decrease the considered range of the list by one.\\nCall the siftDown() function on the list to sift the new first element to its appropriate index in the heap.\\nGo to step (2) unless the considered range of the list is one element.The buildMaxHeap() operation is run once, and is O(n) in performance. The siftDown() function is O(log n), and is called n times. Therefore, the performance of this algorithm is O(n + n log n) = O(n log n).\\n\\nPseudocode\\nThe following is a simple way to implement the algorithm in pseudocode. Arrays are zero-based and swap is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at a[0], while at the end of the sort, the largest element is in a[end].\\n\\nprocedure heapsort(a, count) is\\n    input: an unordered array a of length count\\n \\n    (Build the heap in array a so that largest value is at the root)\\n    heapify(a, count)\\n\\n    (The following loop maintains the invariants that a[0:end] is a heap and every element\\n     beyond end is greater than everything before it (so a[end:count] is in sorted order))\\n    end \\u2190 count - 1\\n    while end > 0 do\\n        (a[0] is the root and largest value. The swap moves it in front of the sorted elements.)\\n        swap(a[end], a[0])\\n        (the heap size is reduced by one)\\n        end \\u2190 end - 1\\n        (the swap ruined the heap property, so restore it)\\n        siftDown(a, 0, end)\\n\\nThe sorting routine uses two subroutines, heapify and siftDown. The former is the common in-place heap construction routine, while the latter is a common subroutine for implementing heapify.\\n\\n(Put elements of 'a' in heap order, in-place)\\nprocedure heapify(a, count) is\\n    (start is assigned the index in 'a' of the last parent node)\\n    (the last element in a 0-based array is at index count-1; find the parent of that element)\\n    start \\u2190 iParent(count-1)\\n    \\n    while start \\u2265 0 do\\n        (sift down the node at index 'start' to the proper place such that all nodes below\\n         the start index are in heap order)\\n        siftDown(a, start, count - 1)\\n        (go to the next parent node)\\n        start \\u2190 start - 1\\n    (after sifting down the root all nodes/elements are in heap order)\\n\\n(Repair the heap whose root element is at index 'start', assuming the heaps rooted at its children are valid)\\nprocedure siftDown(a, start, end) is\\n    root \\u2190 start\\n\\n    while iLeftChild(root) \\u2264 end do    (While the root has at least one child)\\n        child \\u2190 iLeftChild(root)   (Left child of root)\\n        swap \\u2190 root                (Keeps track of child to swap with)\\n\\n        if a[swap] < a[child] then\\n            swap \\u2190 child\\n        (If there is a right child and that child is greater)\\n        if child+1 \\u2264 end and a[swap] < a[child+1] then\\n            swap \\u2190 child + 1\\n        if swap = root then\\n            (The root holds the largest element. Since we assume the heaps rooted at the\\n             children are valid, this means that we are done.)\\n            return\\n        else\\n            Swap(a[root], a[swap])\\n            root \\u2190 swap          (repeat to continue sifting down the child now)\\n\\nThe heapify procedure can be thought of as building a heap from the bottom up by successively sifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand. This siftUp version can be visualized as starting with an empty heap and successively inserting elements, whereas the siftDown version given above treats the entire input array as a full but \\\"broken\\\" heap and \\\"repairs\\\" it starting from the last non-trivial sub-heap (that is, the last parent node).\\n\\nAlso, the siftDown version of heapify has O(n) time complexity, while the siftUp version given below has O(n log n) time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.\\nThis may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never affects asymptotic analysis.\\nTo grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call increases with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more \\\"deep\\\" nodes than there are \\\"shallow\\\" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the \\\"bottom\\\" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call decreases as the depth of the node on which the call is made increases. Thus, when the siftDown heapify begins and is calling siftDown on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the \\\"height\\\" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.\\nThe heapsort algorithm itself has O(n log n) time complexity using either version of heapify.\\n\\n procedure heapify(a,count) is\\n     (end is assigned the index of the first (left) child of the root)\\n     end := 1\\n     \\n     while end < count\\n         (sift up the node at index end to the proper place such that all nodes above\\n          the end index are in heap order)\\n         siftUp(a, 0, end)\\n         end := end + 1\\n     (after sifting up the last node all nodes are in heap order)\\n \\n procedure siftUp(a, start, end) is\\n     input:  start represents the limit of how far up the heap to sift.\\n                   end is the node to sift up.\\n     child := end \\n     while child > start\\n         parent := iParent(child)\\n         if a[parent] < a[child] then (out of max-heap order)\\n             swap(a[parent], a[child])\\n             child := parent (repeat to continue sifting up the parent now)\\n         else\\n             return\\n\\nNote that unlike siftDown approach where, after each swap, you need to call only the siftDown subroutine to fix the broken heap; the siftUp subroutine alone cannot fix the broken heap. The heap needs to be built every time after a swap by calling the heapify procedure since \\\"siftUp\\\" assumes that the element getting swapped ends up in its final place, as opposed to \\\"siftDown\\\" allows for continuous adjustments of items lower in the heap until the invariant is satisfied. The adjusted pseudocode for using siftUp approach is given below.\\n\\n procedure heapsort(a, count) is\\n    input: an unordered array a of length count\\n \\n    (Build the heap in array a so that largest value is at the root)\\n    heapify(a, count)\\n\\n    (The following loop maintains the invariants that a[0:end] is a heap and every element\\n     beyond end is greater than everything before it (so a[end:count] is in sorted order))\\n    end \\u2190 count - 1\\n    while end > 0 do\\n        (a[0] is the root and largest value. The swap moves it in front of the sorted elements.)\\n        swap(a[end], a[0])\\n        (rebuild the heap using siftUp after the swap ruins the heap property)\\n        heapify(a, end)\\n        (reduce the heap size by one)\\n        end \\u2190 end - 1\\n\\nVariations\\nFloyd's heap construction\\nThe most important variation to the basic algorithm, which is included in all practical implementations, is a heap-construction algorithm by Floyd which runs in O(n) time and uses siftdown rather than siftup, avoiding the need to implement siftup at all.\\nRather than starting with a trivial heap and repeatedly adding leaves, Floyd's algorithm starts with the leaves, observing that they are trivial but valid heaps by themselves, and then adds parents. Starting with element n/2 and working backwards, each internal node is made the root of a valid heap by sifting down. The last step is sifting down the first element, after which the entire array obeys the heap property.\\nThe worst-case number of comparisons during the Floyd's heap-construction phase of heapsort is known to be equal to 2n \\u2212 2s2(n) \\u2212 e2(n), where s2(n) is the number of 1 bits in the binary representation of n and e2(n) is number of trailing 0 bits.The standard implementation of Floyd's heap-construction algorithm causes a large number of cache misses once the size of the data exceeds that of the CPU cache.:\\u200a87\\u200a Much better performance on large data sets can be obtained by merging in depth-first order, combining subheaps as soon as possible, rather than combining all subheaps on one level before proceeding to the one above.\\n\\nBottom-up heapsort\\nBottom-up heapsort is a variant which reduces the number of comparisons required by a significant factor. While ordinary heapsort requires 2n log2 n + O(n) comparisons worst-case and on average, the bottom-up variant requires n log2n + O(1) comparisons on average, and 1.5n log2n + O(n) in the worst case.If comparisons are cheap (e.g. integer keys) then the difference is unimportant, as top-down heapsort compares values that have already been loaded from memory. If, however, comparisons require a function call or other complex logic, then bottom-up heapsort is advantageous.\\nThis is accomplished by improving the siftDown procedure. The change improves the linear-time heap-building phase somewhat, but is more significant in the second phase. Like ordinary heapsort, each iteration of the second phase extracts the top of the heap, a[0], and fills the gap it leaves with a[end], then sifts this latter element down the heap. But this element comes from the lowest level of the heap, meaning it is one of the smallest elements in the heap, so the sift-down will likely take many steps to move it back down. In ordinary heapsort, each step of the sift-down requires two comparisons, to find the minimum of three elements: the new node and its two children.\\nBottom-up heapsort instead finds the path of largest children to the leaf level of the tree (as if it were inserting \\u2212\\u221e) using only one comparison per level. Put another way, it finds a leaf which has the property that it and all of its ancestors are greater than or equal to their siblings. (In the absence of equal keys, this leaf is unique.) Then, from this leaf, it searches upward (using one comparison per level) for the correct position in that path to insert a[end]. This is the same location as ordinary heapsort finds, and requires the same number of exchanges to perform the insert, but fewer comparisons are required to find that location.Because it goes all the way to the bottom and then comes back up, it is called heapsort with bounce by some authors.\\nfunction leafSearch(a, i, end) is\\n    j \\u2190 i\\n    while iRightChild(j) \\u2264 end do\\n        (Determine which of j's two children is the greater)\\n        if a[iRightChild(j)] > a[iLeftChild(j)] then\\n            j \\u2190 iRightChild(j)\\n        else\\n            j \\u2190 iLeftChild(j)\\n    (At the last level, there might be only one child)\\n    if iLeftChild(j) \\u2264 end then\\n        j \\u2190 iLeftChild(j)\\n    return j\\n\\nThe return value of the leafSearch is used in the modified siftDown routine:\\nprocedure siftDown(a, i, end) is\\n    j \\u2190 leafSearch(a, i, end)\\n    while a[i] > a[j] do\\n        j \\u2190 iParent(j)\\n    x \\u2190 a[j]\\n    a[j] \\u2190 a[i]\\n    while j > i do\\n        swap x, a[iParent(j)]\\n        j \\u2190 iParent(j)\\n\\nBottom-up heapsort was announced as beating quicksort (with median-of-three pivot selection) on arrays of size \\u226516000.A 2008 re-evaluation of this algorithm showed it to be no faster than ordinary heapsort for integer keys, presumably because modern branch prediction nullifies the cost of the predictable comparisons which bottom-up heapsort manages to avoid.A further refinement does a binary search in the path to the selected leaf, and sorts in a worst case of (n+1)(log2(n+1) + log2 log2(n+1) + 1.82) + O(log2n) comparisons, approaching the information-theoretic lower bound of n log2n \\u2212 1.4427n comparisons.A variant which uses two extra bits per internal node (n\\u22121 bits total for an n-element heap) to cache information about which child is greater (two bits are required to store three cases: left, right, and unknown) uses less than n log2n + 1.1n compares.\\n\\nOther variations\\nTernary heapsort uses a ternary heap instead of a binary heap; that is, each element in the heap has three children. It is more complicated to program, but does a constant number of times fewer swap and comparison operations. This is because each sift-down step in a ternary heap requires three comparisons and one swap, whereas in a binary heap two comparisons and one swap are required. Two levels in a ternary heap cover 32 = 9 elements, doing more work with the same number of comparisons as three levels in the binary heap, which only cover 23 = 8. This is primarily of academic interest, or as a student exercise, as the additional complexity is not worth the minor savings, and bottom-up heapsort beats both.\\nMemory-optimized heapsort:\\u200a87\\u200a improves heapsort's locality of reference by increasing the number of children even more. This increases the number of comparisons, but because all children are stored consecutively in memory, reduces the number of cache lines accessed during heap traversal, a net performance improvement.\\nOut-of-place heapsort improves on bottom-up heapsort by eliminating the worst case, guaranteeing n log2n + O(n) comparisons. When the maximum is taken, rather than fill the vacated space with an unsorted data value, fill it with a \\u2212\\u221e sentinel value, which never \\\"bounces\\\" back up. It turns out that this can be used as a primitive in an in-place (and non-recursive) \\\"QuickHeapsort\\\" algorithm. First, you perform a quicksort-like partitioning pass, but reversing the order of the partitioned data in the array. Suppose (without loss of generality) that the smaller partition is the one greater than the pivot, which should go at the end of the array, but our reversed partitioning step places it at the beginning. Form a heap out of the smaller partition and do out-of-place heapsort on it, exchanging the extracted maxima with values from the end of the array. These are less than the pivot, meaning less than any value in the heap, so serve as \\u2212\\u221e sentinel values. Once the heapsort is complete (and the pivot moved to just before the now-sorted end of the array), the order of the partitions has been reversed, and the larger partition at the beginning of the array may be sorted in the same way. (Because there is no non-tail recursion, this also eliminates quicksort's O(log n) stack usage.)\\nThe smoothsort algorithm is a variation of heapsort developed by Edsger W. Dijkstra in 1981. Like heapsort, smoothsort's upper bound is O(n log n). The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n log n) regardless of the initial sorted state. Due to its complexity, smoothsort is rarely used.\\nLevcopoulos and Petersson describe a variation of heapsort based on a heap of Cartesian trees. First, a Cartesian tree is built from the input in O(n) time, and its root is placed in a 1-element binary heap. Then we repeatedly extract the minimum from the binary heap, output the tree's root element, and add its left and right children (if any) which are themselves Cartesian trees, to the binary heap. As they show, if the input is already nearly sorted, the Cartesian trees will be very unbalanced, with few nodes having left and right children, resulting in the binary heap remaining small, and allowing the algorithm to sort more quickly than O(n log n) for inputs that are already nearly sorted.\\nSeveral variants such as weak heapsort require n log2 n+O(1) comparisons in the worst case, close to the theoretical minimum, using one extra bit of state per node. While this extra bit makes the algorithms not truly in-place, if space for it can be found inside the element, these algorithms are simple and efficient,:\\u200a40\\u200a but still slower than binary heaps if key comparisons are cheap enough (e.g. integer keys) that a constant factor does not matter.\\nKatajainen's \\\"ultimate heapsort\\\" requires no extra storage, performs n log2 n+O(1) comparisons, and a similar number of element moves. It is, however, even more complex and not justified unless comparisons are very expensive.\\n\\nComparison with other sorts\\nHeapsort primarily competes with quicksort, another very efficient general purpose in-place comparison-based sort algorithm.\\nHeapsort's primary advantages are its simple, non-recursive code, minimal auxiliary storage requirement, and reliably good performance: its best and worst cases are within a small constant factor of each other, and of the theoretical lower bound on comparison sorts. While it cannot do better than O(n log n) for pre-sorted inputs, it does not suffer from quicksort's O(n2) worst case, either. (The latter can be avoided with careful implementation, but that makes quicksort far more complex, and one of the most popular solutions, introsort, uses heapsort for the purpose.)\\nIts primary disadvantages are its poor locality of reference and its inherently serial nature; the accesses to the implicit tree are widely scattered and mostly random, and there is no straightforward way to convert it to a parallel algorithm.\\nThis makes it popular in embedded systems, real-time computing, and systems concerned with maliciously chosen inputs, such as the Linux kernel. It is also a good choice for any application which does not expect to be bottlenecked on sorting.\\nA well-implemented quicksort is usually 2\\u20133 times faster than heapsort. Although quicksort requires fewer comparisons, this is a minor factor. (Results claiming twice as many comparisons are measuring the top-down version; see \\u00a7 Bottom-up heapsort.) The main advantage of quicksort is its much better locality of reference: partitioning is a linear scan with good spatial locality, and the recursive subdivision has good temporal locality. With additional effort, quicksort can also be implemented in mostly branch-free code, and multiple CPUs can be used to sort subpartitions in parallel. Thus, quicksort is preferred when the additional performance justifies the implementation effort.\\nThe other major O(n log n) sorting algorithm is merge sort, but that rarely competes directly with heapsort because it is not in-place. Merge sort's requirement for \\u03a9(n) extra space (roughly half the size of the input) is usually prohibitive except in the situations where merge sort has a clear advantage:\\n\\nWhen a stable sort is required\\nWhen taking advantage of (partially) pre-sorted input\\nSorting linked lists (in which case merge sort requires minimal extra space)\\nParallel sorting; merge sort parallelizes even better than quicksort and can easily achieve close to linear speedup\\nExternal sorting; merge sort has excellent locality of reference\\n\\nExample\\nLet { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)\\n\\nBuild the heap\\nSorting\\nNotes\\nReferences\\nWilliams, J. W. J. (1964). \\\"Algorithm 232 \\u2013 Heapsort\\\". Communications of the ACM. 7 (6): 347\\u2013348. doi:10.1145/512274.512284.\\nFloyd, Robert W. (1964). \\\"Algorithm 245 \\u2013 Treesort 3\\\". Communications of the ACM. 7 (12): 701. doi:10.1145/355588.365103. S2CID 52864987.\\nCarlsson, Svante [in Swedish] (1987). \\\"Average-case results on heapsort\\\". BIT Numerical Mathematics. 27 (1): 2\\u201317. doi:10.1007/bf01937350. S2CID 31450060.\\nKnuth, Donald (1997). \\\"\\u00a75.2.3, Sorting by Selection\\\". The Art of Computer Programming. Vol. 3: Sorting and Searching (3rd ed.). Addison-Wesley. pp. 144\\u2013155. ISBN 978-0-201-89685-5.\\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. ISBN 0-262-03293-7. Chapters 6 and 7 Respectively: Heapsort and Priority Queues\\nA PDF of Dijkstra's original paper on Smoothsort\\nHeaps and Heapsort Tutorial by David Carlson, St. Vincent College\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Heap Sort at the Wayback Machine (archived 6 March 2015) \\u2013 graphical demonstration\\nCourseware on Heapsort from Univ. Oldenburg \\u2013 With text, animations and interactive exercises\\nNIST's Dictionary of Algorithms and Data Structures: Heapsort\\nHeapsort implemented in 12 languages\\nSorting revisited by Paul Hsieh\\nA PowerPoint presentation demonstrating how Heap sort works that is for educators.\\nOpen Data Structures \\u2013 Section 11.1.3 \\u2013 Heap-Sort, Pat Morin\"}, {\"Insertion sort\": \"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time by comparisons. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:\\n\\nSimple implementation: Jon Bentley shows a three-line C/C++ version that is five lines when optimized.\\nEfficient for (quite) small data sets, much like other quadratic (i.e., O(n2)) sorting algorithms\\nMore efficient in practice than most other simple quadratic algorithms such as selection sort or bubble sort\\nAdaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(kn) when each element in the input is no more than k places away from its sorted position\\nStable; i.e., does not change the relative order of elements with equal keys\\nIn-place; i.e., only requires a constant amount O(1) of additional memory space\\nOnline; i.e., can sort a list as it receives itWhen people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.\\n\\nAlgorithm\\nInsertion sort iterates, consuming one input element each repetition, and grows a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.\\nSorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.\\nThe resulting array after k iterations has the property where the first k + 1 entries are sorted (\\\"+1\\\" because the first entry is skipped). In each iteration the first remaining entry of the input is removed, and inserted into the result at the correct position, thus extending the result:\\n\\nbecomes\\n\\nwith each element greater than x copied to the right as it is compared against x.\\nThe most common variant of insertion sort, which operates on arrays, can be described as follows:\\n\\nSuppose there exists a function called Insert designed to insert a value into a sorted sequence at the beginning of an array. It operates by beginning at the end of the sequence and shifting each element one place to the right until a suitable position is found for the new element. The function has the side effect of overwriting the value stored immediately after the sorted sequence in the array.\\nTo perform an insertion sort, begin at the left-most element of the array and invoke Insert to insert each element encountered into its correct position. The ordered sequence into which the element is inserted is stored at the beginning of the array in the set of indices already examined. Each insertion overwrites a single value: the value being inserted.Pseudocode of the complete algorithm follows, where the arrays are zero-based:\\ni \\u2190 1\\nwhile i < length(A)\\n    j \\u2190 i\\n    while j > 0 and A[j-1] > A[j]\\n        swap A[j] and A[j-1]\\n        j \\u2190 j - 1\\n    end while\\n    i \\u2190 i + 1\\nend while\\n\\nThe outer loop runs over all the elements except the first one, because the single-element prefix A[0:1] is trivially sorted, so the invariant that the first i entries are sorted is true from the start. The inner loop moves element A[i] to its correct place so that after the loop, the first i+1 elements are sorted. Note that the and-operator in the test must use short-circuit evaluation, otherwise the test might result in an array bounds error, when j=0 and it tries to evaluate A[j-1] > A[j] (i.e. accessing A[-1] fails).\\nAfter expanding the swap operation in-place as x \\u2190 A[j]; A[j] \\u2190 A[j-1]; A[j-1] \\u2190 x (where x is a temporary variable), a slightly faster version can be produced that moves A[i] to its position in one go and only performs one assignment in the inner loop body:\\ni \\u2190 1\\nwhile i < length(A)\\n    x \\u2190 A[i]\\n    j \\u2190 i - 1\\n    while j >= 0 and A[j] > x\\n        A[j+1] \\u2190 A[j]\\n        j \\u2190 j - 1\\n    end while\\n    A[j+1] \\u2190 x\\n    i \\u2190 i + 1\\nend while\\n\\nThe new inner loop shifts elements to the right to clear a spot for x = A[i].\\nThe algorithm can also be implemented in a recursive way. The recursion just replaces the outer loop, calling itself and storing successively smaller values of n on the stack until n equals 0, where the function then returns up the call chain to execute the code after each recursive call starting with n equal to 1, with n increasing by 1 as each instance of the function returns to the prior instance. The initial call would be insertionSortR(A, length(A)-1).\\n\\nfunction insertionSortR(array A, int n)\\n    if n > 0\\n        insertionSortR(A, n-1)\\n        x \\u2190 A[n]\\n        j \\u2190 n-1\\n        while j >= 0 and A[j] > x\\n            A[j+1] \\u2190 A[j]\\n            j \\u2190 j-1\\n        end while\\n        A[j+1] \\u2190 x\\n    end if\\nend function\\n\\nIt does not make the code any shorter, it also doesn't reduce the execution time, but it increases the additional memory consumption from O(1) to O(N) (at the deepest level of recursion the stack contains N references to the A array, each with accompanying value of variable n from N down to 1).\\n\\nBest, worst, and average cases\\nThe best case input is an array that is already sorted. In this case insertion sort has a linear running time (i.e., O(n)). During each iteration, the first remaining element of the input is only compared with the right-most element of the sorted subsection of the array.\\nThe simplest worst case input is an array sorted in reverse order. The set of all worst case inputs consists of all arrays where each element is the smallest or second-smallest of the elements before it. In these cases every iteration of the inner loop will scan and shift the entire sorted subsection of the array before inserting the next element. This gives insertion sort a quadratic running time (i.e., O(n2)).\\nThe average case is also quadratic, which makes insertion sort impractical for sorting large arrays. However, insertion sort is one of the fastest algorithms for sorting very small arrays, even faster than quicksort; indeed, good quicksort implementations use insertion sort for arrays smaller than a certain threshold, also when arising as subproblems; the exact threshold must be determined experimentally and depends on the machine, but is commonly around ten.\\nExample: The following table shows the steps for sorting the sequence {3, 7, 4, 9, 5, 2, 6, 1}.  In each step, the key under consideration is underlined. The key that was moved (or left in place because it was the biggest yet considered) in the previous step is marked with an asterisk.\\n\\n3  7  4  9  5  2  6  1\\n3* 7  4  9  5  2  6  1\\n3  7* 4  9  5  2  6  1\\n3  4* 7  9  5  2  6  1\\n3  4  7  9* 5  2  6  1\\n3  4  5* 7  9  2  6  1\\n2* 3  4  5  7  9  6  1\\n2  3  4  5  6* 7  9  1\\n1* 2  3  4  5  6  7  9\\n\\nRelation to other sorting algorithms\\nInsertion sort is very similar to selection sort. As in selection sort, after k passes through the array, the first k elements are in sorted order. However, the fundamental difference between the two algorithms is that insertion sort scans backwards from the current key, while selection sort scans forwards.  This results in selection sort making the first k elements the k smallest elements of the unsorted input, while in insertion sort they are simply the first k elements of the input.\\nThe primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the (k + 1)-st element is greater than the k-th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort. On average (assuming the rank of the (k + 1)-st element rank is random), insertion sort will require comparing and shifting half of the previous k elements, meaning that insertion sort will perform about half as many comparisons as selection sort on average.\\nIn the worst case for insertion sort (when the input array is reverse-sorted), insertion sort performs just as many comparisons as selection sort. However, a disadvantage of insertion sort over selection sort is that it requires more writes due to the fact that, on each iteration, inserting the (k + 1)-st element into the sorted portion of the array requires many element swaps to shift all of the following elements, while only a single swap is required for each iteration of selection sort. In general, insertion sort will write to the array O(n2) times, whereas selection sort will write only O(n) times. For this reason selection sort may be preferable in cases where writing to memory is significantly more expensive than reading, such as with EEPROM or flash memory.\\nWhile some divide-and-conquer algorithms such as quicksort and mergesort outperform insertion sort for larger arrays, non-recursive sorting algorithms such as insertion sort or selection sort are generally faster for very small arrays (the exact size varies by environment and implementation, but is typically between 7 and 50 elements). Therefore, a useful optimization in the implementation of those algorithms is a hybrid approach, using the simpler algorithm when the array has been divided to a small size.\\n\\nVariants\\nD.L. Shell made substantial improvements to the algorithm; the modified version is called Shell sort.  The sorting algorithm compares elements separated by a distance that decreases on each pass. Shell sort has distinctly improved running times in practical work, with two simple variants requiring O(n3/2) and O(n4/3) running time.If the cost of comparisons exceeds the cost of swaps, as is the case for example with string keys stored by reference or with human interaction (such as choosing one of a pair displayed side-by-side), then using binary insertion sort may yield better performance. Binary insertion sort employs a binary search to determine the correct location to insert new elements, and therefore performs \\u2308log2 n\\u2309 comparisons in the worst case. When each element in the array is searched for and inserted this is O(n log n). The algorithm as a whole still has a running time of O(n2) on average because of the series of swaps required for each insertion.The number of swaps can be reduced by calculating the position of multiple elements before moving them. For example, if the target position of two elements is calculated before they are moved into the proper position, the number of swaps can be reduced by about 25% for random data. In the extreme case, this variant works similar to merge sort.\\nA variant named binary merge sort uses a binary insertion sort to sort groups of 32 elements, followed by a final sort using merge sort. It combines the speed of insertion sort on small data sets with the speed of merge sort on large data sets.To avoid having to make a series of swaps for each insertion, the input could be stored in a linked list, which allows elements to be spliced into or out of the list in constant time when the position in the list is known.  However, searching a linked list requires sequentially following the links to the desired position: a linked list does not have random access, so it cannot use a faster method such as binary search.  Therefore, the running time required for searching is O(n), and the time for sorting is O(n2). If a more sophisticated data structure (e.g., heap or binary tree) is used, the time required for searching and insertion can be reduced significantly; this is the essence of heap sort and binary tree sort.\\nIn 2006 Bender, Martin Farach-Colton, and Mosteiro published a new variant of insertion sort called library sort or gapped insertion sort that leaves a small number of unused spaces (i.e., \\\"gaps\\\") spread throughout the array. The benefit is that insertions need only shift elements over until a gap is reached. The authors show that this sorting algorithm runs with high probability in O(n log n) time.If a skip list is used, the insertion time is brought down to O(log n), and swaps are not needed because the skip list is implemented on a linked list structure.  The final running time for insertion would be O(n log n).\\n\\nList insertion sort code in C\\nIf the items are stored in a linked list, then the list can be sorted with O(1) additional space.  The algorithm starts with an initially empty (and therefore trivially sorted) list. The input items are taken off the list one at a time, and then inserted in the proper place in the sorted list. When the input list is empty, the sorted list has the desired result.\\n\\nThe algorithm below uses a trailing pointer for the insertion into the sorted list. A simpler recursive method rebuilds the list each time (rather than splicing) and can use O(n) stack space.\\n\\nReferences\\nFurther reading\\nKnuth, Donald (1998), \\\"5.2.1: Sorting by Insertion\\\", The Art of Computer Programming, vol. 3. Sorting and Searching (second ed.), Addison-Wesley, pp. 80\\u2013105, ISBN 0-201-89685-0.\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Insertion Sort at the Wayback Machine (archived 8 March 2015) \\u2013 graphical demonstration\\nAdamovsky, John Paul, Binary Insertion Sort \\u2013 Scoreboard \\u2013 Complete Investigation and C Implementation, Pathcom.\\nInsertion Sort \\u2013 a comparison with other O(n2) sorting algorithms, UK: Core war.\\nInsertion sort (C) (wiki), LiteratePrograms \\u2013 implementations of insertion sort in C and several other programming languages\"}, {\"Integer sorting\": \"In computer science, integer sorting is the algorithmic problem of sorting a collection of data values by integer keys. Algorithms designed for integer sorting may also often be applied to sorting problems in which the keys are floating point numbers, rational numbers, or text strings. The ability to perform integer arithmetic on the keys allows integer sorting algorithms to be faster than comparison sorting algorithms in many cases, depending on the details of which operations are allowed in the model of computing and how large the integers to be sorted are.\\nInteger sorting algorithms including pigeonhole sort, counting sort, and radix sort are widely used and practical. Other integer sorting algorithms with smaller worst-case time bounds are not believed to be practical for computer architectures with 64 or fewer bits per word. Many such algorithms are known, with performance depending on a combination of the number of items to be sorted, number of bits per key, and number of bits per word of the computer performing the sorting algorithm.\\n\\nGeneral considerations\\nModels of computation\\nTime bounds for integer sorting algorithms typically depend on three parameters: the number n of data values to be sorted, the magnitude K of the largest possible key to be sorted, and the number w of bits that can be represented in a single machine word of the computer on which the algorithm is to be performed. Typically, it is assumed that w \\u2265 log2(max(n, K)); that is, that machine words are large enough to represent an index into the sequence of input data, and also large enough to represent a single key.Integer sorting algorithms are usually designed to work in either the pointer machine or random access machine models of computing. The main difference between these two models is in how memory may be addressed. The random access machine allows any value that is stored in a register to be used as the address of memory read and write operations, with unit cost per operation. This ability allows certain complex operations on data to be implemented quickly using table lookups. In contrast, in the pointer machine model, read and write operations use addresses stored in pointers, and it is not allowed to perform arithmetic operations on these pointers. In both models, data values may be added, and bitwise Boolean operations and binary shift operations may typically also be performed on them, in unit time per operation. Different integer sorting algorithms make different assumptions, however, about whether integer multiplication is also allowed as a unit-time operation. Other more specialized models of computation such as the parallel random access machine have also been considered.Andersson, Miltersen & Thorup (1999) showed that in some cases the multiplications or table lookups required by some integer sorting algorithms could be replaced by customized operations that would be more easily implemented in hardware but that are not typically available on general-purpose computers. Thorup (2003) improved on this by showing how to replace these special operations by the bit field manipulation instructions already available on Pentium processors.\\nIn external memory models of computing, no known integer sorting algorithm is faster than comparison sorting. Researchers have shown that, in these models, restricted classes of algorithms that are limited in how they manipulate their keys cannot be faster than comparison sorting, and that an integer sorting algorithm that is faster than comparison sorting would imply the falsity of a standard conjecture in network coding.\\n\\nSorting versus integer priority queues\\nA priority queue is a data structure for maintaining a collection of items with numerical priorities, having operations for finding and removing the item with the minimum priority value. Comparison-based priority queues such as the binary heap take logarithmic time per update, but other structures such as the van Emde Boas tree or bucket queue may be faster for inputs whose priorities are small integers. These data structures can be used in the selection sort algorithm, which sorts a collection of elements by repeatedly finding and removing the smallest element from the collection, and returning the elements in the order they were found. A priority queue can be used to maintain the collection of elements in this algorithm, and the time for this algorithm on a collection of n elements can be bounded by the time to initialize the priority queue and then to perform n find and remove operations. For instance, using a binary heap as a priority queue in selection sort leads to the heap sort algorithm, a comparison sorting algorithm that takes O(n log n) time. Instead, using selection sort with a bucket queue gives a form of pigeonhole sort, and using van Emde Boas trees or other integer priority queues leads to other fast integer sorting algorithms.Instead of using an integer priority queue in a sorting algorithm, it is possible to go the other direction, and use integer sorting algorithms as subroutines within an integer priority queue data structure. Thorup (2007) used this idea to show that, if it is possible to perform integer sorting in time T(n) per key, then the same time bound applies to the time per insertion or deletion operation in a priority queue data structure. Thorup's reduction is complicated and assumes the availability of either fast multiplication operations or table lookups, but he also provides an alternative priority queue using only addition and Boolean operations with time T(n) + T(log n) + T(log log n) + ... per operation, at most multiplying the time by an iterated logarithm.\\n\\nUsability\\nThe classical integer sorting algorithms of pigeonhole sort, counting sort, and radix sort are widely used and practical. Much of the subsequent research on integer sorting algorithms has focused less on practicality and more on theoretical improvements in their worst case analysis, and the algorithms that come from this line of research are not believed to be practical for current 64-bit computer architectures, although\\nexperiments have shown that some of these methods may be an improvement on radix sorting for data with 128 or more bits per key. Additionally, for large data sets, the near-random memory access patterns of many integer sorting algorithms can handicap them compared to comparison sorting algorithms that have been designed with the memory hierarchy in mind.Integer sorting provides one of the six benchmarks in the DARPA High Productivity Computing Systems Discrete Mathematics benchmark suite, and one of eleven benchmarks in the NAS Parallel Benchmarks suite.\\n\\nPractical algorithms\\nPigeonhole sort or counting sort can both sort n data items having keys in the range from 0 to K \\u2212 1 in time O(n + K). In pigeonhole sort (often called bucket sort), pointers to the data items are distributed to a table of buckets, represented as collection data types such as linked lists, using the keys as indices into the table. Then, all of the buckets are concatenated together to form the output list. Counting sort uses a table of counters in place of a table of buckets, to determine the number of items with each key. Then, a prefix sum computation is used to determine the range of positions in the sorted output at which the values with each key should be placed. Finally, in a second pass over the input, each item is moved to its key's position in the output array. Both algorithms involve only simple loops over the input data (taking time O(n)) and over the set of possible keys (taking time O(K)), giving their O(n + K) overall time bound.\\nRadix sort is a sorting algorithm that works for larger keys than pigeonhole sort or counting sort by performing multiple passes over the data. Each pass sorts the input using only part of the keys, by using a different sorting algorithm (such as pigeonhole sort or counting sort) that is suited only for small keys. To break the keys into parts, the radix sort algorithm computes the positional notation for each key,\\naccording to some chosen radix; then, the part of the key used for the ith pass of the algorithm is the ith digit in the positional notation for the full key, starting from the least significant digit and progressing to the most significant. For this algorithm to work correctly, the sorting algorithm used in each pass over the data must be stable: items with equal digits should not change positions with each other. For greatest efficiency, the radix should be chosen to be near the number of data items, n. Additionally, using a power of two near n as the radix allows the keys for each pass to be computed quickly using only fast binary shift and mask operations. With these choices,  and with pigeonhole sort or counting sort as the base algorithm, the radix sorting algorithm can sort n data items having keys in the range from 0 to K \\u2212 1 in time O(n logn K).\\n\\nTheoretical algorithms\\nMany integer sorting algorithms have been developed whose theoretical analysis shows them to behave better than comparison sorting, pigeonhole sorting, or radix sorting for large enough\\ncombinations of the parameters defining the number of items to be sorted, range of keys, and machine word size.\\nWhich algorithm has the best performance depends on the values of these parameters.\\nHowever, despite their theoretical advantages, these algorithms are not an improvement for the typical ranges of these parameters that arise in practical sorting problems.\\n\\nAlgorithms for small keys\\nA Van Emde Boas tree may be used as a priority queue to sort a set of n keys, each in the range from 0 to K \\u2212 1, in time O(n log log K). This is a theoretical improvement over radix sorting when K is sufficiently large. However, in order to use a Van Emde Boas tree, one either needs a directly addressable memory of K words, or one needs to simulate it using a hash table, reducing the space to linear but making the algorithm be randomized. Another priority queue with similar performance (including the need for randomization in the form of hash tables) is the Y-fast trie of Willard (1983).\\nA more sophisticated technique with a similar flavor and with better theoretical performance was developed by Kirkpatrick & Reisch (1984). They observed that each pass of radix sort can be interpreted as a range reduction technique that, in linear time, reduces the maximum key size by a factor of n; instead, their technique reduces the key size to the square root of its previous value (halving the number of bits needed to represent a key), again in linear time. As in radix sort, they interpret the keys as two-digit base-b numbers for a base b that is approximately \\u221aK. They then group the items to be sorted into buckets according to their high digits, in linear time, using either a large but uninitialized direct addressed memory or a hash table. Each bucket has a representative, the item in the bucket with the largest key; they then sort the list of items using as keys the high digits for the representatives and the low digits for the non-representatives. By grouping the items from this list into buckets again, each bucket may be placed into sorted order, and by extracting the representatives from the sorted list the buckets may be concatenated together into sorted order. Thus, in linear time, the sorting problem is reduced to another recursive sorting problem in which the keys are much smaller, the square root of their previous magnitude. Repeating this range reduction until the keys are small enough to bucket sort leads to an algorithm with running time O(n log logn K).\\nA complicated randomized algorithm of Han & Thorup (2002) in the word RAM model of computation allows these time bounds to be reduced even farther, to O(n\\u221alog log K).\\n\\nAlgorithms for large words\\nAn integer sorting algorithm is said to be non-conservative if it requires a word size w that is significantly larger than log max(n, K). As an extreme instance, if w \\u2265 K, and all keys are distinct, then the set of keys may be sorted in linear time by representing it as a bitvector, with a 1 bit in position i when i is one of the input keys, and then repeatedly removing the least significant bit.The non-conservative packed sorting algorithm of Albers & Hagerup (1997) uses a subroutine, based on Ken Batcher's bitonic sorting network, for merging two sorted sequences of keys that are each short enough to be packed into a single machine word. The input to the packed sorting algorithm, a sequence of items stored one per word, is transformed into a packed form, a sequence of words each holding multiple items in sorted order, by using this subroutine repeatedly to double the number of items packed into each word. Once the sequence is in packed form, Albers and Hagerup use a form of merge sort to sort it; when two sequences are being merged to form a single longer sequence, the same bitonic sorting subroutine can be used to repeatedly extract packed words consisting of the smallest remaining elements of the two sequences. This algorithm gains enough of a speedup from its packed representation to sort its input in linear time whenever it is possible for a single word to contain  \\u03a9(log n log log n) keys; that is, when log K log n log log n \\u2264 cw for some constant c > 0.\\n\\nAlgorithms for few items\\nPigeonhole sort, counting sort, radix sort, and Van Emde Boas tree sorting all work best when the key size is small; for large enough keys, they become slower than comparison sorting algorithms. However, when the key size or the word size is very large relative to the number of items (or equivalently when the number of items is small), it may again become possible to sort quickly, using different algorithms that take advantage of the parallelism inherent in the ability to perform arithmetic operations on large words.\\nAn early result in this direction was provided by Ajtai, Fredman & Koml\\u00f3s (1984) using the cell-probe model of computation (an artificial model in which the complexity of an algorithm is measured only by the number of memory accesses it performs). Building on their work, Fredman & Willard (1994) described two data structures, the Q-heap and the atomic heap, that are implementable on a random access machine. The Q-heap is a bit-parallel version of a binary trie, and allows both priority queue operations and successor and predecessor queries to be performed in constant time for sets of O((log N)1/4) items, where N \\u2264 2w is the size of the precomputed tables needed to implement the data structure. The atomic heap is a B-tree in which each tree node is represented as a Q-heap; it allows constant time priority queue operations (and therefore sorting) for sets of (log N)O(1) items.\\nAndersson et al. (1998) provide a randomized algorithm called signature sort that allows for linear time sorting of sets of up to 2O((log w)1/2 \\u2212 \\u03b5) items at a time, for any constant \\u03b5 > 0. As in the algorithm of Kirkpatrick and Reisch, they perform range reduction using a representation of the keys as numbers in base b for a careful choice of b. Their range reduction algorithm replaces each digit by a signature, which is a hashed value with O(log n) bits such that different digit values have different signatures. If n is sufficiently small, the numbers formed by this replacement process will be significantly smaller than the original keys, allowing the non-conservative packed sorting algorithm of Albers & Hagerup (1997) to sort the replaced numbers in linear time. From the sorted list of replaced numbers, it is possible to form a compressed trie of the keys in linear time, and the children of each node in the trie may be sorted recursively using only keys of size b, after which a tree traversal produces the sorted order of the items.\\n\\nTrans-dichotomous algorithms\\nFredman & Willard (1993) introduced the transdichotomous model of analysis for integer sorting algorithms, in which nothing is assumed about the range of the integer keys and one must bound the algorithm's performance by a function of the number of data values alone. Alternatively, in this model, the running time for an algorithm on a set of n items is assumed to be the worst case running time for any possible combination of values of K and w. The first algorithm of this type was Fredman and Willard's fusion tree sorting algorithm, which runs in time O(n log n / log log n); this is an improvement over comparison sorting for any choice of K and w. An alternative version of their algorithm that includes the use of random numbers and integer division operations improves this to O(n\\u221alog n).\\nSince their work, even better algorithms have been developed. For instance, by repeatedly applying the Kirkpatrick\\u2013Reisch range reduction technique until the keys are small enough to apply the Albers\\u2013Hagerup packed sorting algorithm, it is possible to sort in time O(n log log n); however, the range reduction part of this algorithm requires either a large memory (proportional to \\u221aK) or randomization in the form of hash tables.Han & Thorup (2002) showed how to sort in randomized time O(n\\u221alog log n). Their technique involves using ideas related to signature sorting to partition the data into many small sublists, of a size small enough that signature sorting can sort each of them efficiently. It is also possible to use similar ideas to sort integers deterministically in time O(n log log n) and linear space. Using only simple arithmetic operations (no multiplications or table lookups) it is possible to sort in randomized expected time O(n log log n) or deterministically in time O(n (log log n)1 + \\u03b5) for any constant \\u03b5 > 0.\\n\\nReferences\\nFootnotes\\nSecondary sources\\nPrimary sources\"}, {\"Internal sort\": \"An internal sort is any data sorting process that takes place entirely within the main memory of a computer. This is possible whenever the data to be sorted is small enough to all be held in the main memory. like a hard-disk. Any reading or writing of data to and from this slower media can slow the sortation process considerably. This issue has implications for different sort algorithms.\\nSome common internal sorting algorithms include:\\n\\nBubble Sort\\nInsertion Sort\\nQuick Sort\\nHeap Sort\\nRadix Sort\\nSelection sortConsider a Bubblesort, where adjacent records are swapped in order to get them into the right order, so that records appear to \\u201cbubble\\u201d up and down through the dataspace. If this has to be done in chunks, then when we have sorted all the records in chunk 1, we move on to chunk 2, but we find that some of the records in chunk 1 need to \\u201cbubble through\\u201d chunk 2, and vice versa (i.e., there are records in chunk 2 that belong in chunk 1, and records in chunk 1 that belong in chunk 2 or later chunks). This will cause the chunks to be read and written back to disk many times as records cross over the boundaries between them, resulting in a considerable degradation of performance. If the data can all be held in memory as one large chunk, then this performance hit is avoided.\\nOn the other hand, some algorithms handle external sorting rather better. A Merge sort breaks the data up into chunks, sorts the chunks by some other algorithm (maybe bubblesort or Quick sort) and then recombines the chunks two by two so that each recombined chunk is in order. This approach minimises the number or reads and writes of data-chunks from disk, and is a popular external sort method.\"}, {\"Interpolation sort\": \"Interpolation sort is a kind of bucket sort. It uses an interpolation formula to assign data to the bucket. A general interpolation formula is:\\nInterpolation = INT(((Array[i] - min) / (max - min)) * (ArraySize - 1))\\n\\nAlgorithm\\nInterpolation sort (or histogram sort).\\nIt is a sorting algorithm that uses the interpolation formula to disperse data divide and conquer. Interpolation sort is also a variant of bucket sort algorithm. \\nThe interpolation sort method uses an array of record bucket lengths corresponding to the original number column. By operating the maintenance length array, the recursive algorithm can be prevented from changing the space complexity to \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   due to memory stacking. The segmentation record of the length array can using secondary function dynamically declare and delete the memory space of the array. The space complexity required to control the recursive program is \\n  \\n    \\n      \\n        O\\n        (\\n        3\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(3n)}\\n  . Contains a two-dimensional array of dynamically allocated memories and an array of record lengths. However the execution complexity can still be maintained as an efficient sorting method of \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        +\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n+k)}\\n  .\\nArray of dynamically allocated memory can be implemented by linked list, stack, queue, associative array, tree structure, etc. An array object such as JavaScript is applicable. The difference in data structure is related to the speed of data access and thus the time required for sorting.When the values in the ordered array are uniformly distributed approximately the arithmetic progression, the linear time of interpolation sort ordering is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\n\\nInterpolation sort algorithm\\nSet a bucket length array to record the length of the unsorted bucket. Initialize into the original array length.\\n[Main Sort] If the bucket length array is cleared and sorted is completed. Execute [Divide function] if it is not cleared.\\n[Divide function] Execute Divide by pop a bucket length from the end of the bucket length array. Find the maximum and minimum values in the bucket. If the maximum value is equal to the minimum value, the sorting is completed to stop Divide.\\nSet up a two-dimensional array as all empty buckets. Divide into the bucket according to the interpolation number.\\nAfter dividing into the buckets, push the length of the buckets into the array of bucket length. And put the items back into the original array one by one from all the buckets that are not empty.\\nReturn to [Main Sort].\\n\\nHistogram sort algorithm\\nThe NIST definition: An efficient 3-pass refinement of a bucket sort algorithm.\\n\\nThe first pass counts the number of items for each bucket in an auxiliary array, and then makes a running total so each auxiliary entry is the number of preceding items.\\nThe second pass puts each item in its proper bucket according to the auxiliary entry for the key of that item.\\nThe last pass sorts each bucket.\\n\\nPractice\\nInterpolation sort implementation\\nJavaScript code:\\n\\nInterpolation sort recursive method\\nWorst-case space complexity : \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n\\nHistogram sort implementation\\nVariant\\nInterpolation tag sort\\nInterpolation Tag Sort is a variant of Interpolation Sort. Applying the bucket sorting and dividing method, the array data is distributed into a limited number of buckets by mathematical interpolation formula, and the bucket then recursively the original processing program until the sorting is completed.\\nInterpolation tag sort is a recursive sorting method for interpolation sorting. To avoid stacking overflow caused by recursion, the memory crashes. Instead, use a Boolean data type tag array to operate the recursive function to release the memory. The extra memory space required is close to \\n  \\n    \\n      \\n        2\\n        n\\n        +\\n        (\\n        n\\n        )\\n        b\\n        i\\n        t\\n        s\\n      \\n    \\n    {\\\\displaystyle 2n+(n)bits}\\n  . Contains a two-dimensional array of dynamically allocated memory and a Boolean data type tag array. Stack, queue, associative array, and tree structure can be implemented as buckets.\\nAs the JavaScript array object is suitable for this sorting method, the difference in data structure is related to the speed of data access and thus the time required for sorting. The linear time \\u0398(n) is used when the values in the array to be sorted are evenly distributed. The bucket sort algorithm does not limit the sorting to the lower limit of \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        l\\n        o\\n        g\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nlogn)}\\n  . Interpolation tag sort average performance complexity is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        +\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n+k)}\\n  .\\n\\nInterpolation tag sort algorithm\\nSet a tag array equal to the original array size and initialize to a false value.\\n[Main Sort] Determines whether all buckets of the original array have been sorted. If the sorting is not completed, the [Divide function] is executed.\\n[Divide function] Find the maximum and minimum values in the bucket. If the maximum value is equal to the minimum value, the sorting is completed and the division is stopped.\\nSet up a two-dimensional array as all the empty buckets. Divide into the bucket according to the interpolation number.\\nAfter dividing into the bucket, mark the starting position of the bucket as a true value in the tag array. And put the items back into the original array one by one from all the buckets that are not empty.\\nReturn to [Main Sort].\\n\\nPractice\\nJavaScript code:\\n\\nIn-place Interpolation Tag Sort\\nThe in-place interpolation tag sort is an in-place algorithm of interpolation sort. In-place Interpolation Tag Sort can achieve sorting by only N times of swapping by maintaining N bit tags; however, the array to be sorted must be a continuous integer sequence and not repeated, or the series is completely evenly distributed to approximate The number of arithmetical progression.\\nThe factor column data must not be repeated. For example, sorting 0~100 can be sorted in one step. The number of exchanges is: \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  , the calculation time complexity is: \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  , and the worst space complexity is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n        b\\n        i\\n        t\\n        s\\n      \\n    \\n    {\\\\displaystyle O(n)bits}\\n  . If the characteristics of the series meet the conditional requirements of this sorting method: \\\"The array is a continuous integer or an arithmetical progression that does not repeat\\\", the in-place interpolation tag sort will be an excellent sorting method that is extremely fast and saves memory space.\\n\\nIn-place Interpolation Tag Sort Algorithm\\nIn-place Interpolation Tag Sort sorts non-repeating consecutive integer series, only one Boolean data type tag array with the same length as the original array, the array calculates the interpolation of the data from the beginning, and the interpolation points to a new position of the array. Position, the position that has been swapped is marked as true in the corresponding position of the tag array, and is incremented until the end of the array is sorted.\\nAlgorithm process:\\n\\nSet an equal number of tag arrays to initialize to false values.\\nVisit the array when tag[i] is false, calculate the position corresponding to the interpolation=p.\\nSwap a[i] and a[p], let tag[p] = true.\\nThe tour array is completed and the sorting is completed.\\n\\nPractice\\nJavaScript code:\\n\\nThe origin of In-place sorting performed in\\nO\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time\\nIn \\\"Mathematical Analysis of Algorithms\\\", (Information Processing '71, North Holland Publ.'72) Donald Knuth remarked \\\"... that research on computional complexity is an interesting way to sharpen our tools for more routine problems we face from day to day.\\\" \\nThe famous American computer scientist Donald Knuth in the mathematical analysis of algorithms pointed out that:\\\"With respect to the sorting problem, Knuth points out, that time effective in-situ permutation is inherently connected with the problem of finding the cycle leaders, and in-situ permutations could easily be performed in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time if we would be allowed to manipulate n extra \\\"tag\\\" bits specifying how much of the permutation has been carried out at any time. Without such tag bits, he concludes \\\"it seems reasonable to conjecture that every algorithm will require for in-situ permutation at least \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        l\\n        o\\n        g\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nlogn)}\\n   steps on the average.\\\" \\nThe In-place Interpolation Tag Sort is one of the sorting algorithms that the Donald Knuth professor said: \\\"manipulate n extra \\\"tag\\\" bits...finding the cycle leaders, and in-situ permutations could easily be performed in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time\\\".\\n\\nSimilar sorting method\\nFlashsort\\nProxmap sort\\nAmerican flag sort\\n\\nBucket sort mixing other sorting methods and recursive algorithm\\nBucket sort can be mixed with other sorting methods to complete sorting. If it is sorted by bucket sort and insert sort, also is a fairly efficient sorting method. But when the series appears a large deviation from the value: For example, when the maximum value of the series is greater than N times the next largest value. After the series of columns are processed, the distribution is that all the elements except the maximum value fall into the same bucket. The second sorting method uses insert sort. May cause execution complexity to fall into \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  . This has lost the meaning and high-speed performance of using bucket sort.\\nInterpolation sort is a way of recursively using bucket sort. After performing recursion, still use bucket sort to disperse the series. This can avoid the above situation. If you want to make the recursive interpolation sort execution complexity fall into \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  , it is necessary to present a factorial amplification in the entire series. In fact, there is very little chance that a series of special distributions will occur.\\n\\nReferences\\nExternal links\\ninterpolationSort.html\\nhistogramSort.html\\nThe FlashSort Algorithm\\nMathematical Analysis of Algorithms\\nhttp://www.drdobbs.com/database/the-flashsort1-algorithm/184410496\\n\\u6876\\u6392\\u5e8f\\u905e\\u8ff4\\u65b9\\u5f0f\\u6f14\\u7b97\\u6cd5 Bucket sort Recursive method. Whale Chen 2012/09/16\\n\\u63d2\\u503c\\u6a19\\u7c3d\\u6392\\u5e8f\\u6f14\\u7b97\\u6cd5 Interpolation Tag Sort Algorithm. Whale Chen 2013/03/24\\ninterpolation sort (Pascal version available)\\nw3schools JavaScript Array Sort testing platform\"}, {\"Inversion (discrete mathematics)\": \"In computer science and discrete mathematics, an inversion in a sequence is a pair of elements that are out of their natural order.\\n\\nDefinitions\\nInversion\\nLet \\n  \\n    \\n      \\n        \\u03c0\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   be a permutation.  \\nThere is an inversion of \\n  \\n    \\n      \\n        \\u03c0\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   between \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   and \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n   if \\n  \\n    \\n      \\n        i\\n        <\\n        j\\n      \\n    \\n    {\\\\displaystyle i<j}\\n   and \\n  \\n    \\n      \\n        \\u03c0\\n        (\\n        i\\n        )\\n        >\\n        \\u03c0\\n        (\\n        j\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\pi (i)>\\\\pi (j)}\\n  .  The inversion is indicated by an ordered pair containing either the places \\n  \\n    \\n      \\n        (\\n        i\\n        ,\\n        j\\n        )\\n      \\n    \\n    {\\\\displaystyle (i,j)}\\n   or the elements \\n  \\n    \\n      \\n        \\n          \\n            (\\n          \\n        \\n        \\u03c0\\n        (\\n        i\\n        )\\n        ,\\n        \\u03c0\\n        (\\n        j\\n        )\\n        \\n          \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\bigl (}\\\\pi (i),\\\\pi (j){\\\\bigr )}}\\n  .The inversion set is the set of all inversions.  A permutation's inversion set using place-based notation is the same as the inverse permutation's inversion set using element-based notation with the two components of each ordered pair exchanged.  Likewise, a permutation's inversion set using element-based notation is the same as the inverse permutation's inversion set using place-based notation with the two components of each ordered pair exchanged.Inversions are usually defined for permutations, but may also be defined for sequences:Let \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   be a sequence (or multiset permutation). If \\n  \\n    \\n      \\n        i\\n        <\\n        j\\n      \\n    \\n    {\\\\displaystyle i<j}\\n   and \\n  \\n    \\n      \\n        S\\n        (\\n        i\\n        )\\n        >\\n        S\\n        (\\n        j\\n        )\\n      \\n    \\n    {\\\\displaystyle S(i)>S(j)}\\n  , either the pair of places \\n  \\n    \\n      \\n        (\\n        i\\n        ,\\n        j\\n        )\\n      \\n    \\n    {\\\\displaystyle (i,j)}\\n   or the pair of elements \\n  \\n    \\n      \\n        \\n          \\n            (\\n          \\n        \\n        S\\n        (\\n        i\\n        )\\n        ,\\n        S\\n        (\\n        j\\n        )\\n        \\n          \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\bigl (}S(i),S(j){\\\\bigr )}}\\n   is called an inversion of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n  .\\nFor sequences, inversions according to the element-based definition are not unique, because different pairs of places may have the same pair of values.\\n\\nInversion number\\nThe inversion number \\n  \\n    \\n      \\n        \\n          \\n            i\\n            n\\n            v\\n          \\n        \\n        (\\n        X\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {inv}}(X)}\\n   of a sequence \\n  \\n    \\n      \\n        X\\n        =\\n        \\u27e8\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle X=\\\\langle x_{1},\\\\dots ,x_{n}\\\\rangle }\\n  ,  is the cardinality of the inversion set. It is a common measure of sortedness (sometimes called presortedness) of a permutation or sequence. The inversion number is between 0 and \\n  \\n    \\n      \\n        \\n          \\n            \\n              n\\n              (\\n              n\\n              \\u2212\\n              1\\n              )\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n(n-1)}{2}}}\\n   inclusive.  A permutation and its inverse have the same inversion number.\\nFor example \\n  \\n    \\n      \\n        \\n          \\n            i\\n            n\\n            v\\n          \\n        \\n        (\\n        \\u27e8\\n        1\\n        ,\\n        2\\n        ,\\n        \\u2026\\n        ,\\n        n\\n        \\u27e9\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {inv}}(\\\\langle 1,2,\\\\dots ,n\\\\rangle )=0}\\n   since the sequence is ordered. Also, when \\n  \\n    \\n      \\n        n\\n        =\\n        2\\n        m\\n      \\n    \\n    {\\\\displaystyle n=2m}\\n   is even, \\n  \\n    \\n      \\n        \\n          \\n            i\\n            n\\n            v\\n          \\n        \\n        (\\n        \\u27e8\\n        m\\n        +\\n        1\\n        ,\\n        m\\n        +\\n        2\\n        ,\\n        \\u2026\\n        ,\\n        2\\n        m\\n        ,\\n        1\\n        ,\\n        2\\n        ,\\n        \\u2026\\n        ,\\n        m\\n        \\u27e9\\n        )\\n        =\\n        \\n          m\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {inv}}(\\\\langle m+1,m+2,\\\\dots ,2m,1,2,\\\\dots ,m\\\\rangle )=m^{2}}\\n   (because each pair \\n  \\n    \\n      \\n        (\\n        1\\n        \\u2264\\n        i\\n        \\u2264\\n        m\\n        <\\n        j\\n        \\u2264\\n        2\\n        m\\n        )\\n      \\n    \\n    {\\\\displaystyle (1\\\\leq i\\\\leq m<j\\\\leq 2m)}\\n   is an inversion). This last example shows that a set that is intuitively \\\"nearly sorted\\\" can still have a quadratic number of inversions.\\nThe inversion number is the number of crossings in the arrow diagram of the permutation, the permutation's Kendall tau distance from the identity permutation, and the sum of each of the inversion related vectors defined below.\\nOther measures of sortedness include the minimum number of elements that can be deleted from the sequence to yield a fully sorted sequence, the number and lengths of sorted \\\"runs\\\" within the sequence, the Spearman footrule (sum of distances of each element from its sorted position), and the smallest number of exchanges needed to sort the sequence. Standard comparison sorting algorithms can be adapted to compute the inversion number in time O(n log n).\\n\\nInversion related vectors\\nThree similar vectors are in use that condense the inversions of a permutation into a vector that uniquely determines it. They are often called inversion vector or Lehmer code. (A list of sources is found here.)\\nThis article uses the term inversion vector (\\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n  ) like Wolfram. The remaining two vectors are sometimes called left and right inversion vector, but to avoid confusion with the inversion vector this article calls them  left inversion count (\\n  \\n    \\n      \\n        l\\n      \\n    \\n    {\\\\displaystyle l}\\n  ) and right inversion count (\\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n  ). Interpreted as a factorial number the left inversion count gives the permutations reverse colexicographic, and the right inversion count gives the lexicographic index.\\n\\nInversion vector \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n  :With the element-based definition \\n  \\n    \\n      \\n        v\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle v(i)}\\n   is the number of inversions whose smaller (right) component is \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  .\\n\\n  \\n    \\n      \\n        v\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle v(i)}\\n   is the number of elements in \\n  \\n    \\n      \\n        \\u03c0\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   greater than \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   before \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  .\\n\\n  \\n    \\n      \\n        v\\n        (\\n        i\\n        )\\n         \\n         \\n        =\\n         \\n         \\n        #\\n        {\\n        k\\n        \\u2223\\n        k\\n        >\\n        i\\n         \\n        \\u2227\\n         \\n        \\n          \\u03c0\\n          \\n            \\u2212\\n            1\\n          \\n        \\n        (\\n        k\\n        )\\n        <\\n        \\n          \\u03c0\\n          \\n            \\u2212\\n            1\\n          \\n        \\n        (\\n        i\\n        )\\n        }\\n      \\n    \\n    {\\\\displaystyle v(i)~~=~~\\\\#\\\\{k\\\\mid k>i~\\\\land ~\\\\pi ^{-1}(k)<\\\\pi ^{-1}(i)\\\\}}\\n  Left inversion count \\n  \\n    \\n      \\n        l\\n      \\n    \\n    {\\\\displaystyle l}\\n  :With the place-based definition \\n  \\n    \\n      \\n        l\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle l(i)}\\n   is the number of inversions whose bigger (right) component is \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  .\\n\\n  \\n    \\n      \\n        l\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle l(i)}\\n   is the number of elements in \\n  \\n    \\n      \\n        \\u03c0\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   greater than \\n  \\n    \\n      \\n        \\u03c0\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\pi (i)}\\n   before \\n  \\n    \\n      \\n        \\u03c0\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\pi (i)}\\n  .\\n\\n  \\n    \\n      \\n        l\\n        (\\n        i\\n        )\\n         \\n         \\n        =\\n         \\n         \\n        #\\n        \\n          {\\n          \\n            k\\n            \\u2223\\n            k\\n            <\\n            i\\n             \\n            \\u2227\\n             \\n            \\u03c0\\n            (\\n            k\\n            )\\n            >\\n            \\u03c0\\n            (\\n            i\\n            )\\n          \\n          }\\n        \\n      \\n    \\n    {\\\\displaystyle l(i)~~=~~\\\\#\\\\left\\\\{k\\\\mid k<i~\\\\land ~\\\\pi (k)>\\\\pi (i)\\\\right\\\\}}\\n  Right inversion count \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n  , often called Lehmer code:With the place-based definition \\n  \\n    \\n      \\n        r\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle r(i)}\\n   is the number of inversions whose smaller (left) component is \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  .\\n\\n  \\n    \\n      \\n        r\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle r(i)}\\n   is the number of elements in \\n  \\n    \\n      \\n        \\u03c0\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   smaller than \\n  \\n    \\n      \\n        \\u03c0\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\pi (i)}\\n   after \\n  \\n    \\n      \\n        \\u03c0\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\pi (i)}\\n  .\\n\\n  \\n    \\n      \\n        r\\n        (\\n        i\\n        )\\n         \\n         \\n        =\\n         \\n         \\n        #\\n        {\\n        k\\n        \\u2223\\n        k\\n        >\\n        i\\n         \\n        \\u2227\\n         \\n        \\u03c0\\n        (\\n        k\\n        )\\n        <\\n        \\u03c0\\n        (\\n        i\\n        )\\n        }\\n      \\n    \\n    {\\\\displaystyle r(i)~~=~~\\\\#\\\\{k\\\\mid k>i~\\\\land ~\\\\pi (k)<\\\\pi (i)\\\\}}\\n  Both \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   can be found with the help of a Rothe diagram, which is a permutation matrix with the 1s represented by dots, and an inversion (often represented by a cross) in every position that has a dot to the right and below it. \\n  \\n    \\n      \\n        r\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle r(i)}\\n   is the sum of inversions in row \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   of the Rothe diagram, while \\n  \\n    \\n      \\n        v\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle v(i)}\\n   is the sum of inversions in column \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  . The permutation matrix of the inverse is the transpose, therefore \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   of a permutation is \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   of its inverse, and vice versa.\\n\\nExample: All permutations of four elements\\nThe following sortable table shows the 24 permutations of four elements (in the \\n  \\n    \\n      \\n        \\u03c0\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   column) with their place-based inversion sets (in the p-b column), inversion related vectors (in the \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n  , \\n  \\n    \\n      \\n        l\\n      \\n    \\n    {\\\\displaystyle l}\\n  , and \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   columns), and inversion numbers (in the # column). (The columns with smaller print and no heading are reflections of the columns next to them, and can be used to sort them in colexicographic order.)\\nIt can be seen that \\n  \\n    \\n      \\n        v\\n      \\n    \\n    {\\\\displaystyle v}\\n   and \\n  \\n    \\n      \\n        l\\n      \\n    \\n    {\\\\displaystyle l}\\n   always have the same digits, and that \\n  \\n    \\n      \\n        l\\n      \\n    \\n    {\\\\displaystyle l}\\n   and \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   are both related to the place-based inversion set. The nontrivial elements of \\n  \\n    \\n      \\n        l\\n      \\n    \\n    {\\\\displaystyle l}\\n   are the sums of the descending diagonals of the shown triangle, and those of \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   are the sums of the ascending diagonals. (Pairs in descending diagonals have the right components 2, 3, 4 in common, while pairs in ascending diagonals have the left components 1, 2, 3 in common.)\\nThe default order of the table is reverse colex order by \\n  \\n    \\n      \\n        \\u03c0\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n  , which is the same as colex order by \\n  \\n    \\n      \\n        l\\n      \\n    \\n    {\\\\displaystyle l}\\n  . Lex order by \\n  \\n    \\n      \\n        \\u03c0\\n      \\n    \\n    {\\\\displaystyle \\\\pi }\\n   is the same as lex order by \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n  .\\n\\nWeak order of permutations\\nThe set of permutations on n items can be given the structure of a partial order, called the weak order of permutations, which forms a lattice.\\nThe Hasse diagram of the inversion sets ordered by the subset relation forms the skeleton of a permutohedron.\\nIf a permutation is assigned to each inversion set using the place-based definition, the resulting order of permutations is that of the permutohedron, where an edge corresponds to the swapping of two elements with consecutive values. This is the weak order of permutations. The identity is its minimum, and the permutation formed by reversing the identity is its maximum.\\nIf a permutation were assigned to each inversion set using the element-based definition, the resulting order of permutations would be that of a Cayley graph, where an edge corresponds to the swapping of two elements on consecutive places. This Cayley graph of the symmetric group is similar to its permutohedron, but with each permutation replaced by its inverse.\\n\\nSee also\\nFactorial number system\\nPermutation graph\\nTranspositions, simple transpositions, inversions and sorting\\nDamerau\\u2013Levenshtein distance\\nParity of a permutationSequences in the OEIS:\\n\\nSequences related to factorial base representation\\nFactorial numbers: A007623 and A108731\\nInversion numbers: A034968\\nInversion sets of finite permutations interpreted as binary numbers: A211362   (related permutation: A211363)\\nFinite permutations that have only 0s and 1s in their inversion vectors: A059590   (their inversion sets: A211364)\\nNumber of permutations of n elements with k inversions; Mahonian numbers: A008302   (their row maxima; Kendall-Mann numbers: A000140)\\nNumber of connected labeled graphs with n edges and n nodes: A057500\\n\\nReferences\\nSource bibliography\\nFurther reading\\n\\n\\n=== Presortedness measures ===\"}, {\"K-way merge algorithm\": \"In computer science, k-way merge algorithms or multiway merges are a specific type of sequence merge algorithms that specialize in taking in k sorted lists and merging them into a single sorted list. These merge algorithms generally refer to merge algorithms that take in a number of sorted lists greater than two. Two-way merges are also referred to as binary merges.\\n\\nTwo-way merge\\nA 2-way merge, or a binary merge, has been studied extensively due to its key role in merge sort. An example of such is the classic merge that appears frequently in merge sort examples. The classic merge outputs the data item with the lowest key at each step; given some sorted lists, it produces a sorted list containing all the elements in any of the input lists, and it does so in time proportional to the sum of the lengths of the input lists.\\nDenote by A[1..p] and B[1..q] two arrays sorted in increasing order.\\nFurther, denote by C[1..n] the output array.\\nThe canonical 2-way merge algorithm stores indices i, j, and k into A, B, and C respectively.\\nInitially, these indices refer to the first element, i.e., are 1.\\nIf A[i] < B[j], then the algorithm copies A[i] into C[k] and increases i and k.  \\nOtherwise, the algorithm copies B[j] into C[k] and increases j and k.\\nA special case arises if either i or j have reached the end of A or B.\\nIn this case the algorithm copies the remaining elements of B or A into C and terminates.\\n\\nk-way merge\\nThe k-way merge problem consists of merging k sorted arrays to produce a single sorted array with the same elements.\\nDenote by n the total number of elements.\\nn is equal to the size of the output array and the sum of the sizes of the k input arrays.\\nFor simplicity, we assume that none of the input arrays is empty.\\nAs a consequence \\n  \\n    \\n      \\n        k\\n        \\u2264\\n        n\\n      \\n    \\n    {\\\\displaystyle k\\\\leq n}\\n  , which simplifies the reported running times.\\nThe problem can be solved in \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        (\\n        n\\n        \\u22c5\\n        log\\n        \\u2061\\n        (\\n        k\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}(n\\\\cdot \\\\log(k))}\\n   running time with \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}(n)}\\n   space.\\nSeveral algorithms that achieve this running time exist.\\n\\nIterative 2-way merge\\nThe problem can be solved by iteratively merging two of the k arrays using a 2-way merge until only a single array is left. If the arrays are merged in arbitrary order, then the resulting running time is only O(kn). This is suboptimal.\\nThe running time can be improved by iteratively merging the first with the second, the third with the fourth, and so on. As the number of arrays is halved in each iteration, there are only \\u0398(log k) iterations. In each iteration every element is moved exactly once. The running time per iteration is therefore in \\u0398(n) as n is the number of elements. The total running time is therefore in \\u0398(n log k).\\nWe can further improve upon this algorithm, by iteratively merging the two shortest arrays. It is clear that this minimizes the running time and can therefore not be worse than the strategy described in the previous paragraph. The running time is therefore in O(n log k). Fortunately, in border cases the running time can be better. Consider for example the degenerate case, where all but one array contain only one element. The strategy explained in the previous paragraph needs \\u0398(n log k) running time, while the improved one only needs \\u0398(n + k log k) running time.\\n\\nDirect k-way merge\\nIn this case, we would simultaneously merge k-runs together.\\nA straightforward implementation would scan all k arrays to determine the minimum.\\nThis straightforward implementation results in a running time of \\u0398(kn).\\nNote that this is mentioned only as a possibility, for the sake of discussion. Although it would work, it is not efficient.\\nWe can improve upon this by computing the smallest element faster.\\nBy using either heaps, tournament trees, or splay trees, the smallest element can be determined in O(log k) time.\\nThe resulting running times are therefore in O(n log k).\\nThe heap is more commonly used, although a tournament tree is faster in practice. A heap uses approximately 2*log(k) comparisons in each step because it handles the tree from the root down to the bottom and needs to compare both children of each node. Meanwhile, a tournament tree only needs log(k) comparisons because it starts on the bottom of the tree and works up to the root, only making a single comparison in each layer. The tournament tree should therefore be the preferred implementation.\\n\\nHeap\\nThe idea is to maintain a min-heap of the k lists, each keyed by their smallest current element.  A simple algorithm builds an output buffer with nodes from the heap.  Start by building a min-heap of nodes, where each node consists of a head element of the list, and the rest (or tail) of the list.  Because the lists are sorted initially, the head is the smallest element of each list; the heap property guarantees that the root contains the minimum element over all lists.  Extract the root node from the heap, add the head element to the output buffer, create a new node out of the tail, and insert it into the heap.  Repeat until there is only one node left in the heap, at which point just append that remaining list (head and tail) to the output buffer.\\nUsing pointers, an in-place heap algorithm \\n\\nallocates a min-heap of pointers into the input arrays.\\nInitially these pointers point to the smallest elements of the input array.\\nThe pointers are sorted by the value that they point to.\\nIn an O(k) preprocessing step the heap is created using the standard heapify procedure.\\nAfterwards, the algorithm iteratively transfers the element that the root pointer points to, increases this pointer and executes the standard decrease key procedure upon the root element.\\nThe running time of the increase key procedure is bounded by O(log k).\\nAs there are n elements, the total running time is O(n log k).\\nNote that the operation of replacing the key and iteratively doing decrease-key or sift-down are not supported by many Priority Queue libraries such as C++ stl and Java. Doing an extract-min and insert function is less efficient.\\n\\nTournament Tree\\nThe Tournament Tree  is based on an elimination tournament, as in sports competitions. In each game, two of the input elements compete. The winner is promoted to the next round. Therefore, we get a binary tree of games. The list is sorted in ascending order, so the winner of a game is the smaller one of both elements.\\n\\nFor k-way merging, it is more efficient to only store the loser of each game (see image). The data structure is therefore called a loser tree. When building the tree or replacing an element with the next one from its list, we still promote the winner of the game to the top. The tree is filled like in a sports match but the nodes only store the loser. Usually, an additional node above the root is added that represents the overall winner. Every leaf stores a pointer to one of the input arrays. Every inner node stores a value and an index. The index of an inner node indicates which input array the value comes from. The value contains a copy of the first element of the corresponding input array.\\nThe algorithm iteratively appends the minimum element to the result and then removes the element from the corresponding input list. It updates the nodes on the path from the updated leaf to the root (replacement selection). The removed element is the overall winner. Therefore, it has won each game on the path from the input array to the root. When selecting a new element from the input array, the element needs to compete against the previous losers on the path to the root. When using a loser tree, the partner for replaying the games is already stored in the nodes. The loser of each replayed game is written to the node and the winner is iteratively promoted to the top. When the root is reached, the new overall winner was found and can be used in the next round of merging.\\nThe images of the tournament tree and the loser tree in this section use the same data and can be compared to understand the way a loser tree works.\\n\\nAlgorithm\\nA tournament tree can be represented as a balanced binary tree by adding sentinels to the input lists (i.e. adding a member to the end of each list with a value of infinity) and by adding null lists (comprising only a sentinel) until the number of lists is a power of two. The balanced tree can be stored in a single array. The parent element can be reached by dividing the current index by two.\\nWhen one of the leaves is updated, all games from the leaf to the root are replayed. In the following pseudocode, an object oriented tree is used instead of an array because it is easier to understand. Additionally, the number of lists to merge is assumed to be a power of two.\\n\\nfunction merge(L1, \\u2026, Ln)\\n    buildTree(heads of L1, \\u2026, Ln)\\n    while tree has elements\\n        winner := tree.winner\\n        output winner.value\\n        new := winner.index.next\\n        replayGames(winner, new) // Replacement selection\\n\\nfunction replayGames(node, new)\\n    loser, winner := playGame(node, new)\\n    node.value := loser.value\\n    node.index := loser.index\\n    if node != root\\n        replayGames(node.parent, winner)\\n\\nfunction buildTree(elements)\\n    nextLayer := new Array()\\n    while elements not empty\\n        el1 := elements.take()\\n        el2 := elements.take()\\n        loser, winner := playGame(el1, el2)\\n        parent := new Node(el1, el2, loser)\\n        nextLayer.add(parent)\\n    if nextLayer.size == 1\\n        return nextLayer // only root\\n    else\\n        return buildTree(nextLayer)\\n\\nRunning time\\nIn the beginning, the tree is first created in time \\u0398(k). In each step of merging, only the games on the path from the new element to the root need to be replayed. In each layer, only one comparison is needed. As the tree is balanced, the path from one of the input arrays to the root contains only \\u0398(log k) elements. In total, there are n elements that need to be transferred. The resulting total running time is therefore in \\u0398(n log k).\\n\\nExample\\nThe following section contains a detailed example for the replacement selection step and one example for a complete merge containing multiple replacement selections.\\n\\nReplacement selection\\nGames are replayed from the bottom to the top. In each layer of the tree, the currently stored element of the node and the element that was provided from the layer below compete. The winner is promoted to the top until we found the new overall winner. The loser is stored in the node of the tree.\\n\\nMerge\\nTo execute the merge itself, the overall smallest element is repeatedly replaced with the next input element. After that, the games to the top are replayed.\\nThis example uses four sorted arrays as input.\\n\\n{2, 7, 16}\\n{5, 10, 20}\\n{3, 6, 21}\\n{4, 8, 9}\\n\\nThe algorithm is initiated with the heads of each input list. Using these elements, a binary tree of losers is built. For merging, the lowest list element 2 is determined by looking at the overall minimum element at the top of the tree. That value is then popped off, and its leaf is refilled with 7, the next value in the input list. The games on the way to the top are replayed like in the previous section about replacement selection. The next element that is removed is 3. Starting from the next value in the list, 6, the games are replayed up until the root. This is being repeated until the minimum of the tree equals infinity.\\n\\nLower bound on running time\\nOne can show that no comparison-based k-way merge algorithm exists with a running time in O(n f(k)) where f grows asymptotically slower than a logarithm, and n being the total number of elements.\\n(Excluding data with desirable distributions such as disjoint ranges.)\\nThe proof is a straightforward reduction from comparison-based sorting.\\nSuppose that such an algorithm existed, then we could construct a comparison-based sorting algorithm with running time O(n f(n)) as follows:\\nChop the input array into n arrays of size 1. \\nMerge these n arrays with the k-way merge algorithm.\\nThe resulting array is sorted and the algorithm has a running time in O(n f(n)).\\nThis is a contradiction to the well-known result that no comparison-based sorting algorithm with a worst case running time below O(n log n) exists.\\n\\nExternal sorting\\n\\nk-way merges are used in external sorting procedures. External sorting algorithms are a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted do not fit into the main memory of a computing device (usually RAM) and instead they must reside in the slower external memory (usually a hard drive). k-way merge algorithms usually take place in the second stage of external sorting algorithms, much like they do for merge sort.\\nA multiway merge allows for the files outside of memory to be merged in fewer passes than in a binary merge. If there are 6 runs that need be merged then a binary merge would need to take 3 merge passes, as opposed to a 6-way merge's single merge pass. This reduction of merge passes is especially important considering the large amount of information that is usually being sorted in the first place, allowing for greater speed-ups while also reducing the amount of accesses to slower storage.\\n\\n\\n== References ==\"}, {\"Kaprekar's routine\": \"In number theory, Kaprekar's routine is an iterative algorithm that, with each iteration, takes a natural number in a given number base, creates two new numbers by sorting the digits of its number by descending and ascending order, and subtracts the second from the first to yield the natural number for the next iteration. It is named after its inventor, the Indian mathematician D. R. Kaprekar.\\nKaprekar showed that in the case of four-digit numbers in base 10, if the initial number has at least two distinct digits, after seven iterations this process always yields the number 6174, which is now known as Kaprekar's constant.\\n\\nDefinition and properties\\nThe algorithm is as follows:\\nChoose any natural number \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   in a given number base \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  . This is the first number of the sequence.\\nCreate a new number \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   by sorting the digits of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   in descending order, and another number \\n  \\n    \\n      \\n        \\u03b2\\n      \\n    \\n    {\\\\displaystyle \\\\beta }\\n    by sorting the digits of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   in ascending order. These numbers may have leading zeros, which are discarded (or alternatively, retained). Subtract \\n  \\n    \\n      \\n        \\u03b1\\n        \\u2212\\n        \\u03b2\\n      \\n    \\n    {\\\\displaystyle \\\\alpha -\\\\beta }\\n   to produce the next number of the sequence.\\nRepeat step 2.The sequence is called a Kaprekar sequence and the function \\n  \\n    \\n      \\n        \\n          K\\n          \\n            b\\n          \\n        \\n        (\\n        n\\n        )\\n        =\\n        \\u03b1\\n        \\u2212\\n        \\u03b2\\n      \\n    \\n    {\\\\displaystyle K_{b}(n)=\\\\alpha -\\\\beta }\\n    is the Kaprekar mapping. Some numbers map to themselves; these are the fixed points of the Kaprekar mapping, and are called Kaprekar's constants. Zero is a Kaprekar's constant for all bases \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  , and so is called a trivial Kaprekar's constants. All other Kaprekar's constant are nontrivial Kaprekar's constants.\\nFor example, in base 10, starting with 3524,\\n\\n  \\n    \\n      \\n        \\n          K\\n          \\n            10\\n          \\n        \\n        (\\n        3524\\n        )\\n        =\\n        5432\\n        \\u2212\\n        2345\\n        =\\n        3087\\n      \\n    \\n    {\\\\displaystyle K_{10}(3524)=5432-2345=3087}\\n  \\n  \\n    \\n      \\n        \\n          K\\n          \\n            10\\n          \\n        \\n        (\\n        3087\\n        )\\n        =\\n        8730\\n        \\u2212\\n        378\\n        =\\n        8352\\n      \\n    \\n    {\\\\displaystyle K_{10}(3087)=8730-378=8352}\\n  \\n  \\n    \\n      \\n        \\n          K\\n          \\n            10\\n          \\n        \\n        (\\n        8352\\n        )\\n        =\\n        8532\\n        \\u2212\\n        2358\\n        =\\n        6174\\n      \\n    \\n    {\\\\displaystyle K_{10}(8352)=8532-2358=6174}\\n  \\n  \\n    \\n      \\n        \\n          K\\n          \\n            10\\n          \\n        \\n        (\\n        6174\\n        )\\n        =\\n        7641\\n        \\u2212\\n        1467\\n        =\\n        6174\\n      \\n    \\n    {\\\\displaystyle K_{10}(6174)=7641-1467=6174}\\n  with 6174 as a Kaprekar's constant.\\nAll Kaprekar sequences will either reach one of these fixed points or will result in a repeating cycle. Either way, the end result is reached in a fairly small number of steps.\\nNote that the numbers \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   and \\n  \\n    \\n      \\n        \\u03b2\\n      \\n    \\n    {\\\\displaystyle \\\\beta }\\n   have the same digit sum and hence the same remainder modulo \\n  \\n    \\n      \\n        b\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle b-1}\\n  . Therefore, each number in a Kaprekar sequence of base \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   numbers (other than possibly the first) is a multiple of \\n  \\n    \\n      \\n        b\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle b-1}\\n  .\\nWhen leading zeroes are retained, only repdigits lead to the trivial Kaprekar's constant.\\n\\nFamilies of Kaprekar's constants\\nIn base 4, it can easily be shown that all numbers of the form 3021, 310221, 31102221, 3...111...02...222...1 (where the length of the \\\"1\\\" sequence and the length of the \\\"2\\\" sequence are the same) are fixed points of the Kaprekar mapping.\\nIn base 10, it can easily be shown that all numbers of the form 6174, 631764, 63317664, 6...333...17...666...4 (where the length of the \\\"3\\\" sequence and the length of the \\\"6\\\" sequence are the same) are fixed points of the Kaprekar mapping.\\n\\nb = 2k\\nIt can be shown that all natural numbers\\n\\n  \\n    \\n      \\n        m\\n        =\\n        (\\n        k\\n        )\\n        \\n          b\\n          \\n            2\\n            n\\n            +\\n            3\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                0\\n              \\n              \\n                n\\n                \\u2212\\n                1\\n              \\n            \\n            \\n              b\\n              \\n                i\\n              \\n            \\n          \\n          )\\n        \\n        +\\n        (\\n        k\\n        \\u2212\\n        1\\n        )\\n        \\n          b\\n          \\n            2\\n            n\\n            +\\n            2\\n          \\n        \\n        +\\n        (\\n        2\\n        k\\n        \\u2212\\n        1\\n        )\\n        \\n          b\\n          \\n            n\\n            +\\n            1\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                0\\n              \\n              \\n                n\\n              \\n            \\n            \\n              b\\n              \\n                i\\n              \\n            \\n          \\n          )\\n        \\n        +\\n        (\\n        k\\n        \\u2212\\n        1\\n        )\\n        b\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                0\\n              \\n              \\n                n\\n                \\u2212\\n                1\\n              \\n            \\n            \\n              b\\n              \\n                i\\n              \\n            \\n          \\n          )\\n        \\n        +\\n        (\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle m=(k)b^{2n+3}\\\\left(\\\\sum _{i=0}^{n-1}b^{i}\\\\right)+(k-1)b^{2n+2}+(2k-1)b^{n+1}\\\\left(\\\\sum _{i=0}^{n}b^{i}\\\\right)+(k-1)b\\\\left(\\\\sum _{i=0}^{n-1}b^{i}\\\\right)+(k)}\\n  are fixed points of the Kaprekar mapping in even base \\n  \\n    \\n      \\n        b\\n        =\\n        2\\n        k\\n      \\n    \\n    {\\\\displaystyle b=2k}\\n   for all natural numbers \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  .\\n\\nKaprekar's constants and cycles of the Kaprekar mapping for specific base b\\nAll numbers are expressed in base \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  , using A\\u2212Z to represent digit values 10 to 35.\\n\\nKaprekar's constants in base 10\\nNumbers of length four digits\\nIn 1949 D. R. Kaprekar discovered that if the above process is applied to base 10 numbers of four digits, the resulting sequence will almost always converge to the value 6174 in at most eight iterations, except for a small set of initial numbers which converge instead to 0. The number 6174 is the first Kaprekar's constant to be discovered, and thus is sometimes known as Kaprekar's constant.The set of numbers that converge to zero depends on whether leading zeros are  discarded (the usual formulation) or are retained (as in Kaprekar's original formulation).\\nIn the usual formulation, there are 77 four-digit numbers that converge to zero, for example 2111. However, in Kaprekar's original formulation the leading zeros are retained, and only repdigits such as 1111 or 2222 map to zero. This contrast is illustrated below:\\n\\nBelow is a flowchart. Leading zeros are retained, however the only difference when leading zeros are discarded is that instead of 0999 connecting to 8991, we get 999 connecting to 0.\\n\\nNumbers of length three digits\\nIf the Kaprekar routine is applied to numbers of three digits in base 10, the resulting sequence will almost always converge to the value 495 in at most six iterations, except for a small set of initial numbers which converge instead to 0.The set of numbers that converge to zero depends on whether leading zeros are discarded (the usual formulation) or are retained (as in Kaprekar's original formulation). In the usual formulation, there are 60 three-digit numbers that converge to zero, for example 211. However, in Kaprekar's original formulation the leading zeros are retained, and only repdigits such as 111 or 222 map to zero.\\nBelow is a flowchart. Leading zeros are retained, however the only difference when leading zeros are discarded is that instead of 099 connecting to 891, we get 99 connecting to 0.\\n\\nOther digit lengths\\nFor digit lengths other than three or four (in base 10), the routine may terminate at one of several fixed points or may enter one of several cycles instead, depending on the starting value of the sequence. See the table in the section above for base 10 fixed points and cycles.\\nThe number of cycles increases rapidly with larger digit lengths, and all but a small handful of these cycles are of length three. For example, for 20-digit numbers in base 10, there are fourteen constants (cycles of length one) and ninety-six cycles of length greater than one, all but two of which are of length three. Odd digit lengths produce fewer different end results than even digit lengths.\\n\\nProgramming example\\nThe example below implements the Kaprekar mapping described in the definition above to search for Kaprekar's constants and cycles in Python.\\n\\nLeading zeroes discarded\\nLeading zeroes retained\\nSee also\\nArithmetic dynamics\\nDudeney number\\nFactorion\\nHappy number\\nKaprekar number\\nMeertens number\\nNarcissistic number\\nPerfect digit-to-digit invariant\\nPerfect digital invariant\\nSum-product number\\nSorting algorithm\\n\\nCitations\\nReferences\\nExternal links\\n\\nBowley, Roger. \\\"6174 is Kaprekar's Constant\\\". Numberphile. University of Nottingham: Brady Haran. Archived from the original on 2017-08-23. Retrieved 2013-04-01.\\nWorking link to YouTube\\nSample (Perl) code to walk any four-digit number to Kaprekar's Constant\"}, {\"Kirkpatrick\\u2013Reisch sort\": \"Kirkpatrick\\u2013Reisch sorting is a fast sorting algorithm for items with limited-size integer keys. It is notable for having an asymptotic time complexity that is better than radix sort.\\n\\n\\n== References ==\"}, {\"Library sort\": \"Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions. The name comes from an analogy:\\n\\nSuppose a librarian were to store their books alphabetically on a long shelf, starting with the As at the left end, and continuing to the right along the shelf with no spaces between the books until the end of the Zs. If the librarian acquired a new book that belongs to the B section, once they find the correct space in the B section, they will have to move every book over, from the middle of the Bs all the way down to the Zs in order to make room for the new book. This is an insertion sort. However, if they were to leave a space after every letter, as long as there was still space after B, they would only have to move a few books to make room for the new one. This is the basic principle of the Library Sort.\\nThe algorithm was proposed by Michael A. Bender, Mart\\u00edn Farach-Colton, and Miguel Mosteiro in 2004 and was published in 2006.Like the insertion sort it is based on, library sort is a comparison sort; however, it was shown to have a high probability of running in O(n log n) time (comparable to quicksort), rather than an insertion sort's O(n2). There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.\\nCompared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. The amount and distribution of that space would be implementation dependent. In the paper the size of the needed array is (1 + \\u03b5)n, but with no further recommendations on how to choose \\u03b5. Moreover, it is neither adaptive nor stable. In order to warrant the with-high-probability time bounds, it requires to randomly permute the input, what changes the relative order of equal elements and shuffles any presorted input. Also, the algorithm uses binary search to find the insertion point for each element, which does not take profit of presorted input.\\nAnother drawback is that it cannot be run as an online algorithm, because it is not possible to randomly shuffle the input. If used without this shuffling, it could easily degenerate into quadratic behaviour.\\nOne weakness of insertion sort is that it may require a high number of swap operations and be costly if memory write is expensive. Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, but is also adding an extra cost in the rebalancing step. In addition, locality of reference will be poor compared to mergesort as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets.\\n\\nImplementation\\nAlgorithm\\nLet us say we have an array of n elements. We choose the gap we intend to give. Then we would have a final array of size (1 + \\u03b5)n. The algorithm works in log n rounds. In each round we insert as many elements as there are in the final array already, before re-balancing the array. For finding the position of inserting, we apply Binary Search in the final array and then swap the following elements till we hit an empty space. Once the round is over, we re-balance the final array by inserting spaces between each element.\\nFollowing are three important steps of the algorithm:\\n\\nBinary Search: Finding the position of insertion by applying binary search within the already inserted elements. This can be done by linearly moving towards left or right side of the array if you hit an empty space in the middle element.\\nInsertion: Inserting the element in the position found and swapping the following elements by 1 position till an empty space is hit. This is done in logarithmic time, with high probability.\\nRe-Balancing: Inserting spaces between each pair of elements in the array. The cost of rebalancing is linear in the number of elements already inserted. As these lengths increase with the powers of 2 for each round, the total cost of rebalancing is also linear.\\n\\nPseudocode\\nprocedure rebalance(A, begin, end) is\\n    r \\u2190 end\\n    w \\u2190 end \\u00f7 2\\n\\n    while r \\u2265 begin do\\n        A[w+1] \\u2190 gap\\n        A[w] \\u2190 A[r]\\n        r \\u2190 r \\u2212 1\\n        w \\u2190 w \\u2212 2\\n\\nprocedure sort(A) is\\n    n \\u2190 length(A)\\n    S \\u2190 new array of n gaps\\n\\n    for i \\u2190 1 to floor(log2(n) + 1) do\\n        for j \\u2190 2^i to 2^(i + 1) do\\n            ins \\u2190 binarysearch(A[j], S, 2^(i \\u2212 1))\\n            insert A[j] at S[ins]\\n\\nHere, binarysearch(el, A, k) performs binary search in the first k elements of A, skipping over gaps, to find a place where to locate element el. Insertion should favor gaps over filled-in elements.\\n\\nReferences\\nExternal links\\nGapped Insertion Sort\"}, {\"Median cut\": \"Median cut is an algorithm to sort data of an arbitrary number of dimensions into series of sets by recursively cutting each set of data at the median point along the longest dimension. Median cut is typically used for color quantization.  For example, to reduce a 64k-colour image to 256 colours, median cut is used to find 256 colours that match the original data well.\\n\\nImplementation of color quantization\\nSuppose we have an image with an arbitrary number of pixels and want to generate a palette of 16 colors. Put all the pixels of the image (that is, their RGB values) in a bucket. Find out which color channel (red, green, or blue) among the pixels in the bucket has the greatest range, then sort the pixels according to that channel's values. For example, if the blue channel has the greatest range, then a pixel with an RGB value of (32, 8, 16) is less than a pixel with an RGB value of (1, 2, 24), because 16 < 24. After the bucket has been sorted, move the upper half of the pixels into a new bucket. (It is this step that gives the median cut algorithm its name; the buckets are divided into two at the median of the list of pixels.) This process can be repeated to further subdivide the set of pixels: choose a bucket to divide (e.g., the bucket with the greatest range in any color channel) and divide it into two. After the desired number of buckets have been produced, average the pixels in each bucket to get the final color palette.\\n\\nSee also\\nk-d tree\\n\\nReferences\\nExternal links\\nImage quantization\\nMedian cut + variations\\nImage::Pngslimmer Perl module at CPAN\"}, {\"Merge algorithm\": \"Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, containing all the elements of the inputs lists in sorted order. These algorithms are used as subroutines in various sorting algorithms, most famously merge sort.\\n\\nApplication\\nThe merge algorithm plays a critical role in the merge sort algorithm, a comparison-based sorting algorithm. Conceptually, the merge sort algorithm consists of two steps:\\n\\nRecursively divide the list into sublists of (roughly) equal length, until each sublist contains only one element, or in the case of iterative (bottom up) merge sort, consider a list of n elements as n sub-lists of size 1. A list containing a single element is, by definition, sorted.\\nRepeatedly merge sublists to create a new sorted sublist until the single list contains all elements. The single list is the sorted list.The merge algorithm is used repeatedly in the merge sort algorithm.\\nAn example merge sort is given in the illustration. It starts with an unsorted array of 7 integers. The array is divided into 7 partitions; each partition contains 1 element and is sorted. The sorted partitions are then merged to produce larger, sorted, partitions, until 1 partition, the sorted array, is left.\\n\\nMerging two lists\\nMerging two sorted lists into one can be done in linear time and linear or constant space (depending on the data access model). The following pseudocode demonstrates an algorithm that merges input lists (either linked lists or arrays) A and B into a new list C.:\\u200a104\\u200a The function head yields the first element of a list; \\\"dropping\\\" an element means removing it from its list, typically by incrementing a pointer or index.\\n\\nalgorithm merge(A, B) is\\n    inputs A, B : list\\n    returns list\\n\\n    C := new empty list\\n    while A is not empty and B is not empty do\\n        if head(A) \\u2264 head(B) then\\n            append head(A) to C\\n            drop the head of A\\n        else\\n            append head(B) to C\\n            drop the head of B\\n\\n    // By now, either A or B is empty. It remains to empty the other input list.\\n    while A is not empty do\\n        append head(A) to C\\n        drop the head of A\\n    while B is not empty do\\n        append head(B) to C\\n        drop the head of B\\n\\n    return C\\n\\nWhen the inputs are linked lists, this algorithm can be implemented to use only a constant amount of working space; the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list.\\nIn the merge sort algorithm, this subroutine is typically used to merge two sub-arrays A[lo..mid], A[mid+1..hi] of a single array A. This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above. The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. Various in-place merge algorithms have been devised, sometimes sacrificing the linear-time bound to produce an O(n log n) algorithm; see Merge sort \\u00a7 Variants for discussion.\\n\\nK-way merging\\nk-way merging generalizes binary merging to an arbitrary number k of sorted input lists. Applications of k-way merging arise in various sorting algorithms, including patience sorting and an external sorting algorithm that divides its input into k = 1/M \\u2212 1 blocks that fit in memory, sorts these one by one, then merges these blocks.:\\u200a119\\u2013120\\u200aSeveral solutions to this problem exist. A naive solution is to do a loop over the k lists to pick off the minimum element each time, and repeat this loop until all lists are empty:\\n\\nIn the worst case, this algorithm performs (k\\u22121)(n\\u2212k/2) element comparisons to perform its work if there are a total of n elements in the lists.\\nIt can be improved by storing the lists in a priority queue (min-heap) keyed by their first element:\\n\\nSearching for the next smallest element to be output (find-min) and restoring heap order can now be done in O(log k) time (more specifically, 2\\u230alog k\\u230b comparisons), and the full problem can be solved in O(n log k) time (approximately 2n\\u230alog k\\u230b comparisons).:\\u200a119\\u2013120\\u200aA third algorithm for the problem is a divide and conquer solution that builds on the binary merge algorithm:\\n\\nWhen the input lists to this algorithm are ordered by length, shortest first, it requires fewer than n\\u2308log k\\u2309 comparisons, i.e., less than half the number used by the heap-based algorithm; in practice, it may be about as fast or slow as the heap-based algorithm.\\n\\nParallel merge\\nA parallel version of the binary merge algorithm can serve as a building block of a parallel merge sort. The following pseudocode demonstrates this algorithm in a parallel divide-and-conquer style (adapted from Cormen et al.:\\u200a800\\u200a). It operates on two sorted arrays A and B and writes the sorted output to array C. The notation A[i...j] denotes the part of A from index i through j, exclusive.\\n\\nalgorithm merge(A[i...j], B[k...\\u2113], C[p...q]) is\\n    inputs A, B, C : array\\n           i, j, k, \\u2113, p, q : indices\\n\\n    let m = j - i,\\n        n = \\u2113 - k\\n\\n    if m < n then\\n        swap A and B  // ensure that A is the larger array: i, j still belong to A; k, \\u2113 to B\\n        swap m and n\\n\\n    if m \\u2264 0 then\\n        return  // base case, nothing to merge\\n\\n    let r = \\u230a(i + j)/2\\u230b\\n    let s = binary-search(A[r], B[k...\\u2113])\\n    let t = p + (r - i) + (s - k)\\n    C[t] = A[r]\\n\\n    in parallel do\\n        merge(A[i...r], B[k...s], C[p...t])\\n        merge(A[r+1...j], B[s...\\u2113], C[t+1...q])\\n\\nThe algorithm operates by splitting either A or B, whichever is larger, into (nearly) equal halves. It then splits the other array into a part with values smaller than the midpoint of the first, and a part with larger or equal values. (The binary search subroutine returns the index in B where A[r] would be, if it were in B; that this always a number between k and \\u2113.) Finally, each pair of halves is merged recursively, and since the recursive calls are independent of each other, they can be done in parallel. Hybrid approach, where serial algorithm is used for recursion base case has been shown to perform well in practice The work performed by the algorithm for two arrays holding a total of n elements, i.e., the running time of a serial version of it, is O(n). This is optimal since n elements need to be copied into C. To calculate the span of the algorithm, it is necessary to derive a Recurrence relation. Since the two recursive calls of merge are in parallel, only the costlier of the two calls needs to be considered. In the worst case, the maximum number of elements in one of the recursive calls is at most \\n  \\n    \\n      \\n        \\n          \\n            3\\n            4\\n          \\n        \\n        n\\n      \\n    \\n    {\\\\textstyle {\\\\frac {3}{4}}n}\\n   since the array with more elements is perfectly split in half. Adding the \\n  \\n    \\n      \\n        \\u0398\\n        \\n          (\\n          \\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Theta \\\\left(\\\\log(n)\\\\right)}\\n   cost of the Binary Search, we obtain this recurrence as an upper bound:\\n\\n  \\n    \\n      \\n        \\n          T\\n          \\n            \\u221e\\n          \\n          \\n            merge\\n          \\n        \\n        (\\n        n\\n        )\\n        =\\n        \\n          T\\n          \\n            \\u221e\\n          \\n          \\n            merge\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                3\\n                4\\n              \\n            \\n            n\\n          \\n          )\\n        \\n        +\\n        \\u0398\\n        \\n          (\\n          \\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle T_{\\\\infty }^{\\\\text{merge}}(n)=T_{\\\\infty }^{\\\\text{merge}}\\\\left({\\\\frac {3}{4}}n\\\\right)+\\\\Theta \\\\left(\\\\log(n)\\\\right)}\\n  \\nThe solution is \\n  \\n    \\n      \\n        \\n          T\\n          \\n            \\u221e\\n          \\n          \\n            merge\\n          \\n        \\n        (\\n        n\\n        )\\n        =\\n        \\u0398\\n        \\n          (\\n          \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              )\\n              \\n                2\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle T_{\\\\infty }^{\\\\text{merge}}(n)=\\\\Theta \\\\left(\\\\log(n)^{2}\\\\right)}\\n  , meaning that it takes that much time on an ideal machine with an unbounded number of processors.:\\u200a801\\u2013802\\u200aNote: The routine is not stable: if equal items are separated by splitting A and B, they will become interleaved in C; also swapping A and B will destroy the order, if equal items are spread among both input arrays. As a result, when used for sorting, this algorithm produces a sort that is not stable.\\n\\nParallel merge of two lists\\nThere are also algorithms that introduce parallelism within a single instance of merging of two sorted lists. These can be used in field-programmable gate arrays (FPGAs), specialized sorting circuits, as well as in modern processors with single-instruction multiple-data (SIMD) instructions. \\nExisting parallel algorithms are based on modifications of the merge part of either the bitonic sorter or odd-even mergesort. In 2018, Saitoh M. et al. introduced MMS  for FPGAs, which focused on removing a multi-cycle feedback datapath that prevented efficient pipelining in hardware. Also in 2018, Papaphilippou P. et al. introduced FLiMS  that improved the hardware utilization and performance by only requiring \\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        P\\n        )\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}(P)+1}\\n   pipeline stages of P/2 compare-and-swap units to merge with a parallelism of P elements per FPGA cycle.\\n\\nLanguage support\\nSome computer languages provide built-in or library support for merging sorted collections.\\n\\nC++\\nThe  C++'s Standard Template Library has the function std::merge, which merges two sorted ranges of iterators, and std::inplace_merge, which merges two consecutive sorted ranges in-place. In addition, the std::list (linked list) class has its own merge method which merges another list into itself. The type of the elements merged must support the less-than (<) operator, or it must be provided with a custom comparator.\\nC++17 allows for differing execution policies, namely sequential, parallel, and parallel-unsequenced.\\n\\nPython\\nPython's standard library (since 2.6) also has a merge function in the heapq module, that takes multiple sorted iterables, and merges them into a single iterator.\\n\\nSee also\\nMerge (revision control)\\nJoin (relational algebra)\\nJoin (SQL)\\nJoin (Unix)\\n\\nReferences\\nFurther reading\\nDonald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Pages 158\\u2013160 of section 5.2.4: Sorting by Merging. Section 5.3.2: Minimum-Comparison Merging, pp. 197\\u2013207.\\n\\nExternal links\\nHigh Performance Implementation of Parallel and Serial Merge in C# with source in GitHub and in C++ GitHub\"}, {\"Merge sort\": \"In computer science, merge sort (also commonly spelled as mergesort) is an efficient, general-purpose, and comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide-and-conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up merge sort appeared in a report by Goldstine and von Neumann as early as 1948.\\n\\nAlgorithm\\nConceptually, a merge sort works as follows:\\n\\nDivide the unsorted list into n sublists, each containing one element (a list of one element is considered sorted).\\nRepeatedly merge sublists to produce new sorted sublists until there is only one sublist remaining.  This will be the sorted list.\\n\\nTop-down implementation\\nExample C-like code using indices for top-down merge sort algorithm that recursively splits the list (called runs in this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step is avoided with alternating the direction of the merge with each level of recursion (except for an initial one-time copy, that can be avoided too). To help understand this, consider an array with two elements. The elements are copied to B[], then merged back to A[]. If there are four elements, when the bottom of the recursion level is reached, single element runs from A[] are merged to B[], and then at the next higher level of recursion, those two-element runs are merged to A[]. This pattern continues with each level of recursion.\\n\\nSorting the entire array is accomplished by TopDownMergeSort(A, B, length(A)).\\n\\nBottom-up implementation\\nExample C-like code using indices for bottom-up merge sort algorithm which treats the list as an array of n sublists (called runs in this example) of size 1, and iteratively merges sub-lists back and forth between two buffers:\\n\\nTop-down implementation using lists\\nPseudocode for top-down merge sort algorithm which recursively divides the input list into smaller sublists until the sublists are trivially sorted, and then merges the sublists while returning up the call chain.\\n\\nfunction merge_sort(list m) is\\n    // Base case. A list of zero or one elements is sorted, by definition.\\n    if length of m \\u2264 1 then\\n        return m\\n\\n    // Recursive case. First, divide the list into equal-sized sublists\\n    // consisting of the first half and second half of the list.\\n    // This assumes lists start at index 0.\\n    var left := empty list\\n    var right := empty list\\n    for each x with index i in m do\\n        if i < (length of m)/2 then\\n            add x to left\\n        else\\n            add x to right\\n\\n    // Recursively sort both sublists.\\n    left := merge_sort(left)\\n    right := merge_sort(right)\\n\\n    // Then merge the now-sorted sublists.\\n    return merge(left, right)\\n\\nIn this example, the merge function merges the left and right sublists.\\n\\nfunction merge(left, right) is\\n    var result := empty list\\n\\n    while left is not empty and right is not empty do\\n        if first(left) \\u2264 first(right) then\\n            append first(left) to result\\n            left := rest(left)\\n        else\\n            append first(right) to result\\n            right := rest(right)\\n\\n    // Either left or right may have elements left; consume them.\\n    // (Only one of the following loops will actually be entered.)\\n    while left is not empty do\\n        append first(left) to result\\n        left := rest(left)\\n    while right is not empty do\\n        append first(right) to result\\n        right := rest(right)\\n    return result\\n\\nBottom-up implementation using lists\\nPseudocode for bottom-up merge sort algorithm which uses a small fixed size array of references to nodes, where array[i] is either a reference to a list of size 2i or nil. node is a reference or pointer to a node. The merge() function would be similar to the one shown in the top-down merge lists example, it merges two already sorted lists, and handles empty lists. In this case, merge() would use node for its input parameters and return value.\\n\\nfunction merge_sort(node head) is\\n    // return if empty list\\n    if head = nil then\\n        return nil\\n    var node array[32]; initially all nil\\n    var node result\\n    var node next\\n    var int  i\\n    result := head\\n    // merge nodes into array\\n    while result \\u2260 nil do\\n        next := result.next;\\n        result.next := nil\\n        for (i = 0; (i < 32) && (array[i] \\u2260 nil); i += 1) do\\n            result := merge(array[i], result)\\n            array[i] := nil\\n        // do not go past end of array\\n        if i = 32 then\\n            i -= 1\\n        array[i] := result\\n        result := next\\n    // merge array into single list\\n    result := nil\\n    for (i = 0; i < 32; i += 1) do\\n        result := merge(array[i], result)\\n    return result\\n\\nAnalysis\\nIn sorting n objects, merge sort has an average and worst-case performance of O(n log n). If the running time of merge sort for a list of length n is T(n), then the recurrence relation T(n) = 2T(n/2) + n follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the n steps taken to merge the resulting two lists). The closed form follows from the master theorem for divide-and-conquer recurrences.\\nThe number of comparisons made by merge sort in the worst case is given by the sorting numbers. These numbers are equal to or slightly smaller than (n \\u2308lg n\\u2309 \\u2212 2\\u2308lg n\\u2309 + 1), which is between (n lg n \\u2212 n + 1) and (n lg n + n + O(lg n)). Merge sort's best case takes about half as many iterations as its worst case.For large n and a randomly ordered input list, merge sort's expected (average) number of comparisons approaches \\u03b1\\u00b7n fewer than the worst case, where \\n  \\n    \\n      \\n        \\u03b1\\n        =\\n        \\u2212\\n        1\\n        +\\n        \\n          \\u2211\\n          \\n            k\\n            =\\n            0\\n          \\n          \\n            \\u221e\\n          \\n        \\n        \\n          \\n            1\\n            \\n              \\n                2\\n                \\n                  k\\n                \\n              \\n              +\\n              1\\n            \\n          \\n        \\n        \\u2248\\n        0.2645.\\n      \\n    \\n    {\\\\displaystyle \\\\alpha =-1+\\\\sum _{k=0}^{\\\\infty }{\\\\frac {1}{2^{k}+1}}\\\\approx 0.2645.}\\n  \\nIn the worst case, merge sort uses approximately 39% fewer comparisons than quicksort does in its average case, and in terms of moves, merge sort's worst case complexity is O(n log n) - the same complexity as quicksort's best case.Merge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially, and is thus popular in languages such as Lisp, where sequentially accessed data structures are very common. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort.\\nMerge sort's most common implementation does not sort in place; therefore, the memory size of the input must be allocated for the sorted output to be stored in (see below for variations that need only n/2 extra spaces).\\n\\nNatural merge sort\\nA natural merge sort is similar to a bottom-up merge sort except that any naturally occurring runs (sorted sequences) in the input are exploited. Both monotonic and bitonic (alternating up/down) runs may be exploited, with lists (or equivalently tapes or files) being convenient data structures (used as FIFO queues or LIFO stacks). In the bottom-up merge sort, the starting point assumes each run is one item long. In practice, random input data will have many short runs that just happen to be sorted. In the typical case, the natural merge sort may not need as many passes because there are fewer runs to merge. In the best case, the input is already sorted (i.e., is one run), so the natural merge sort need only make one pass through the data. In many practical cases, long natural runs are present, and for that reason natural merge sort is exploited as the key component of Timsort. Example:\\n\\nStart       :  3  4  2  1  7  5  8  9  0  6\\nSelect runs : (3  4)(2)(1  7)(5  8  9)(0  6)\\nMerge       : (2  3  4)(1  5  7  8  9)(0  6)\\nMerge       : (1  2  3  4  5  7  8  9)(0  6)\\nMerge       : (0  1  2  3  4  5  6  7  8  9)\\n\\nFormally, the natural merge sort is said to be Runs-optimal, where \\n  \\n    \\n      \\n        \\n          \\n            R\\n            u\\n            n\\n            s\\n          \\n        \\n        (\\n        L\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {Runs}}(L)}\\n   is the number of runs in \\n  \\n    \\n      \\n        L\\n      \\n    \\n    {\\\\displaystyle L}\\n  , minus one.\\nTournament replacement selection sorts are used to gather the initial runs for external sorting algorithms.\\n\\nPing-pong merge sort\\nInstead of merging two blocks at a time, a ping-pong merge merges four blocks at a time. The four sorted blocks are merged simultaneously to auxiliary space into two sorted blocks, then the two sorted blocks are merged back to main memory. Doing so omits the copy operation and reduces the total number of moves by half. An early public domain implementation of a four-at-once merge was by WikiSort in 2014, the method was later that year described as an optimization for patience sorting and named a ping-pong merge. Quadsort implemented the method in 2020 and named it a quad merge.\\n\\nIn-place merge sort\\nOne drawback of merge sort, when implemented on arrays, is its O(n) working memory requirement. Several methods to reduce memory or make merge sort fully in-place have been suggested:\\n\\nKronrod (1969) suggested an alternative version of merge sort that uses constant additional space.\\nKatajainen et al. present an algorithm that requires a constant amount of working memory: enough storage space to hold one element of the input array, and additional space to hold O(1) pointers into the input array. They achieve an O(n log n) time bound with small constants, but their algorithm is not stable.\\nSeveral attempts have been made at producing an in-place merge algorithm that can be combined with a standard (top-down or bottom-up) merge sort to produce an in-place merge sort. In this case, the notion of \\\"in-place\\\" can be relaxed to mean \\\"taking logarithmic stack space\\\", because standard merge sort requires that amount of space for its own stack usage. It was shown by Geffert et al. that in-place, stable merging is possible in O(n log n) time using a constant amount of scratch space, but their algorithm is complicated and has high constant factors: merging arrays of length n and m can take 5n + 12m + o(m) moves. This high constant factor and complicated in-place algorithm was made simpler and easier to understand. Bing-Chao Huang and Michael A. Langston presented a straightforward linear time algorithm practical in-place merge to merge a sorted list using fixed amount of additional space. They both have used the work of Kronrod and others. It merges in linear time and constant extra space. The algorithm takes little more average time than standard merge sort algorithms, free to exploit O(n) temporary extra memory cells, by less than a factor of two. Though the algorithm is much faster in a practical way but it is unstable also for some lists. But using similar concepts, they have been able to solve this problem. Other in-place algorithms include SymMerge, which takes O((n + m) log (n + m)) time in total and is stable. Plugging such an algorithm into merge sort increases its complexity to the non-linearithmic, but still quasilinear, O(n (log n)2).\\nMany applications of external sorting use a form of merge sorting where the input get split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of pages fit into main memory.\\nA modern stable linear and in-place merge variant is block merge sort which creates a section of unique values to use as swap space.\\nThe space overhead can be reduced to sqrt(n) by using binary searches and rotations. This method is employed by the C++ STL library and quadsort.\\nAn alternative to reduce the copying into multiple lists is to associate a new field of information with each key (the elements in m are called keys). This field will be used to link the keys and any associated information together in a sorted list (a key and its related information is called a record). Then the merging of the sorted lists proceeds by changing the link values; no records need to be moved at all. A field which contains only a link will generally be smaller than an entire record so less space will also be used. This is a standard sorting technique, not restricted to merge sort.\\nA simple way to reduce the space overhead to n/2 is to maintain left and right as a combined structure, copy only the left part of m into temporary space, and to direct the merge routine to place the merged output into m. With this version it is better to allocate the temporary space outside the merge routine, so that only one allocation is needed. The excessive copying mentioned previously is also mitigated, since the last pair of lines before the return result statement (function  merge in the pseudo code above) become superfluous.\\n\\nUse with tape drives\\nAn external merge sort is practical to run using disk or tape drives when the data to be sorted is too large to fit into memory. External sorting explains how merge sort is implemented with disk drives. A typical tape drive sort uses four tape drives. All I/O is sequential (except for rewinds at the end of each pass). A minimal implementation can get by with just two record buffers and a few program variables.\\nNaming the four tape drives as A, B, C, D, with the original data on A, and using only two record buffers, the algorithm is similar to the bottom-up implementation, using pairs of tape drives instead of arrays in memory. The basic algorithm can be described as follows:\\n\\nMerge pairs of records from A; writing two-record sublists alternately to C and D.\\nMerge two-record sublists from C and D into four-record sublists; writing these alternately to A and B.\\nMerge four-record sublists from A and B into eight-record sublists; writing these alternately to C and D\\nRepeat until you have one list containing all the data, sorted\\u2014in log2(n) passes.Instead of starting with very short runs, usually a hybrid algorithm is used, where the initial pass will read many records into memory, do an internal sort to create a long run, and then distribute those long runs onto the output set. The step avoids many early passes. For example, an internal sort of 1024 records will save nine passes. The internal sort is often large because it has such a benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory. One of them, the Knuth's 'snowplow' (based on a binary min-heap), generates runs twice as long (on average) as a size of memory used.With some overhead, the above algorithm can be modified to use three tapes. O(n log n) running time can also be achieved using two queues, or a stack and a queue, or three stacks. In the other direction, using k > two tapes (and O(k) items in memory), we can reduce the number of tape operations in O(log k) times by using a k/2-way merge.\\nA more sophisticated merge sort that optimizes tape (and disk) drive usage is the polyphase merge sort.\\n\\nOptimizing merge sort\\nOn modern computers, locality of reference can be of paramount importance in software optimization, because multilevel memory hierarchies are used. Cache-aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the tiled merge sort algorithm stops partitioning subarrays when subarrays of size S are reached, where S is the number of data items fitting into a CPU's cache. Each of these subarrays is sorted with an in-place sorting algorithm such as insertion sort, to discourage memory swaps, and normal merge sort is then completed in the standard recursive fashion. This algorithm has demonstrated better performance on machines that benefit from cache optimization. (LaMarca & Ladner 1997)\\n\\nParallel merge Sort\\nMerge sort parallelizes well due to the use of the divide-and-conquer method. Several different parallel variants of the algorithm have been developed over the years. Some parallel merge sort algorithms are strongly related to the sequential top-down merge algorithm while others have a different general structure and use the K-way merge method.\\n\\nMerge sort with parallel recursion\\nThe sequential merge sort procedure can be described in two phases, the divide phase and the merge phase. The first consists of many recursive calls that repeatedly perform the same division process until the subsequences are trivially sorted (containing one or no element). An intuitive approach is the parallelization of those recursive calls. Following pseudocode describes the merge sort with parallel recursion using the fork and join keywords:\\n\\n// Sort elements lo through hi (exclusive) of array A.\\nalgorithm mergesort(A, lo, hi) is\\n    if lo+1 < hi then  // Two or more elements.\\n        mid := \\u230a(lo + hi) / 2\\u230b\\n        fork mergesort(A, lo, mid)\\n        mergesort(A, mid, hi)\\n        join\\n        merge(A, lo, mid, hi)\\n\\nThis algorithm is the trivial modification of the sequential version and does not parallelize well. Therefore, its speedup is not very impressive. It has a span of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n)}\\n  , which is only an improvement of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (\\\\log n)}\\n   compared to the sequential version (see Introduction to Algorithms). This is mainly due to the sequential merge method, as it is the bottleneck of the parallel executions.\\n\\nMerge sort with parallel merging\\nBetter parallelism can be achieved by using a parallel merge algorithm. Cormen et al. present a binary variant that merges two sorted sub-sequences into one sorted output sequence.In one of the sequences (the longer one if unequal length), the element of the middle index is selected. Its position in the other sequence is determined in such a way that this sequence would remain sorted if this element were inserted at this position. Thus, one knows how many other elements from both sequences are smaller and the position of the selected element in the output sequence can be calculated. For the partial sequences of the smaller and larger elements created in this way, the merge algorithm is again executed in parallel until the base case of the recursion is reached.\\nThe following pseudocode shows the modified parallel merge sort method using the parallel merge algorithm (adopted from Cormen et al.).\\n\\n/**\\n * A: Input array\\n * B: Output array\\n * lo: lower bound\\n * hi: upper bound\\n * off: offset\\n */\\nalgorithm parallelMergesort(A, lo, hi, B, off) is\\n    len := hi - lo + 1\\n    if len == 1 then\\n        B[off] := A[lo]\\n    else let T[1..len] be a new array\\n        mid := \\u230a(lo + hi) / 2\\u230b \\n        mid' := mid - lo + 1\\n        fork parallelMergesort(A, lo, mid, T, 1)\\n        parallelMergesort(A, mid + 1, hi, T, mid' + 1) \\n        join \\n        parallelMerge(T, 1, mid', mid' + 1, len, B, off)\\n\\nIn order to analyze a recurrence relation for the worst case span, the recursive calls of parallelMergesort have to be incorporated only once due to their parallel execution, obtaining\\n\\nFor detailed information about the complexity of the parallel merge procedure, see Merge algorithm.\\nThe solution of this recurrence is given by\\n\\nThis parallel merge algorithm reaches a parallelism of \\n  \\n    \\n      \\n        \\u0398\\n        \\n          (\\n          \\n            \\n              n\\n              \\n                (\\n                log\\n                \\u2061\\n                n\\n                \\n                  )\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\textstyle \\\\Theta \\\\left({\\\\frac {n}{(\\\\log n)^{2}}}\\\\right)}\\n  , which is much higher than the parallelism of the previous algorithm. Such a sort can perform well in practice when combined with a fast stable sequential sort, such as insertion sort, and a fast sequential merge as a base case for merging small arrays.\\n\\nParallel multiway merge sort\\nIt seems arbitrary to restrict the merge sort algorithms to a binary merge method, since there are usually p > 2 processors available. A better approach may be to use a K-way merge method, a generalization of binary merge, in which \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   sorted sequences are merged. This merge variant is well suited to describe a sorting algorithm on a PRAM.\\n\\nBasic Idea\\nGiven an unsorted sequence of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements, the goal is to sort the sequence with \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   available processors. These elements are distributed equally among all processors and sorted locally using a sequential Sorting algorithm. Hence, the sequence consists of sorted sequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{1},...,S_{p}}\\n   of length \\n  \\n    \\n      \\n        \\u2308\\n        \\n          \\n            n\\n            p\\n          \\n        \\n        \\u2309\\n      \\n    \\n    {\\\\textstyle \\\\lceil {\\\\frac {n}{p}}\\\\rceil }\\n  . For simplification let \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   be a multiple of \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  , so that \\n  \\n    \\n      \\n        \\n          |\\n          \\n            S\\n            \\n              i\\n            \\n          \\n          |\\n        \\n        =\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle \\\\left\\\\vert S_{i}\\\\right\\\\vert ={\\\\frac {n}{p}}}\\n   for \\n  \\n    \\n      \\n        i\\n        =\\n        1\\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        p\\n      \\n    \\n    {\\\\displaystyle i=1,...,p}\\n  .\\nThese sequences will be used to perform a multisequence selection/splitter selection. For \\n  \\n    \\n      \\n        j\\n        =\\n        1\\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        p\\n      \\n    \\n    {\\\\displaystyle j=1,...,p}\\n  , the algorithm determines splitter elements \\n  \\n    \\n      \\n        \\n          v\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{j}}\\n   with global rank \\n  \\n    \\n      \\n        k\\n        =\\n        j\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle k=j{\\\\frac {n}{p}}}\\n  . Then the corresponding positions of \\n  \\n    \\n      \\n        \\n          v\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          v\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{1},...,v_{p}}\\n   in each sequence \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   are determined with binary search and thus the \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   are further partitioned into \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   subsequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n            ,\\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            i\\n            ,\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i,1},...,S_{i,p}}\\n   with \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n            ,\\n            j\\n          \\n        \\n        :=\\n        {\\n        x\\n        \\u2208\\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        r\\n        a\\n        n\\n        k\\n        (\\n        \\n          v\\n          \\n            j\\n            \\u2212\\n            1\\n          \\n        \\n        )\\n        <\\n        r\\n        a\\n        n\\n        k\\n        (\\n        x\\n        )\\n        \\u2264\\n        r\\n        a\\n        n\\n        k\\n        (\\n        \\n          v\\n          \\n            j\\n          \\n        \\n        )\\n        }\\n      \\n    \\n    {\\\\textstyle S_{i,j}:=\\\\{x\\\\in S_{i}|rank(v_{j-1})<rank(x)\\\\leq rank(v_{j})\\\\}}\\n  .\\nFurthermore, the elements of \\n  \\n    \\n      \\n        \\n          S\\n          \\n            1\\n            ,\\n            i\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            p\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{1,i},...,S_{p,i}}\\n   are assigned to processor \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , means all elements between rank \\n  \\n    \\n      \\n        (\\n        i\\n        \\u2212\\n        1\\n        )\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle (i-1){\\\\frac {n}{p}}}\\n   and rank \\n  \\n    \\n      \\n        i\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle i{\\\\frac {n}{p}}}\\n  , which are distributed over all \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n  . Thus, each processor receives a sequence of sorted sequences. The fact that the rank \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   of the splitter elements \\n  \\n    \\n      \\n        \\n          v\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{i}}\\n   was chosen globally, provides two important properties: On the one hand, \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   was chosen so that each processor can still operate on \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\textstyle n/p}\\n   elements after assignment. The algorithm is perfectly load-balanced. On the other hand, all elements on processor \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   are less than or equal to all elements on processor \\n  \\n    \\n      \\n        i\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle i+1}\\n  . Hence, each processor performs the p-way merge locally and thus obtains a sorted sequence from its sub-sequences. Because of the second property, no further p-way-merge has to be performed, the results only have to be put together in the order of the processor number.\\n\\nMulti-sequence selection\\nIn its simplest form, given \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   sorted sequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{1},...,S_{p}}\\n   distributed evenly on \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   processors and a rank \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  , the task is to find an element \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   with a global rank \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   in the union of the sequences. Hence, this can be used to divide each \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   in two parts at a splitter index \\n  \\n    \\n      \\n        \\n          l\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle l_{i}}\\n  , where the lower part contains only elements which are smaller than \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  , while the elements bigger than \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   are located in the upper part.\\nThe presented sequential algorithm returns the indices of the splits in each sequence, e.g. the indices \\n  \\n    \\n      \\n        \\n          l\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle l_{i}}\\n   in sequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   such that \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        [\\n        \\n          l\\n          \\n            i\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle S_{i}[l_{i}]}\\n   has a global rank less than \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   and \\n  \\n    \\n      \\n        \\n          r\\n          a\\n          n\\n          k\\n        \\n        \\n          (\\n          \\n            \\n              S\\n              \\n                i\\n              \\n            \\n            [\\n            \\n              l\\n              \\n                i\\n              \\n            \\n            +\\n            1\\n            ]\\n          \\n          )\\n        \\n        \\u2265\\n        k\\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {rank} \\\\left(S_{i}[l_{i}+1]\\\\right)\\\\geq k}\\n  .\\nalgorithm msSelect(S : Array of sorted Sequences [S_1,..,S_p], k : int) is\\n    for i = 1 to p do \\n\\t(l_i, r_i) = (0, |S_i|-1)\\n\\t\\n    while there exists i: l_i < r_i do\\n\\t// pick Pivot Element in S_j[l_j], .., S_j[r_j], chose random j uniformly\\n\\tv := pickPivot(S, l, r)\\n\\tfor i = 1 to p do \\n\\t    m_i = binarySearch(v, S_i[l_i, r_i]) // sequentially\\n\\tif m_1 + ... + m_p >= k then // m_1+ ... + m_p is the global rank of v\\n\\t    r := m  // vector assignment\\n\\telse\\n\\t    l := m \\n\\t\\n    return l\\n\\nFor the complexity analysis the PRAM model is chosen. If the data is evenly distributed over all \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  , the p-fold execution of the binarySearch method has a running time of \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                n\\n                \\n                  /\\n                \\n                p\\n              \\n              )\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\log \\\\left(n/p\\\\right)\\\\right)}\\n  . The expected recursion depth is \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                \\n                  \\u2211\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  S\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n              \\n              )\\n            \\n          \\n          )\\n        \\n        =\\n        \\n          \\n            O\\n          \\n        \\n        (\\n        log\\n        \\u2061\\n        (\\n        n\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(\\\\log \\\\left(\\\\textstyle \\\\sum _{i}|S_{i}|\\\\right)\\\\right)={\\\\mathcal {O}}(\\\\log(n))}\\n   as in the ordinary Quickselect. Thus the overall expected running time is \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\log(n/p)\\\\log(n)\\\\right)}\\n  .\\nApplied on the parallel multiway merge sort, this algorithm has to be invoked in parallel such that all splitter elements of rank \\n  \\n    \\n      \\n        i\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle i{\\\\frac {n}{p}}}\\n   for \\n  \\n    \\n      \\n        i\\n        =\\n        1\\n        ,\\n        .\\n        .\\n        ,\\n        p\\n      \\n    \\n    {\\\\displaystyle i=1,..,p}\\n   are found simultaneously. These splitter elements can then be used to partition each sequence in \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   parts, with the same total running time of \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\,\\\\log(n/p)\\\\log(n)\\\\right)}\\n  .\\n\\nPseudocode\\nBelow, the complete pseudocode of the parallel multiway merge sort algorithm is given. We assume that there is a barrier synchronization before and after the multisequence selection such that every processor can determine the splitting elements and the sequence partition properly.\\n\\n/**\\n * d: Unsorted Array of Elements\\n * n: Number of Elements\\n * p: Number of Processors\\n * return Sorted Array\\n */\\nalgorithm parallelMultiwayMergesort(d : Array, n : int, p : int) is\\n    o := new Array[0, n]                         // the output array\\n    for i = 1 to p do in parallel                // each processor in parallel\\n        S_i := d[(i-1) * n/p, i * n/p] \\t         // Sequence of length n/p\\n\\tsort(S_i)                                // sort locally\\n        synch\\n\\tv_i := msSelect([S_1,...,S_p], i * n/p)          // element with global rank i * n/p\\n        synch\\n\\t(S_i,1, ..., S_i,p) := sequence_partitioning(si, v_1, ..., v_p) // split s_i into subsequences\\n\\t    \\n\\to[(i-1) * n/p, i * n/p] := kWayMerge(s_1,i, ..., s_p,i)  // merge and assign to output array\\n\\t\\n    return o\\n\\nAnalysis\\nFirstly, each processor sorts the assigned \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\displaystyle n/p}\\n   elements locally using a sorting algorithm with complexity \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            n\\n            \\n              /\\n            \\n            p\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(n/p\\\\;\\\\log(n/p)\\\\right)}\\n  . After that, the splitter elements have to be calculated in time \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\,\\\\log(n/p)\\\\log(n)\\\\right)}\\n  . Finally, each group of \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   splits have to be merged in parallel by each processor with a running time of \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        (\\n        log\\n        \\u2061\\n        (\\n        p\\n        )\\n        \\n        n\\n        \\n          /\\n        \\n        p\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}(\\\\log(p)\\\\;n/p)}\\n   using a sequential p-way merge algorithm. Thus, the overall running time is given by\\n\\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                n\\n                p\\n              \\n            \\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                \\n                  n\\n                  p\\n                \\n              \\n              )\\n            \\n            +\\n            p\\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                \\n                  n\\n                  p\\n                \\n              \\n              )\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n            +\\n            \\n              \\n                n\\n                p\\n              \\n            \\n            log\\n            \\u2061\\n            (\\n            p\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left({\\\\frac {n}{p}}\\\\log \\\\left({\\\\frac {n}{p}}\\\\right)+p\\\\log \\\\left({\\\\frac {n}{p}}\\\\right)\\\\log(n)+{\\\\frac {n}{p}}\\\\log(p)\\\\right)}\\n  .\\n\\nPractical adaption and application\\nThe multiway merge sort algorithm is very scalable through its high parallelization capability, which allows the use of many processors. This makes the algorithm a viable candidate for sorting large amounts of data, such as those processed in computer clusters. Also, since in such systems memory is usually not a limiting resource, the disadvantage of space complexity of merge sort is negligible. However, other factors become important in such systems, which are not taken into account when modelling on a PRAM. Here, the following aspects need to be considered: Memory hierarchy, when the data does not fit into the processors cache, or the communication overhead of exchanging data between processors, which could become a bottleneck when the data can no longer be accessed via the shared memory.\\nSanders et al. have presented in their paper a bulk synchronous parallel algorithm for multilevel multiway mergesort, which divides \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   processors into \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   groups of size \\n  \\n    \\n      \\n        \\n          p\\n          \\u2032\\n        \\n      \\n    \\n    {\\\\displaystyle p'}\\n  . All processors sort locally first. Unlike single level multiway mergesort, these sequences are then partitioned into \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   parts and assigned to the appropriate processor groups. These steps are repeated recursively in those groups. This reduces communication and especially avoids problems with many small messages. The hierarchical structure of the underlying real network can be used to define the processor groups (e.g. racks, clusters,...).\\n\\nFurther variants\\nMerge sort was one of the first sorting algorithms where optimal speed up was achieved, with Richard Cole using a clever subsampling algorithm to ensure O(1) merge. Other sophisticated parallel sorting algorithms can achieve the same or better time bounds with a lower constant. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW parallel random-access machine (PRAM) with n processors by performing partitioning implicitly. Powers further shows that a pipelined version of Batcher's Bitonic Mergesort at O((log n)2) time on a butterfly sorting network is in practice actually faster than his O(log n) sorts on a PRAM, and he provides detailed discussion of the hidden overheads in comparison, radix and parallel sorting.\\n\\nComparison with other sort algorithms\\nAlthough heapsort has the same time bounds as merge sort, it requires only \\u0398(1) auxiliary space instead of merge sort's \\u0398(n). On typical modern architectures, efficient quicksort implementations generally outperform merge sort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only \\u0398(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.\\nAs of Perl 5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl). In Java, the Arrays.sort() methods use merge sort or a tuned quicksort depending on the datatypes and for implementation efficiency switch to insertion sort when fewer than seven array elements are being sorted. The Linux kernel uses merge sort for its linked lists. Python uses Timsort, another tuned hybrid of merge sort and insertion sort, that has become the standard sort algorithm in Java SE 7 (for arrays of non-primitive types), on the Android platform, and in GNU Octave.\\n\\nNotes\\nReferences\\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2009) [1990]. Introduction to Algorithms (3rd ed.). MIT Press and McGraw-Hill. ISBN 0-262-03384-4.\\nKatajainen, Jyrki; Pasanen, Tomi; Teuhola, Jukka (1996). \\\"Practical in-place mergesort\\\". Nordic Journal of Computing. 3 (1): 27\\u201340. CiteSeerX 10.1.1.22.8523. ISSN 1236-6064. Archived from the original on 2011-08-07. Retrieved 2009-04-04.. Also Practical In-Place Mergesort. Also [3]\\nKnuth, Donald (1998). \\\"Section 5.2.4: Sorting by Merging\\\". Sorting and Searching. The Art of Computer Programming. Vol. 3 (2nd ed.). Addison-Wesley. pp. 158\\u2013168. ISBN 0-201-89685-0.\\nKronrod, M. A. (1969). \\\"Optimal ordering algorithm without operational field\\\". Soviet Mathematics - Doklady. 10: 744.\\nLaMarca, A.; Ladner, R. E. (1997). \\\"The influence of caches on the performance of sorting\\\". Proc. 8th Ann. ACM-SIAM Symp. On Discrete Algorithms (SODA97): 370\\u2013379. CiteSeerX 10.1.1.31.1153.\\nSkiena, Steven S. (2008). \\\"4.5: Mergesort: Sorting by Divide-and-Conquer\\\". The Algorithm Design Manual (2nd ed.). Springer. pp. 120\\u2013125. ISBN 978-1-84800-069-8.\\nSun Microsystems. \\\"Arrays API (Java SE 6)\\\". Retrieved 2007-11-19.\\nOracle Corp. \\\"Arrays (Java SE 10 & JDK 10)\\\". Retrieved 2018-07-23.\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Merge Sort at the Wayback Machine (archived 6 March 2015) \\u2013 graphical demonstration\\nOpen Data Structures - Section 11.1.1 - Merge Sort, Pat Morin\"}, {\"Merge-insertion sort\": \"In computer science, merge-insertion sort or the Ford\\u2013Johnson algorithm is a comparison sorting algorithm published in 1959 by L. R. Ford Jr. and Selmer M. Johnson. It uses fewer comparisons in the worst case than the best previously known algorithms, binary insertion sort and merge sort, and for 20 years it was the sorting algorithm with the fewest known comparisons. Although not of practical significance, it remains of theoretical interest in connection with the problem of sorting with a minimum number of comparisons. The same algorithm may have also been independently discovered by Stanis\\u0142aw Trybu\\u0142a and Czen Ping.\\n\\nAlgorithm\\nMerge-insertion sort performs the following steps, on an input \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements:\\nGroup the elements of \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   into \\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   pairs of elements, arbitrarily, leaving one element unpaired if there is an odd number of elements.\\nPerform \\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   comparisons, one per pair, to determine the larger of the two elements in each pair.\\nRecursively sort the \\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   larger elements from each pair, creating a sorted sequence \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   of \\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   of the input elements, in ascending order.\\nInsert at the start of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   the element that was paired with the first and smallest element of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n  .\\nInsert the remaining \\n  \\n    \\n      \\n        \\u2308\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u2309\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\lceil n/2\\\\rceil -1}\\n   elements of \\n  \\n    \\n      \\n        X\\n        \\u2216\\n        S\\n      \\n    \\n    {\\\\displaystyle X\\\\setminus S}\\n   into \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n  , one at a time, with a specially chosen insertion ordering described below. Use binary search in subsequences of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   (as described below) to determine the position at which each element should be inserted.The algorithm is designed to take advantage of the fact that the binary searches used to insert elements into \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   are most efficient (from the point of view of worst case analysis) when the length of the subsequence that is searched is one less than a power of two. This is because, for those lengths, all outcomes of the search use the same number of comparisons as each other. To choose an insertion ordering that produces these lengths, consider the sorted sequence \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   after step 4 of the outline above (before inserting the remaining elements),\\nand let \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   denote the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  th element of this sorted sequence. Thus,\\n\\n  \\n    \\n      \\n        S\\n        =\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        ,\\n        \\u2026\\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle S=(x_{1},x_{2},x_{3},\\\\dots ),}\\n  where each element \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   with \\n  \\n    \\n      \\n        i\\n        \\u2265\\n        3\\n      \\n    \\n    {\\\\displaystyle i\\\\geq 3}\\n   is paired with an element \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        <\\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}<x_{i}}\\n   that has not yet been inserted. (There are no elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{1}}\\n   or \\n  \\n    \\n      \\n        \\n          y\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{2}}\\n   because \\n  \\n    \\n      \\n        \\n          x\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{1}}\\n   and \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{2}}\\n   were paired with each other.) If \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is odd, the remaining unpaired element should also be numbered as \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   with \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   larger than the indexes of the paired elements.\\nThen, the final step of the outline above can be expanded into the following steps:\\nPartition the uninserted elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   into groups with contiguous indexes. There are two elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{3}}\\n   and \\n  \\n    \\n      \\n        \\n          y\\n          \\n            4\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{4}}\\n   in the first group, and the sums of sizes of every two adjacent groups form a sequence of powers of two. Thus, the sizes of groups are: 2, 2, 6, 10, 22, 42, ...\\nOrder the uninserted elements by their groups (smaller indexes to larger indexes), but within each group order them from larger indexes to smaller indexes. Thus, the ordering becomes\\n  \\n    \\n      \\n        \\n          y\\n          \\n            4\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            3\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            6\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            5\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            12\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            11\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            10\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            9\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            8\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            7\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            22\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            21\\n          \\n        \\n        \\u2026\\n      \\n    \\n    {\\\\displaystyle y_{4},y_{3},y_{6},y_{5},y_{12},y_{11},y_{10},y_{9},y_{8},y_{7},y_{22},y_{21}\\\\dots }\\n  Use this ordering to insert the elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   into \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n  . For each element \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n  , use a binary search from the start of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   up to but not including \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   to determine where to insert \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n  .\\n\\nAnalysis\\nLet \\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle C(n)}\\n   denote the number of comparisons that merge-insertion sort makes, in the worst case, when sorting \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements.\\nThis number of comparisons can be broken down as the sum of three terms:\\n\\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   comparisons among the pairs of items,\\n\\n  \\n    \\n      \\n        C\\n        (\\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n        )\\n      \\n    \\n    {\\\\displaystyle C(\\\\lfloor n/2\\\\rfloor )}\\n   comparisons for the recursive call, and\\nsome number of comparisons for the binary insertions used to insert the remaining elements.In the third term, the worst-case number of comparisons for the elements in the first group is two, because each is inserted into a subsequence of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   of length at most three. First, \\n  \\n    \\n      \\n        \\n          y\\n          \\n            4\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{4}}\\n   is inserted into the three-element subsequence \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},x_{3})}\\n  . Then, \\n  \\n    \\n      \\n        \\n          y\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{3}}\\n   is inserted into some permutation of the three-element subsequence \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            4\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},y_{4})}\\n  , or in some cases into the two-element subsequence \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2})}\\n  . Similarly, the elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            6\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{6}}\\n   and \\n  \\n    \\n      \\n        \\n          y\\n          \\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{5}}\\n   of the second group are each inserted into a subsequence of length at most seven, using three comparisons. More generally, the worst-case number of comparisons for the elements in the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  th group is \\n  \\n    \\n      \\n        i\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle i+1}\\n  , because each is inserted into a subsequence of length at most \\n  \\n    \\n      \\n        \\n          2\\n          \\n            i\\n            +\\n            1\\n          \\n        \\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle 2^{i+1}-1}\\n  . By summing the number of comparisons used for all the elements and solving the resulting recurrence relation,\\nthis analysis can be used to compute the values of \\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle C(n)}\\n  , giving the formula\\n\\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          \\u2308\\n          \\n            \\n              log\\n              \\n                2\\n              \\n            \\n            \\u2061\\n            \\n              \\n                \\n                  3\\n                  i\\n                \\n                4\\n              \\n            \\n          \\n          \\u2309\\n        \\n        \\u2248\\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        1.415\\n        n\\n      \\n    \\n    {\\\\displaystyle C(n)=\\\\sum _{i=1}^{n}\\\\left\\\\lceil \\\\log _{2}{\\\\frac {3i}{4}}\\\\right\\\\rceil \\\\approx n\\\\log _{2}n-1.415n}\\n  or, in closed form,\\n\\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n        =\\n        n\\n        \\n          \\n            \\u2308\\n          \\n        \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        \\n          \\n            \\n              3\\n              n\\n            \\n            4\\n          \\n        \\n        \\n          \\n            \\u2309\\n          \\n        \\n        \\u2212\\n        \\n          \\n            \\u230a\\n          \\n        \\n        \\n          \\n            \\n              2\\n              \\n                \\u230a\\n                \\n                  log\\n                  \\n                    2\\n                  \\n                \\n                \\u2061\\n                6\\n                n\\n                \\u230b\\n              \\n            \\n            3\\n          \\n        \\n        \\n          \\n            \\u230b\\n          \\n        \\n        +\\n        \\n          \\n            \\u230a\\n          \\n        \\n        \\n          \\n            \\n              \\n                log\\n                \\n                  2\\n                \\n              \\n              \\u2061\\n              6\\n              n\\n            \\n            2\\n          \\n        \\n        \\n          \\n            \\u230b\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle C(n)=n{\\\\biggl \\\\lceil }\\\\log _{2}{\\\\frac {3n}{4}}{\\\\biggr \\\\rceil }-{\\\\biggl \\\\lfloor }{\\\\frac {2^{\\\\lfloor \\\\log _{2}6n\\\\rfloor }}{3}}{\\\\biggr \\\\rfloor }+{\\\\biggl \\\\lfloor }{\\\\frac {\\\\log _{2}6n}{2}}{\\\\biggr \\\\rfloor }.}\\n  For \\n  \\n    \\n      \\n        n\\n        =\\n        1\\n        ,\\n        2\\n        ,\\n        \\u2026\\n      \\n    \\n    {\\\\displaystyle n=1,2,\\\\dots }\\n   the numbers of comparisons are\\n0, 1, 3, 5, 7, 10, 13, 16, 19, 22, 26, 30, 34, ... (sequence A001768 in the OEIS)\\n\\nRelation to other comparison sorts\\nThe algorithm is called merge-insertion sort because the initial comparisons that it performs before its recursive call (pairing up arbitrary items and comparing each pair) are the same as the initial comparisons of merge sort,\\nwhile the comparisons that it performs after the recursive call (using binary search to insert elements one by one into a sorted list) follow the same principle as insertion sort. In this sense, it is a hybrid algorithm that combines both merge sort and insertion sort.For small inputs (up to \\n  \\n    \\n      \\n        n\\n        =\\n        11\\n      \\n    \\n    {\\\\displaystyle n=11}\\n  ) its numbers of comparisons equal the lower bound on comparison sorting of \\n  \\n    \\n      \\n        \\u2308\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        !\\n        \\u2309\\n        \\u2248\\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        1.443\\n        n\\n      \\n    \\n    {\\\\displaystyle \\\\lceil \\\\log _{2}n!\\\\rceil \\\\approx n\\\\log _{2}n-1.443n}\\n  . However, for larger inputs the number of comparisons made by the merge-insertion algorithm is bigger than this lower bound.\\nMerge-insertion sort also performs fewer comparisons than the sorting numbers, which count the comparisons made by binary insertion sort or merge sort in the worst case. The sorting numbers fluctuate between \\n  \\n    \\n      \\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        0.915\\n        n\\n      \\n    \\n    {\\\\displaystyle n\\\\log _{2}n-0.915n}\\n   and \\n  \\n    \\n      \\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        n\\n      \\n    \\n    {\\\\displaystyle n\\\\log _{2}n-n}\\n  , with the same leading term but a worse constant factor in the lower-order linear term.Merge-insertion sort is the sorting algorithm with the minimum possible comparisons for \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   items whenever \\n  \\n    \\n      \\n        n\\n        \\u2264\\n        15\\n      \\n    \\n    {\\\\displaystyle n\\\\leq 15}\\n   or \\n  \\n    \\n      \\n        20\\n        \\u2264\\n        n\\n        \\u2264\\n        22\\n      \\n    \\n    {\\\\displaystyle 20\\\\leq n\\\\leq 22}\\n  , and it has the fewest comparisons known for \\n  \\n    \\n      \\n        n\\n        \\u2264\\n        46\\n      \\n    \\n    {\\\\displaystyle n\\\\leq 46}\\n  .\\nFor 20 years, merge-insertion sort was the sorting algorithm with the fewest comparisons known for all input lengths.\\nHowever, in 1979 Glenn Manacher published another sorting algorithm that used even fewer comparisons, for large enough inputs.\\nIt remains unknown exactly how many comparisons are needed for sorting, for all \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  , but Manacher's algorithm\\nand later record-breaking sorting algorithms have all used modifications of the merge-insertion sort ideas.\\n\\n\\n== References ==\"}, {\"Odd\\u2013even sort\": \"In computing, an odd\\u2013even sort or odd\\u2013even transposition sort (also known as brick sort or parity sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections.  It is a comparison sort related to bubble sort, with which it shares many characteristics.  It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched.  The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.\\n\\nSorting on processor arrays\\nOn parallel processors, with one value per processor and only local left\\u2013right neighbor connections, the processors all concurrently do a compare\\u2013exchange operation with their neighbors, alternating between odd\\u2013even and even\\u2013odd pairings.  This algorithm was originally presented, and shown to be efficient on such processors, by Habermann in 1972.The algorithm extends efficiently to the case of multiple items per processor.  In the Baudet\\u2013Stevenson odd\\u2013even merge-splitting algorithm, each processor sorts its own sublist at each step, using any efficient sort algorithm, and then performs a merge splitting, or transposition\\u2013merge, operation with its neighbor, with neighbor pairing alternating between odd\\u2013even and even\\u2013odd on each step.\\n\\nBatcher's odd\\u2013even mergesort\\nA related but more efficient sort algorithm is the Batcher odd\\u2013even mergesort, using compare\\u2013exchange operations and perfect-shuffle operations.\\nBatcher's method is efficient on parallel processors with long-range connections.\\n\\nAlgorithm\\nThe single-processor algorithm, like bubblesort, is simple but not very efficient. Here a zero-based index is assumed:\\n\\nProof of correctness\\nClaim:  Let \\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{1},...,a_{n}}\\n   be a sequence of data ordered by <.  The odd\\u2013even sort algorithm correctly sorts this data in \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   passes.  (A pass here is defined to be a full sequence of odd\\u2013even, or even\\u2013odd comparisons.  The passes occur in order pass 1: odd\\u2013even, pass 2: even\\u2013odd, etc.)\\nProof:\\nThis proof is based loosely on one by Thomas Worsch.Since the sorting algorithm only involves comparison-swap operations and is oblivious (the order of comparison-swap operations does not depend on the data), by Knuth's 0\\u20131 sorting principle, it suffices to check correctness when each \\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{i}}\\n   is either 0 or 1. Assume that there are \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n   1s.\\nObserve that the rightmost 1 can be either in an even or odd position, so it might not be moved by the first odd\\u2013even pass. But after the first odd\\u2013even pass, the rightmost 1 will be in an even position. It follows that it will be moved to the right by all remaining passes. Since the rightmost one starts in position greater than or equal to \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n  , it must be moved at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n      \\n    \\n    {\\\\displaystyle n-e}\\n   steps. It follows that it takes at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle n-e+1}\\n   passes to move the rightmost 1 to its correct position.\\nNow, consider the second rightmost 1. After two passes, the 1 to its right will have moved right by at least one step. It follows that, for all remaining passes, we can view the second rightmost 1 as the rightmost 1. The second rightmost 1 starts in position at least \\n  \\n    \\n      \\n        e\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle e-1}\\n   and must be moved to position at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n  , so it must be moved at most \\n  \\n    \\n      \\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\u2212\\n        (\\n        e\\n        \\u2212\\n        1\\n        )\\n        =\\n        n\\n        \\u2212\\n        e\\n      \\n    \\n    {\\\\displaystyle (n-1)-(e-1)=n-e}\\n   steps. After at most 2 passes, the rightmost 1 will have already moved, so the entry to the right of the second rightmost 1 will be 0.  Hence, for all passes after the first two, the second rightmost 1 will move to the right. It thus takes at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        2\\n      \\n    \\n    {\\\\displaystyle n-e+2}\\n   passes to move the second rightmost 1 to its correct position.\\nContinuing in this manner, by induction it can be shown that the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th rightmost 1 is moved to its correct position in at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        i\\n      \\n    \\n    {\\\\displaystyle n-e+i}\\n   passes. Since \\n  \\n    \\n      \\n        i\\n        \\u2264\\n        e\\n      \\n    \\n    {\\\\displaystyle i\\\\leq e}\\n  , it follows that the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th rightmost 1 is moved to its correct position in at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        e\\n        =\\n        n\\n      \\n    \\n    {\\\\displaystyle n-e+e=n}\\n   passes. The list is thus correctly sorted in \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   passes. QED.\\nWe remark that each pass takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   steps, so this algorithm has \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   complexity.\\n\\n\\n== References ==\"}, {\"Oscillating merge sort\": \"Oscillating merge sort or oscillating sort is a variation of merge sort used with tape drives that can read backwards.  Instead of doing a complete distribution as is done in a tape merge, the distribution of the input and the merging of runs are interspersed.  The oscillating merge sort does not waste rewind time or have tape drives sit idle as in the conventional tape merge.\\nThe oscillating merge sort \\\"was designed for tapes that can be read backward and is more efficient generally than either the polyphase or cascade merges.\\\"\\n\\nReferences\\nBradley, James (1982), File and Data Base Techniques, Holt, Rinehart and Winston, ISBN 0-03-058673-9\\n\\nFurther reading\\nFlores, Ivan (1969), Computer Sorting, Prentice-Hall, ISBN 978-0-13165746-5\\nKnuth, D. E. (1975), Sorting and Searching, The Art of Computer Programming, vol. 3, Addison Wesley\\nLowden, B. G. T., \\\"A note on the oscillating sort\\\" (PDF), The Computer Journal, 20 (1): 92, doi:10.1093/comjnl/20.1.92\\nMartin, W. A. (1971), \\\"Sorting\\\", Computing Surveys, ACM, 3 (4): 147\\u2013174, doi:10.1145/356593.356594\\nSobel, Sheldon (July 1962), \\\"Oscillating Sort\\u2013A New Sort Merging Technique\\\", Journal of the ACM, New York, NY: ACM, 9 (3): 372\\u2013374, doi:10.1145/321127.321133, S2CID 11554742\\n\\nExternal links\\nMihaldinecz, Maximilian (2016), \\\"A variation of Oscillating Merge Sort implemented in Matlab\\\", GitHub\"}, {\"Pairwise sorting network\": \"The pairwise sorting network is a sorting network discovered and published by Ian Parberry in 1992 in Parallel Processing Letters. The pairwise sorting network has the same size (number of comparators) and depth as the odd\\u2013even mergesort network. At the time of publication, the network was one of several known networks with a depth of \\n  \\n    \\n      \\n        O\\n        (\\n        l\\n        o\\n        \\n          g\\n          \\n            2\\n          \\n        \\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(log^{2}n)}\\n  . It requires \\n  \\n    \\n      \\n        n\\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n        (\\n        log\\n        \\u2061\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\n          /\\n        \\n        4\\n        +\\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n(\\\\log n)(\\\\log n-1)/4+n-1}\\n   comparators and has depth \\n  \\n    \\n      \\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n        (\\n        log\\n        \\u2061\\n        n\\n        +\\n        1\\n        )\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle (\\\\log n)(\\\\log n+1)/2}\\n  .\\nThe sorting procedure implemented by the network is as follows (guided by the zero-one principle):\\n\\nSort consecutive pairwise bits of the input (corresponds to the first layer of the diagram)\\nSort all pairs into lexicographic order by recursively sorting all odd bits and even bits separately (corresponds to the next 14 layers of the diagram)\\nSort the pairs in nondecreasing order using a specialized network (corresponds to the final layers of the diagram)\\n\\nRelation to Batcher odd-even mergesort\\nThe pairwise sorting network is very similar to the Batcher odd-even mergesort, but differs in the structure of operations. While Batcher repeatedly divides, sorts and merges increasingly longer subsequences, the pairwise method does all the subdivision first, then does all the merging at the end in the reverse sequence. In certain applications like encoding cardinality constraints, the pairwise sorting network is superior to the Batcher network.\\n\\nReferences\\nExternal links\\nSorting Networks \\u2013 Archive of web page by the author.\"}, {\"Pancake sorting\": \"Pancake sorting is the mathematical problem of sorting a disordered stack of pancakes in order of size when a spatula can be inserted at any point in the stack and used to flip all pancakes above it.  A pancake number is the minimum number of flips required for a given number of pancakes. In this form, the problem was first discussed by American geometer Jacob E. Goodman. A variant of the problem is concerned with burnt pancakes, where each pancake has a burnt side and all pancakes must, in addition, end up with the burnt side on bottom.\\nAll sorting methods require pairs of elements to be compared. For the traditional sorting problem, the usual problem studied is to minimize the number of comparisons required to sort a list. The number of actual operations, such as swapping two elements, is then irrelevant. For pancake sorting problems, in contrast, the aim is to minimize the number of operations, where the only allowed operations are reversals of the elements of some prefix of the sequence. Now, the number of comparisons is irrelevant.\\n\\nThe pancake problems\\nThe original pancake problem\\nThe minimum number of flips required to sort any stack of n pancakes has been shown to lie between 15/14n and 18/11n (approximately 1.07n and 1.64n,) but the exact value is not known.The simplest pancake sorting algorithm performs at most 2n \\u2212 3 flips. In this algorithm, a kind of selection sort, we bring the largest pancake not yet sorted to the top with one flip; take it down to its final position with one more flip; and repeat this process for the remaining pancakes.\\nIn 1979, Bill Gates and Christos Papadimitriou gave a lower bound of 1.06n flips and an upper bound of (5n+5)/3. The upper bound was improved, thirty years later, to  18/11n by a team of researchers at the University of Texas at Dallas, led by Founders Professor Hal Sudborough.In 2011, Laurent Bulteau, Guillaume Fertin, and Irena Rusu proved that the problem of finding the shortest sequence of flips for a given stack of pancakes is NP-hard, thereby answering a question that had been open for over three decades.\\n\\nThe burnt pancake problem\\nIn a variation called the burnt pancake problem, the bottom of each pancake in the pile is burnt, and the sort must be completed with the burnt side of every pancake down. It is a signed permutation, and if a pancake i is \\\"burnt side up\\\" a negative element i` is put in place of i in the permutation. In 2008, a group of undergraduates built a bacterial computer that can solve a simple example of the burnt pancake problem by programming E. coli to flip segments of DNA which are analogous to burnt pancakes. DNA has an orientation (5' and 3') and an order (promoter before coding). Even though the processing power expressed by DNA flips is low, the high number of bacteria in a culture provides a large parallel computing platform. The bacteria report when they have solved the problem by becoming antibiotic resistant.\\n\\nThe identical pancakes stack problem\\nThis is inspired from the way Indian bread (roti or chapati) is cooked. Initially, all rotis are stacked in one column, and the cook uses a spatula to flip the rotis so that each side of each roti touches the base fire at some point to toast. Several variants are possible: the rotis can be considered as single-sided or two-sided, and it may be forbidden or not to toast the same side twice. This version of the problem was first explored by Arka Roychowdhury.\\n\\nThe pancake problem on strings\\nThe discussion above presumes that each pancake is unique, that is, the sequence on which the prefix reversals are performed is a permutation. However, \\\"strings\\\" are sequences in which a symbol can repeat, and this repetition may reduce the number of prefix reversals required to sort. Chitturi and Sudborough (2010) and Hurkens et al. (2007) independently showed that the complexity of transforming a compatible string into another with the minimum number of prefix reversals is NP-complete. They also gave bounds for the same. Hurkens et al. gave an exact algorithm to sort binary and ternary strings. Chitturi  (2011) proved that the complexity of transforming a compatible signed string into another with the minimum number of signed prefix reversals\\u2014the burnt pancake problem on strings\\u2014is NP-complete.\\n\\nHistory\\nThe pancake sorting problem was first posed by Jacob E. Goodman, writing under the pseudonym \\\"Harry Dweighter\\\" (\\\"harried waiter\\\").Although seen more often as an educational device, pancake sorting also appears in applications in parallel processor networks, in which it can provide an effective routing algorithm between processors.The problem is notable as the topic of the only well-known mathematics paper by Microsoft founder Bill Gates (as William Gates), entitled \\\"Bounds for Sorting by Prefix Reversal\\\" and co-authored with Christos Papadimitriou. Published in 1979, it describes an efficient algorithm for pancake sorting. In addition, the most notable paper published by Futurama co-creator David X. Cohen (as David S. Cohen), co-authored with Manuel Blum, concerned the burnt pancake problem.The connected problems of signed sorting by reversals and sorting by reversals were also studied more recently. Whereas efficient exact algorithms have been found for the signed sorting by reversals, the problem of sorting by reversals has been proven to be hard even to approximate to within certain constant factor, and also proven to be approximable in polynomial time to within the approximation factor 1.375.\\n\\nPancake graphs\\nAn n-pancake graph is a graph whose vertices are the permutations of n symbols from 1 to n and its edges are given between permutations transitive by prefix reversals. It is a regular graph with n! vertices, its degree is n\\u22121. The pancake sorting problem and the problem to obtain the diameter of the pancake graph is equivalent.The pancake graph of dimension n, Pn can be constructed recursively from n copies of Pn\\u22121, by assigning a different element from the set {1, 2, \\u2026, n} as a suffix to each copy.\\nTheir girth:\\n\\n  \\n    \\n      \\n        g\\n        (\\n        \\n          P\\n          \\n            n\\n          \\n        \\n        )\\n        =\\n        6\\n        \\n          , if \\n        \\n        n\\n        >\\n        2\\n      \\n    \\n    {\\\\displaystyle g(P_{n})=6{\\\\text{, if }}n>2}\\n  .The \\u03b3(Pn) genus of Pn is:\\n\\n  \\n    \\n      \\n        n\\n        !\\n        \\n          (\\n          \\n            \\n              \\n                n\\n                \\u2212\\n                4\\n              \\n              6\\n            \\n          \\n          )\\n        \\n        +\\n        1\\n        \\u2264\\n        \\u03b3\\n        (\\n        \\n          P\\n          \\n            n\\n          \\n        \\n        )\\n        \\u2264\\n        n\\n        !\\n        \\n          (\\n          \\n            \\n              \\n                n\\n                \\u2212\\n                3\\n              \\n              4\\n            \\n          \\n          )\\n        \\n        \\u2212\\n        \\n          \\n            n\\n            2\\n          \\n        \\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle n!\\\\left({\\\\frac {n-4}{6}}\\\\right)+1\\\\leq \\\\gamma (P_{n})\\\\leq n!\\\\left({\\\\frac {n-3}{4}}\\\\right)-{\\\\frac {n}{2}}+1}\\n  Since pancake graphs have many interesting properties such as symmetric and recursive structures, small degrees and diameters compared against the size of the graph, much attention is paid to them as a model of interconnection networks for parallel computers. When we regard the pancake graphs as the model of the interconnection networks, the diameter of the graph is a measure that represents the delay of communication.The pancake graphs are Cayley graphs (thus are vertex-transitive) and are especially attractive for parallel processing. They have sublogarithmic degree and diameter, and are relatively sparse (compared to e.g. hypercubes).\\n\\nAlgorithm\\nAn example of the pancake sorting algorithm is given below in Python. The code is similar to bubble sort or selection sort.\\n\\nRelated integer sequences\\nSequences from The Online Encyclopedia of Integer Sequences:\\n\\nOEIS: A058986 \\u2013 maximum number of flips\\nOEIS: A067607 \\u2013 number of stacks requiring the maximum number of flips (above)\\nOEIS: A078941 \\u2013 maximum number of flips for a \\\"burnt\\\" stack\\nOEIS: A078942 \\u2013 the number of flips for a sorted \\\"burnt-side-on-top\\\" stack\\nOEIS: A092113 \\u2013 the above triangle, read by rows\\n\\nReferences\\nFurther reading\\nExternal links\\nCut-the-Knot: Flipping pancakes puzzle, including a Java applet for the pancake problem and some discussion.\\nDouglas B. West's \\\"The Pancake Problems\\\"\\nWeisstein, Eric W. \\\"Pancake Sorting\\\". MathWorld.\\nAnimation explaining the bacterial computer that can solve the burnt pancake problem.\\n\\\"Tower1/Pancake Flip\\\" by Arka. A game based on pancake problem principle\"}, {\"Partial sorting\": \"In computer science, partial sorting is a relaxed variant of the sorting problem. Total sorting is the problem of returning a list of items such that its elements all appear in order, while partial sorting is returning a list of the k smallest (or k largest) elements in order. The other elements (above the k smallest ones) may also be sorted, as in an in-place partial sort, or may be discarded, which is common in streaming partial sorts. A common practical example of partial sorting is computing the \\\"Top 100\\\" of some list.\\nIn terms of indices, in a partially sorted list, for every index i from 1 to k, the i-th element is in the same place as it would be in the fully sorted list: element i of the partially sorted list contains order statistic i of the input list.\\n\\nOffline problems\\nHeap-based solution\\nHeaps admit a simple single-pass partial sort when k is fixed: insert the first k elements of the input into a max-heap. Then make one pass over the remaining elements, add each to the heap in turn, and remove the largest element. Each insertion operation takes O(log k) time, resulting in O(n log k) time overall; this \\\"partial heapsort\\\" algorithm is practical for small values of k and in online settings. An \\\"online heapselect\\\" algorithm described below, based on a min-heap, takes O(n + k log n).\\n\\nSolution by partitioning selection\\nA further relaxation requiring only a list of the k smallest elements, but without requiring that these be ordered, makes the problem equivalent to partition-based selection; the original partial sorting problem can be solved by such a selection algorithm to obtain an array where the first k elements are the k smallest, and sorting these, at a total cost of O(n + k log k) operations. A popular choice to implement this algorithm scheme is to combine quickselect and quicksort; the result is sometimes called \\\"quickselsort\\\".Common in current (as of 2022) C++ STL implementations is a pass of heapselect for a list of k elements, followed by a heapsort for the final result.\\n\\nSpecialised sorting algorithms\\nMore efficient than the aforementioned are specialized partial sorting algorithms based on mergesort and quicksort. In the quicksort variant, there is no need to recursively sort partitions which only contain elements that would fall after the k'th place in the final sorted array (starting from the \\\"left\\\" boundary). Thus, if the pivot falls in position k or later, we recurse only on the left partition:\\nfunction partial_quicksort(A, i, j, k) is\\n    if i < j then\\n        p \\u2190 pivot(A, i, j)\\n        p \\u2190 partition(A, i, j, p)\\n        partial_quicksort(A, i, p-1, k)\\n        if p < k-1 then\\n            partial_quicksort(A, p+1, j, k)\\n\\nThe resulting algorithm is called partial quicksort and requires an expected time of only O(n + k log k), and is quite efficient in practice, especially if a selection sort is used as a base case when k becomes small relative to n. However, the worst-case time complexity is still very bad, in the case of a bad pivot selection. Pivot selection along the lines of the worst-case linear time selection algorithm (see Quicksort \\u00a7 Choice of pivot) could be used to get better worst-case performance. Partial quicksort, quickselect (including the multiple variant), and quicksort can all be generalized into what is known as a chunksort.\\n\\nIncremental sorting\\nIncremental sorting is a version of the partial sorting problem where the input is given up front but k is unknown: given a k-sorted array, it should be possible to extend the partially sorted part so that the array becomes (k+1)-sorted.Heaps lead to an O(n + k log n) \\\"online heapselect\\\" solution to incremental partial sorting: first \\\"heapify\\\", in linear time, the complete input array to produce a min-heap. Then extract the minimum of the heap k times.A different incremental sort can be obtained by modifying quickselect. The version due to Paredes and Navarro maintains a stack of pivots across calls, so that incremental sorting can be accomplished by repeatedly requesting the smallest item of an array A from the following algorithm:\\n\\nThe stack S is initialized to contain only the length n of A. k-sorting the array is done by calling IQS(A, i, S) for i = 0, 1, 2, ...; this sequence of calls has average-case complexity O(n + k log k), which is asymptotically equivalent to O(n + k log n). The worst-case time is quadratic, but this can be fixed by replacing the random pivot selection by the median of medians algorithm.\\n\\nLanguage/library support\\nThe C++ standard specifies a library function called std::partial_sort.\\nThe Python standard library includes functions nlargest and nsmallest in its heapq module.\\nThe Julia standard library includes a PartialQuickSort algorithm used in partialsort! and variants.\\n\\nSee also\\nSelection algorithm\\n\\nReferences\\nExternal links\\nJ.M. Chambers (1971). Partial sorting. CACM 14(5):357\\u2013358.\"}, {\"Patience sorting\": \"In computer science, patience sorting is a sorting algorithm inspired by, and named after, the card game patience. A variant of the algorithm efficiently computes the length of a longest increasing subsequence in a given array.\\n\\nOverview\\nThe algorithm's name derives from a simplified variant of the patience card game. The game begins with a shuffled deck of cards. The cards are dealt one by one into a sequence of piles on the table, according to the following rules.\\nInitially, there are no piles. The first card dealt forms a new pile consisting of the single card.\\nEach subsequent card is placed on the leftmost existing pile whose top card has a value greater than or equal to the new card's value, or to the right of all of the existing piles, thus forming a new pile.\\nWhen there are no more cards remaining to deal, the game ends.This card game is turned into a two-phase sorting algorithm, as follows. Given an array of n elements from some totally ordered domain, consider this array as a collection of cards and simulate the patience sorting game. When the game is over, recover the sorted sequence by repeatedly picking off the minimum visible card; in other words, perform a k-way merge of the p piles, each of which is internally sorted.\\n\\nAnalysis\\nThe first phase of patience sort, the card game simulation, can be implemented to take O(n log n) comparisons in the worst case for an n-element input array: there will be at most n piles, and by construction, the top cards of the piles form an increasing sequence from left to right, so the desired pile can be found by binary search. The second phase, the merging of piles, can be done in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   time as well using a priority queue.When the input data contain natural \\\"runs\\\", i.e., non-decreasing subarrays, then performance can be strictly better. In fact, when the input array is already sorted, all values form a single pile and both phases run in O(n) time. The average-case complexity is still O(n log n): any uniformly random sequence of values will produce an expected number of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O({\\\\sqrt {n}})}\\n   piles, which take \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        \\n          \\n            n\\n          \\n        \\n        )\\n        =\\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log {\\\\sqrt {n}})=O(n\\\\log n)}\\n   time to produce and merge.An evaluation of the practical performance of patience sort is given by Chandramouli and Goldstein, who show that a naive version is about ten to twenty times slower than a state-of-the-art quicksort on their benchmark problem. They attribute this to the relatively small amount of research put into patience sort, and develop several optimizations that bring its performance to within a factor of two of that of quicksort.If values of cards are in the range 1, . . . , n, there is an efficient implementation with \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   worst-case running time for putting the cards into piles, relying on a Van Emde Boas tree.\\n\\nRelations to other problems\\nPatience sorting is closely related to a card game called Floyd's game. This game is very similar to the game sketched earlier:\\nThe first card dealt forms a new pile consisting of the single card.\\nEach subsequent card is placed on some existing pile whose top card has a value no less than the new card's value, or to the right of all of the existing piles, thus forming a new pile.\\nWhen there are no more cards remaining to deal, the game ends.The object of the game is to finish with as few piles as possible. The difference with the patience sorting algorithm is that there is no requirement to place a new card on the leftmost pile where it is allowed. Patience sorting constitutes a greedy strategy for playing this game.\\nAldous and Diaconis suggest defining 9 or fewer piles as a winning outcome for n = 52, which happens with approximately 5% probability.\\n\\nAlgorithm for finding a longest increasing subsequence\\nFirst, execute the sorting algorithm as described above. The number of piles is the length of a longest subsequence. Whenever a card is placed on top of a pile, put a back-pointer to the top card in the previous pile (that, by assumption, has a lower value than the new card has). In the end, follow the back-pointers from the top card in the last pile to recover a decreasing subsequence of the longest length; its reverse is an answer to the longest increasing subsequence algorithm.\\nS. Bespamyatnikh and M. Segal give a description of an efficient implementation of the algorithm, incurring no additional asymptotic cost over the sorting one (as the back-pointers storage, creation and traversal require linear time and space). They further show how to report all the longest increasing subsequences from the same resulting data structures.\\n\\nHistory\\nPatience sorting was named by C. L. Mallows, who attributed its invention to A.S.C. Ross in the early 1960s.\\nAccording to Aldous and Diaconis, patience sorting was first recognized as an algorithm to compute the longest increasing subsequence length by Hammersley. A.S.C. Ross and independently Robert W. Floyd recognized it as a sorting algorithm. Initial analysis was done by Mallows. Floyd's game was developed by Floyd in correspondence with Donald Knuth.\\n\\nUse\\nThe patience sorting algorithm can be applied to process control. Within a series of measurements, the existence of a long increasing subsequence can be used as a trend marker. A 2002 article in SQL Server magazine includes a SQL implementation, in this context, of the patience sorting algorithm for the length of the longest increasing subsequence.\\n\\n\\n== References ==\"}, {\"Pigeonhole sort\": \"Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number n of elements and the length N of the range of possible key values are approximately the same. It requires O(n + N) time.  It is similar to counting sort, but differs in that it \\\"moves items twice: once to the bucket array and again to the final destination [whereas] counting sort builds an auxiliary array then uses the array to compute each item's final destination and move the item there.\\\"The pigeonhole algorithm works as follows:\\n\\nGiven an array of values to be sorted, set up an auxiliary array of initially empty \\\"pigeonholes\\\", one pigeonhole for each key in the range of the keys in the original array.\\nGoing over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole eventually contains a list of all values with that key.\\nIterate over the pigeonhole array in increasing order of keys, and for each pigeonhole, put its elements into the original array in increasing order.\\n\\nSee also\\nPigeonhole principle\\nRadix sort\\nBucket queue, a related priority queue data structure\\n\\n\\n== References ==\"}, {\"Polyphase merge sort\": \"A polyphase merge sort is a variation of a bottom-up merge sort that sorts a list using an initial uneven distribution of sub-lists (runs), primarily used for external sorting, and is more efficient than an ordinary merge sort when there are fewer than eight external working files (such as a tape drive or a file on a hard drive). A polyphase merge sort is not a stable sort.\\n\\nOrdinary merge sort\\nA merge sort splits the records of a dataset into sorted runs of records and then repeatedly merges sorted runs into larger sorted runs until only one run, the sorted dataset, remains.\\nAn ordinary merge sort using four working files organizes them as a pair of input files and a pair of output files. The dataset is distributed evenly between two of the working files, either as sorted runs or in the simplest case, single records, which can be considered to be sorted runs of size 1. Once all of the dataset is transferred to the two working files, those two working files become the input files for the first merge iteration. Each merge iteration merges runs from the two input working files, alternating the merged output between the two output files, again distributing the merged runs evenly between the two output files (until the final merge iteration). Once all of the runs from the two inputs files are merged and output, then the output files become the input files and vice versa for the next merge iteration. The number of runs decreases by a factor of 2 at each iteration, such as 64, 32, 16, 8, 4, 2, 1. For the final merge iteration, the two input files only have one sorted run (1/2 of the dataset) each, and the merged result is a single sorted run (the sorted dataset) on one of the output files. This is also described at Merge sort \\u00a7 Use with tape drives.\\nIf there are only three working files, then an ordinary merge sort merges sorted runs from two working files onto a single working file, then distributes the runs evenly between the two output files. The merge iteration reduces run count by a factor of 2, the redistribute iteration doesn't reduce run count (the factor is 1). Each iteration could be considered to reduce the run count by an average factor of \\u221a2 \\u2248 1.41. If there are 5 working files, then the pattern alternates between a 3 way merge and a 2 way merge, for an average factor of \\u221a6 \\u2248 2.45.\\nIn general, for an even number N of working files, each iteration of an ordinary merge sort reduces run count by a factor of N/2, while for an odd number N of working files, each iteration reduces the run count by an average factor of \\u221a(N2\\u22121)/4 = \\u221aN2\\u22121/2.\\n\\nPolyphase merge\\nFor N < 8 working files, a polyphase merge sort achieves a higher effective run count reduction factor by unevenly distributing sorted runs between N\\u22121 working files (explained in next section). Each iteration merges runs from N\\u22121 working files onto a single output working file. When the end of one of the N\\u22121 working files is reached, then it becomes the new output file and what was the output file becomes one of the N\\u22121 working input files, starting a new iteration of polyphase merge sort. Each iteration merges only a fraction of the dataset (about 1/2 to 3/4), except for the last iteration which merges all of the dataset into a single sorted run. The initial distribution is set up so that only one input working file is emptied at a time, except for the final merge iteration which merges N\\u22121 single runs (of varying size, this is explained next) from the N\\u22121 input working files to the single output file, resulting in a single sorted run, the sorted dataset.\\nFor each polyphase iteration, the total number of runs follows a pattern similar to a reversed Fibonacci numbers of higher order sequence. With 4 files, and a dataset consisting of 57 runs, the total run count on each iteration would be 57, 31, 17, 9, 5, 3, 1. Note that except for the last iteration, the run count reduction factor is a bit less than 2, 57/31, 31/17, 17/9, 9/5, 5/3, 3/1, about 1.84 for a 4 file case, but each iteration except the last reduced the run count while processing about 65% of the dataset, so the run count reduction factor per dataset processed during the intermediate iterations is about 1.84 / 0.65 = 2.83. For a dataset consisting of 57 runs of 1 record each, then after the initial distribution, polyphase merge sort moves 232 records during the 6 iterations it takes to sort the dataset, for an overall reduction factor of 2.70 (this is explained in more detail later).\\nAfter the first polyphase iteration, what was the output file now contains the results of merging N\\u22121 original runs, but the remaining N\\u22122 input working files still contain the remaining original runs, so the second merge iteration produces runs of size (N\\u22121) + (N\\u22122) = (2N \\u2212 3) original runs. The third iteration produces runs of size (4N \\u2212 7) original runs. With 4 files, the first iteration creates runs of size 3 original runs, the second iteration 5 original runs, the third iteration 9 original runs and so on, following the Fibonacci like pattern, 1, 3, 5, 9, 17, 31, 57, ... , so the increase in run size follows the same pattern as the decrease in run count in reverse. In the example case of 4 files and 57 runs of 1 record each, the last iteration merges 3 runs of size 31, 17, 9, resulting in a single sorted run of size 31+17+9 = 57 records, the sorted dataset. An example of the run counts and run sizes for 4 files, 31 records can be found in table 4.3 of.\\n\\nPerfect 3 file polyphase merge sort\\nIt is easiest to look at the polyphase merge starting from its ending conditions and working backwards. At the start of each iteration, there will be two input files and one output file. At the end of the iteration, one input file will have been completely consumed and will become the output file for the next iteration. The current output file will become an input file for the next iteration. The remaining files (just one in the 3 file case) have only been partially consumed and their remaining runs will be input for the next iteration.\\nFile 1 just emptied and became the new output file. One run is left on each input tape, and merging those runs together will make the sorted file.\\n\\nFile 1 (out):                                           <1 run> *        (the sorted file)\\nFile 2 (in ): ... | <1 run> *               -->     ... <1 run> | *          (consumed)\\nFile 3 (in ):     | <1 run> *                           <1 run> | *          (consumed)\\n\\n...  possible runs that have already been read\\n|    marks the read pointer of the file\\n*    marks end of file\\n\\nStepping back to the previous iteration, we were reading from 1 and 2. One run is merged from 1 and 2 before file 1 goes empty.  Notice that file 2 is not completely consumed\\u2014it has one run left to match the final merge (above).\\n\\nFile 1 (in ): ... | <1 run> *                      ... <1 run> | *\\nFile 2 (in ):     | <2 run> *           -->            <1 run> | <1 run> *\\nFile 3 (out):                                          <1 run> *\\n\\nStepping back another iteration, 2 runs are merged from 1 and 3 before file 3 goes empty.\\n\\nFile 1 (in ):     | <3 run>                        ... <2 run> | <1 run> *\\nFile 2 (out):                               -->        <2 run> *\\nFile 3 (in ): ... | <2 run> *                          <2 run> | *\\n\\nStepping back another iteration, 3 runs are merged from 2 and 3 before file 2 goes empty.\\n\\nFile 1 (out):                                          <3 run> *\\nFile 2 (in ): ... | <3 run> *               -->    ... <3 run> | *\\nFile 3 (in ):     | <5 run> *                          <3 run> | <2 run> *\\n\\nStepping back another iteration, 5 runs are merged from 1 and 2 before file 1 goes empty.\\n\\nFile 1 (in ): ... | <5 run> *                      ... <5 run> | *\\nFile 2 (in ):     | <8 run> *               -->        <5 run> | <3 run> *\\nFile 3 (out):                                          <5 run> *\\n\\nDistribution for polyphase merge sort\\nLooking at the perfect 3 file case, the number of runs for merged working backwards: 1, 1, 2, 3, 5, ... reveals a Fibonacci sequence. The sequence for more than 3 files is a bit more complicated; for 4 files, starting at the final state and working backwards, the run count pattern is {1,0,0,0}, {0,1,1,1}, {1,0,2,2}, {3,2,0,4}, {7,6,4,0}, {0,13,11,7}, {13,0,24,20}, ... .\\nFor everything to work out optimally, the last merge phase should have exactly one run on each input file.  If any input file has more than one run, then another phase would be required. Consequently, the polyphase merge sort needs to be clever about the initial distribution of the input data's runs to the initial output files.  For example, an input file with 13 runs would write 5 runs to file 1 and 8 runs to file 2.\\nIn practice, the input file will not have the exact number of runs needed for a perfect distribution. One way to deal with this is by padding the actual distribution with imaginary \\\"dummy runs\\\" to simulate an ideal run distribution. A dummy run behaves like a run with no records in it. Merging one or more dummy runs with one or more real runs just merges the real runs, and merging one or more dummy runs with no real runs results in a single dummy run. Another approach is to emulate dummy runs as needed during the merge operations.\\\"Optimal\\\" distribution algorithms require knowing the number of runs in advance. Otherwise, in the more common case where the number of runs is not known in advance, \\\"near optimal\\\" distribution algorithms are used. Some distribution algorithms include rearranging runs. If the number of runs is known in advance, only a partial distribution is needed before starting the merge phases. For example, consider the 3 file case, starting with n runs in File_1. Define Fi = Fi\\u22121 + Fi\\u22122 as the ith Fibonacci number.  If n = Fi, then move Fi\\u22122 runs to File_2, leaving Fi\\u22121 runs remaining on File_1, a perfect run distribution. If Fi < n < Fi+1, move n\\u2212Fi runs to File_2 and Fi+1\\u2212n runs to File_3. The first merge iteration merges n\\u2212Fi runs from File_1 and File_2, appending the n\\u2212Fi merged runs to the Fi+1\\u2212n runs already moved to File_3. File_1 ends up with Fi\\u22122 runs remaining, File_2 is emptied, and File_3 ends up with Fi\\u22121 runs, again a perfect run distribution. For 4 or more files, the math is more complicated, but the concept is the same.\\n\\nComparison versus ordinary merge sort\\nAfter the initial distribution, an ordinary merge sort using 4 files will sort 16 single record runs in 4 iterations of the entire dataset, moving a total of 64 records in order to sort the dataset after the initial distribution. A polyphase merge sort using 4 files will sort 17 single record runs in 4 iterations, but since each iteration but the last iteration only moves a fraction of the dataset, it only moves a total of 48 records in order to sort the dataset after the initial distribution. In this case, ordinary merge sort factor is 2.0, while polyphase overall factor is \\u22482.73.\\nTo explain how the reduction factor is related to sort performance, the reduction factor equations are:\\n\\nreduction_factor = exp(number_of_runs*log(number_of_runs)/run_move_count)\\nrun_move_count = number_of_runs * log(number_of_runs)/log(reduction_factor)\\nrun_move_count = number_of_runs * log_reduction_factor(number_of_runs)\\n\\nUsing the run move count equation for the above examples: \\n\\nordinary merge sort \\u2192 \\n  \\n    \\n      \\n        16\\n        \\u00d7\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        16\\n        )\\n        =\\n        64\\n      \\n    \\n    {\\\\displaystyle 16\\\\times \\\\log _{2}(16)=64}\\n  ,\\npolyphase merge sort \\u2192 \\n  \\n    \\n      \\n        17\\n        \\u00d7\\n        \\n          log\\n          \\n            2.73\\n          \\n        \\n        \\u2061\\n        (\\n        17\\n        )\\n        =\\n        48\\n      \\n    \\n    {\\\\displaystyle 17\\\\times \\\\log _{2.73}(17)=48}\\n  .Here is a table of effective reduction factors for polyphase and ordinary merge sort listed by number of files, based on actual sorts of a few million records. This table roughly corresponds to the reduction factor per dataset moved tables shown in fig 3 and fig 4 of polyphase merge sort.pdf\\n\\n# files\\n|     average fraction of data per iteration\\n|     |     polyphase reduction factor on ideal sized data\\n|     |     |     ordinary reduction factor on ideal sized data\\n|     |     |     |\\n3     .73   1.94  1.41  (sqrt  2)\\n4     .63   2.68  2.00\\n5     .58   3.20  2.45  (sqrt  6)\\n6     .56   3.56  3.00\\n7     .55   3.80  3.46  (sqrt 12)\\n8     .54   3.95  4.00\\n9     .53   4.07  4.47  (sqrt 20)\\n10    .53   4.15  5.00\\n11    .53   4.22  5.48  (sqrt 30)\\n12    .53   4.28  6.00\\n32    .53   4.87 16.00\\n\\nIn general, polyphase merge sort is better than ordinary merge sort when there are fewer than 8 files, while ordinary merge sort starts to become better at around 8 or more files.\\n\\nReferences\\nFurther reading\\nBradley, James (1982), File and Data Base Techniques, Holt, Rinehart and Winston, ISBN 0-03-058673-9\\nReynolds, Samuel W. (August 1961), \\\"A generalized polyphase merge algorithm\\\", Communications of the ACM, New York, NY: ACM, 4 (8): 347\\u2013349, doi:10.1145/366678.366689, S2CID 28416100\\nSedgewick, Robert (1983), Algorithms, Addison-Wesley, pp. 163\\u2013165, ISBN 0-201-06672-6\\n\\n\\n== External links ==\"}, {\"Pre-topological order\": \"In the field of computer science, a pre-topological order or pre-topological ordering of a directed graph is a linear ordering of its vertices such that if there is a directed path from vertex u to vertex v and v comes before u in the ordering, then there is also a directed path from vertex v to vertex u.If the graph is a directed acyclic graph (DAG), topological orderings are pre-topological orderings and vice versa. In other cases, any pre-topological ordering gives a partial order.\\n\\n\\n== References ==\"}, {\"Proportion extend sort\": \"Proportion extend sort (abbreviated as PESort) is an in-place, comparison-based sorting algorithm which attempts to improve on the performance, particularly the worst-case performance, of quicksort.\\nThe basic partitioning operation in quicksort has a linear access pattern which is extremely efficient on modern memory hierarchies, but the performance of the algorithm is critically dependent on the choice of a pivot value.  A good pivot will divide the data to be sorted into nearly-equal halves.  A poor choice will result in a grossly lopsided division, leaving one part almost as large as the original problem and causing O(n2) performance.\\nProportion extend sort begins with a sorted prefix of k elements, then uses the median of that sample to partition the following pk elements.  By bounding the size ratio p between the sample and the data being partitioned (i.e. the proportion by which the sorted prefix is extended), the imbalance is limited.  In this, it has some similarities to samplesort.\\n\\nHistory\\nProportion extend sort was published by Jing-Chao Chen in 2001 as an improvement on his earlier proportion split sort design.  Its average-case performance, which was only experimentally measured in the original paper, was analyzed by Richard Cole and David C. Kandathil in 2004 and by Chen in 2006, and shown to require log2n + O(n) comparisons on average.  A slightly refined variant, symmetry partition sort, was published in 2007.\\n\\nAlgorithm\\nThe algorithm begins with an array divided into a sorted part S adjacent to an unsorted part U.  (The original proportion extend sort always had the sorted part precede the unsorted part; the symmetric variant allows either order.)  It is possible to begin with the first element as the sorted part (a single element is always sorted), or to sort a small number of elements using a simpler insertion sort.  The initially sorted elements may also be taken from across the array to improve performance in the case of pre-sorted data.\\nNext, and most critically, the length of the unsorted part |U| is bounded to a multiple p of the length of the sorted part |S|.  Specifically, if |U| > p2|S|, then recursively sort S and the adjacent p|S| elements of U, make the result (p+1 times longer than the original) the new S, and repeat until the condition is satisfied.\\nIf there is no limit on the unsorted part (p=\\u221e), then the algorithm is equivalent to quicksort.  If the unsorted part is of length 1 (p=0, almost), then the algorithm is equivalent to binary insertion sort.  Values around p\\u224816 give the best average-case performance, competitive with quicksort,:\\u200a764\\u200a while smaller values improve worst-case performance.Eliezer Albacea published a similar algorithm in 1995 called Leapfrogging samplesort where the size is limited so |U| \\u2264 |S|+1, later generalized to (2k\\u22121)(|S|+1).The sorted part of the array is divided in half (at the median), and one half is moved (by exchanging it with unsorted elements) to the far end of the array, so we have an initial partially-partitioned array of the form LUR, where L is the left half of the sorted part, U is the bounded-length unsorted part, and R is the right half of the sorted part.\\nThen the standard quicksort partitioning step is performed on U, dividing it (in place) into UL and UR.  UL and UR are not sorted, but every element of UL is less than or equal to the median, and every element of UR is greater or equal.  The final result LULURR consists of two arrays of the necessary form (a sorted part adjacent to an unsorted part) and are sorted recursively.\\nLeapfrogging samplesort and the original proportion extend sort have the sorted part always precede the unsorted part, achieved by partitioning U before moving R, resulting in LRULUR, and then exchanging R with the end of UL, resulting in LULRUR.  While the symmetric version is a bit trickier, it has the advantage that the L and R parts act as sentinel values for the partitioning loops, eliminating the need to test in the loop if the bounds of U have been reached.[1]Most of the implementation refinements used for quicksort can be applied, including techniques for detecting and efficiently handling mostly-sorted inputs.  In particular, sub-sorts below a certain size threshold are usually implemented using a simple insertion sort.\\nAs with quicksort, the number of recursive levels can be limited to log2n if the smaller sub-sort is done first and the larger is implemented as a tail call.  Unlike quicksort, the number of levels is bounded by O(log n) even if this is not done.:\\u200a781\\n\\nNotes\\nReferences\\nExternal links\\nhttps://github.com/jingchaochen/Symmetry-Partition-Sort Example code\"}, {\"Proxmap sort\": \"ProxmapSort, or Proxmap sort, is a sorting algorithm that works by partitioning an array of data items, or keys, into a number of \\\"subarrays\\\" (termed buckets, in similar sorts). The name is short for computing a \\\"proximity map,\\\" which indicates for each key K the beginning of a subarray where K will reside in the final sorted order. Keys are placed into each subarray using insertion sort. If keys are \\\"well distributed\\\" among the subarrays, sorting occurs in linear time. The computational complexity estimates involve the number of subarrays and the proximity mapping function, the \\\"map key,\\\" used. It is a form of bucket and radix sort.\\nOnce a ProxmapSort is complete, ProxmapSearch can be used to find keys in the sorted array in \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   time if the keys were well distributed during the sort.\\nBoth algorithms were invented in the late 1980s by Prof. Thomas A. Standish at the University of California, Irvine.\\n\\nOverview\\nBasic strategy\\nIn general:\\nGiven an array A with n keys:\\n\\nmap a key to a subarray of the destination array A2, by applying the map key function to each array item\\ndetermine how many keys will map to the same subarray, using an array of \\\"hit counts,\\\" H\\ndetermine where each subarray will begin in the destination array so that each bucket is exactly the right size to hold all the keys that will map to it, using an array of \\\"proxmaps,\\\" P\\nfor each key, compute the subarray it will map to, using an array of \\\"locations,\\\" L\\nfor each key, look up its location, place it into that cell of A2; if it collides with a key already in that position, insertion sort the key into place, moving keys greater than this key to the right by one to make a space for this key. Since the subarray is big enough to hold all the keys mapped to it, such movement will never cause the keys to overflow into the following subarray.Simplied version:\\nGiven an array A with n keys\\n\\nInitialize: Create and initialize 2 arrays of n size: hitCount, proxMap, and 2 arrays of A.length: location, and A2.\\nPartition: Using a carefully chosen mapKey function, divide the A2 into subarrays using the keys in A\\nDisperse: Read over A, dropping each key into its bucket in A2; insertion sorting as needed.\\nCollect: Visit the subarrays in order and put all the elements back into the original array, or simply use A2.Note: \\\"keys\\\" may also contain other data, for instance an array of Student objects that contain the key plus a student ID and name. This makes ProxMapSort suitable for organizing groups of objects, not just keys themselves.\\n\\nExample\\nConsider a full array: A[0 to n-1] with n keys. Let i be an index of A. Sort A's keys into array A2 of equal size.\\nThe map key function is defined as mapKey(key) = floor(K).\\n\\nPseudocode\\nHere A is the array to be sorted and the mapKey functions determines the number of subarrays to use. For example, floor(K) will simply assign as many subarrays as there are integers from the data in A. Dividing the key by a constant reduces the number of subarrays; different functions can be used to translate the range of elements in A to subarrays, such as converting the letters A\\u2013Z to 0\\u201325 or returning the first character (0\\u2013255) for sorting strings. Subarrays are sorted as the data comes in, not after all data has been placed into the subarray, as is typical in bucket sorting.\\n\\nProxmap searching\\nProxmapSearch uses the proxMap array generated by a previously done ProxmapSort to find keys in the sorted array A2 in constant time.\\n\\nBasic strategy\\nSort the keys using ProxmapSort, keeping  the MapKey function, and the P and A2 arrays\\nTo search for a key, go to P[MapKey(k)], the start of the subarray that contains the key, if that key is in the data set\\nSequentially search the subarray; if the key  is found, return it (and associated information); if find a value greater than the key, the key is not in the data set\\nComputing P[MapKey(k)] takes \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   time. If a map key that gives a good distribution of keys was used during the sort, each subarray is bounded above by a constant c, so at most c comparisons are needed to find the key or know it is not present; therefore ProxmapSearch is \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n  . If the worst map key was used, all keys are in the same subarray, so ProxmapSearch, in this worst case, will require \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   comparisons.\\n\\nPseudocode\\nfunction mapKey(key) is\\n    return floor(key)\\n\\n    proxMap \\u2190 previously generated proxmap array of size n\\n    A2 \\u2190 previously sorted array of size n\\nfunction proxmap-search(key) is\\n    for i = proxMap[mapKey(key)] to length(array) \\u2212 1 do\\n        if sortedArray[i].key == key then\\n            return sortedArray[i]\\n\\nAnalysis\\nPerformance\\nComputing H, P, and L all take \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time. Each is computed with one pass through an array, with constant time spent at each array location.\\n\\nWorst case: MapKey places all items into one subarray, resulting in a standard insertion sort, and time of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  .\\nBest case: MapKey delivers the same small number of items to each subarray in an order where the best case of insertion sort occurs. Each insertion sort is \\n  \\n    \\n      \\n        O\\n        (\\n        c\\n        )\\n      \\n    \\n    {\\\\displaystyle O(c)}\\n  , c the size of the subarrays; there are p subarrays thus p * c = n, so the insertion phase take O(n); thus, ProxmapSort is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\nAverage case: Each subarray is at most size c, a constant; insertion sort for each subarray is then O(c^2) at worst \\u2013 a constant. (The actual time can be much better, since c items are not sorted until the last item is placed in the bucket). Total time is the number of buckets, (n/c), times \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          c\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(c^{2})}\\n   = \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .Having a good MapKey function is imperative for avoiding the worst case. We must know something about the distribution of the data to come up with a good key.\\n\\nOptimizations\\nSave time: Save the MapKey(i) values so they don't have to be recomputed (as they are in the code above)\\nSave space: The proxMaps can be stored in the hitCount array, as the hit counts are not needed once the proxmap is computed; the data can be sorted back into A, instead of using A2, if one takes care to note which A values have been sorted so far, and which not.JavaScript code implementation:\\n\\nComparison with other sorting algorithms\\nSince ProxmapSort is not a comparison sort, the \\u03a9(n log n) lower bound is inapplicable. Its speed can be attributed to it not being comparison-based and using arrays instead of dynamically allocated objects and pointers that must be followed, such as is done with when using a binary search tree.\\nProxmapSort allows for the use of ProxmapSearch. Despite the O(n) build time, ProxMapSearch makes up for it with its \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   average access time, making it very appealing for large databases. If the data doesn't need to be updated often, the access time may make this function more favorable than other non-comparison sorting based sorts.\\n\\nGeneric bucket sort related to ProxmapSort\\nLike ProxmapSort, bucket sort generally operates on a list of n numeric inputs between zero and some maximum key or value M and divides the value range into n buckets each of size M/n. If each bucket is sorted using insertion sort, ProxmapSort and bucket sort can be shown to run in predicted linear time. However, the performance of this sort degrades with clustering (or too few buckets with too many keys); if many values occur close together, they will all fall into a single bucket and performance will be severely diminished. This behavior also holds for ProxmapSort: if the buckets are too large, its performance will degrade severely.\\n\\nReferences\\nThomas A. Standish. Data Structures in Java. Addison Wesley Longman, 1998. ISBN 0-201-30564-X. Section 10.6, pp. 394\\u2013405.\\nStandish, T. A.; Jacobson, N. (2005). \\\"Using O(n) Proxmap Sort and O(1) Proxmap Search to motivate CS2 students (Part I)\\\". ACM SIGCSE Bulletin. 37 (4). doi:10.1145/1113847.1113874.\\nStandish, T. A.; Jacobson, N. (2006). \\\"Using O(n) Proxmap Sort and O(1) Proxmap Search to motivate CS2 students, Part II\\\". ACM SIGCSE Bulletin. 38 (2). doi:10.1145/1138403.1138427.\\nNorman Jacobson \\\"A Synopsis of ProxmapSort & ProxmapSearch\\\" from Department of Computer Science, Donald Bren School of Information and Computer Sciences, UC Irvine.\\n\\nExternal links\\nhttp://www.cs.uah.edu/~rcoleman/CS221/Sorting/ProxMapSort.html\\nhttps://web.archive.org/web/20120712094020/http://www.valdosta.edu/~sfares/cs330/cs3410.a.sorting.1998.fa.html\\nhttps://web.archive.org/web/20120314220616/http://www.cs.uml.edu/~giam/91.102/Demos/ProxMapSort/ProxMapSort.c\"}, {\"Qsort\": \"qsort is a C standard library function that implements a polymorphic sorting algorithm for arrays of arbitrary objects according to a user-provided comparison function. It is named after the \\\"quicker sort\\\" algorithm (a quicksort variant due to R. S. Scowen), which was originally used to implement it in the Unix C library, although the C standard does not require it to implement quicksort.Implementations of the qsort function achieve polymorphism, the ability to sort different kinds of data, by taking a function pointer to a three-way comparison function, as well as a parameter that specifies the size of its individual input objects. The C standard requires the comparison function to implement a total order on the items in the input array.\\n\\nHistory\\nA qsort function was implemented by Lee McMahon in 1972. It was in place in Version 3 Unix as a library function, but was then an assembler subroutine.A C version, with roughly the interface of the standard C version, was in place in Version 6 Unix. It was rewritten in 1983 for BSD. The function was standardized in ANSI C (1989).\\nIn 1991, Bell Labs employees observed that McMahon's and BSD versions of qsort would consume quadratic time for some simple inputs. Thus Jon Bentley and Douglas McIlroy engineered a new faster and more robust implementation. McIlroy would later produce a more complex quadratic-time input, termed AntiQuicksort, in 1998. This function constructs adversary data on-the-fly.\\n\\nExample\\nThe following piece of C code shows how to sort a list of integers using qsort.\\n\\nExtensions\\nSince the comparison function of the original qsort only accepts two pointers, passing in additional parameters (e.g. producing a comparison function that compares by the two value's difference with another value) must be done using global variables. The issue was solved by the BSD and GNU Unix-like systems by a introducing qsort_r function, which allows for an additional parameter to be passed to the comparison function. The two versions of qsort_r have different argument orders. C11 Annex K defines a qsort_s essentially identical to GNU's qsort_r. The macOS and FreeBSD libcs also contain qsort_b, a variant that uses blocks, an analogue to closures, as an alternate solution to the same problem.\\n\\n\\n== References ==\"}, {\"Quantum sort\": \"A quantum sort is any sorting algorithm that runs on a quantum computer. Any comparison-based quantum sorting algorithm would take at least \\n  \\n    \\n      \\n        \\u03a9\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Omega (n\\\\log n)}\\n   steps, which is already achievable by classical algorithms. Thus, for this task, quantum computers are no better than classical ones. However, in space-bounded sorts, quantum algorithms outperform their classical counterparts.\\n\\n\\n== References ==\"}, {\"Quicksort\": \"Quicksort is an efficient, general-purpose sorting algorithm. Quicksort was developed by British computer scientist Tony Hoare in 1959 and published in 1961. It is still a commonly used algorithm for sorting. Overall, it is slightly faster than merge sort and heapsort for randomized data, particularly on larger distributions.Quicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. For this reason, it is sometimes called partition-exchange sort. The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting.\\nQuicksort is a comparison sort, meaning that it can sort items of any type for which a \\\"less-than\\\" relation (formally, a total order) is defined. Most implementations of quicksort are not stable, meaning that the relative order of equal sort items is not preserved.\\nMathematical analysis of quicksort shows that, on average, the algorithm takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        \\n          n\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log {n})}\\n   comparisons to sort n items. In the worst case, it makes \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   comparisons.\\n\\nHistory\\nThe quicksort algorithm was developed in 1959 by Tony Hoare while he was a visiting student at Moscow State University. At that time, Hoare was working on a machine translation project for the National Physical Laboratory. As a part of the translation process, he needed to sort the words in Russian sentences before looking them up in a Russian-English dictionary, which was in alphabetical order on magnetic tape. After recognizing that his first idea, insertion sort, would be slow, he came up with a new idea. He wrote the partition part in Mercury Autocode but had trouble dealing with the list of unsorted segments. On return to England, he was asked to write code for Shellsort. Hoare mentioned to his boss that he knew of a faster algorithm and his boss bet a sixpence that he did not. His boss ultimately accepted that he had lost the bet. Later, Hoare learned about ALGOL and its ability to do recursion that enabled him to publish the code in Communications of the Association for Computing Machinery, the premier computer science journal of the time.Quicksort gained widespread adoption, appearing, for example, in Unix as the default library sort subroutine. Hence, it lent its name to the C standard library subroutine qsort and in the reference implementation of Java.\\nRobert Sedgewick's PhD thesis in 1975 is considered a milestone in the study of Quicksort where he resolved many open problems related to the analysis of various pivot selection schemes including Samplesort, adaptive partitioning by Van Emden as well as derivation of expected number of comparisons and swaps. Jon Bentley and Doug McIlroy in 1993 incorporated various improvements for use in programming libraries, including a technique to deal with equal elements and a pivot scheme known as pseudomedian of nine, where a sample of nine elements is divided into groups of three and then the median of the three medians from three groups is chosen. Bentley described another simpler and compact partitioning scheme in his book Programming Pearls that he attributed to Nico Lomuto. Later Bentley wrote that he used Hoare's version for years but never really understood it but Lomuto's version was simple enough to prove correct. Bentley described Quicksort as the \\\"most beautiful code I had ever written\\\" in the same essay. Lomuto's partition scheme was also popularized by the textbook Introduction to Algorithms although it is inferior to Hoare's scheme because it does three times more swaps on average and degrades to O(n2) runtime when all elements are equal. McIlroy would further produce an AntiQuicksort (aqsort) function in 1998, which consistently drives even his 1993 variant of Quicksort into quadratic behavior by producing adversarial data on-the-fly.\\n\\nAlgorithm\\nQuicksort is a type of divide and conquer algorithm for sorting an array, based on a partitioning routine; the details of this partitioning can vary somewhat, so that quicksort is really a family of closely related algorithms. Applied to a range of at least two elements, partitioning produces a division into two consecutive non empty sub-ranges, in such a way that no element of the first sub-range is greater than any element of the second sub-range. After applying this partition, quicksort then recursively sorts the sub-ranges, possibly after excluding from them an element at the point of division that is at this point known to be already in its final location. Due to its recursive nature, quicksort (like  the partition routine) has to be formulated so as to be callable for a range within a larger array, even if the ultimate goal is to sort a complete array. The steps for in-place quicksort are:\\n\\nIf the range has fewer than two elements, return immediately as there is nothing to do. Possibly for other very short lengths a special-purpose sorting method is applied and the remainder of these steps skipped.\\nOtherwise pick a value, called a pivot, that occurs in the range (the precise manner of choosing depends on the partition routine, and can involve randomness).\\nPartition the range: reorder its elements, while determining a point of division, so that all elements with values less than the pivot come before the division, while all elements with values greater than the pivot come after it; elements that are equal to the pivot can go either way. Since at least one instance of the pivot is present,  most partition routines ensure that the value that ends up at the point of division is equal to the pivot, and is now in its final position (but termination of quicksort does not depend on this, as long as sub-ranges strictly smaller than the original are produced).\\nRecursively apply the quicksort to the sub-range up to the point of division and to the sub-range after it, possibly excluding from both ranges the element equal to the pivot at the point of division. (If the partition produces a possibly larger sub-range near the boundary where all elements are known to be equal to the pivot, these can be excluded as well.)The choice of partition routine (including the pivot selection) and other details not entirely specified above can affect the algorithm's performance, possibly to a great extent for specific input arrays. In discussing the efficiency of quicksort, it is therefore necessary to specify these choices first. Here we mention two specific partition methods.\\n\\nLomuto partition scheme\\nThis scheme is attributed to Nico Lomuto and popularized by Bentley in his book Programming Pearls and Cormen et al. in their book Introduction to Algorithms. In most formulations this scheme chooses as the pivot the last element in the array. The algorithm maintains index i as it scans the array using another index j such that the elements at lo through i-1 (inclusive) are less than the pivot, and the elements at i through j (inclusive) are equal to or greater than the pivot. As this scheme is more compact and easy to understand, it is frequently used in introductory material, although it is less efficient than Hoare's original scheme e.g., when all elements are equal. The complexity of Quicksort with this scheme degrades to O(n2) when the array is already in order, due to the partition being the worst possible one. There have been various variants proposed to boost performance including various ways to select the pivot, deal with equal elements, use other sorting algorithms such as insertion sort for small arrays, and so on. In pseudocode, a quicksort that sorts elements at lo through hi (inclusive) of an array A can be expressed as:\\n// Sorts a (portion of an) array, divides it into partitions, then sorts those\\nalgorithm quicksort(A, lo, hi) is \\n  // Ensure indices are in correct order\\n  if lo >= hi || lo < 0 then \\n    return\\n    \\n  // Partition array and get the pivot index\\n  p := partition(A, lo, hi) \\n      \\n  // Sort the two partitions\\n  quicksort(A, lo, p - 1) // Left side of pivot\\n  quicksort(A, p + 1, hi) // Right side of pivot\\n\\n// Divides array into two partitions\\nalgorithm partition(A, lo, hi) is \\n  pivot := A[hi] // Choose the last element as the pivot\\n\\n  // Temporary pivot index\\n  i := lo - 1\\n\\n  for j := lo to hi - 1 do \\n    // If the current element is less than or equal to the pivot\\n    if A[j] <= pivot then \\n      // Move the temporary pivot index forward\\n      i := i + 1\\n      // Swap the current element with the element at the temporary pivot index\\n      swap A[i] with A[j]\\n\\n  // Move the pivot element to the correct pivot position (between the smaller and larger elements)\\n  i := i + 1\\n  swap A[i] with A[hi]\\n  return i // the pivot index\\n\\nSorting the entire array is accomplished by quicksort(A, 0, length(A) - 1).\\n\\nHoare partition scheme\\nThe original partition scheme described by Tony Hoare uses two pointers (indices into the range) that start at both ends of the array being partitioned, then move toward each other, until they detect an inversion: a pair of elements, one greater than the bound (Hoare's terms for the pivot value) at the first pointer, and one less than the bound at the second pointer; if at this point the first pointer is still before the second, these elements are in the wrong order relative to each other, and they are then exchanged. After this the pointers are moved inwards, and the search for an inversion is repeated; when eventually the pointers cross (the first points after the second), no exchange is performed; a valid partition is found, with the point of division between the crossed pointers (any entries that might be strictly between the crossed pointers are equal to the pivot and can be excluded from both sub-ranges formed). With this formulation it is possible that one sub-range turns out to be the whole original range, which would prevent the algorithm from advancing. Hoare therefore stipulates that at the end, the sub-range containing the pivot element (which still is at its original position) can be decreased in size by excluding that pivot, after (if necessary) exchanging it with the sub-range element closest to the separation; thus, termination of quicksort is ensured.\\nWith respect to this original description, implementations often make minor but important variations. Notably, the scheme as presented below includes elements equal to the pivot among the candidates for an inversion (so \\\"greater than or equal\\\" and \\\"less than or equal\\\" tests are used instead of \\\"greater than\\\" and \\\"less than\\\" respectively; since the formulation uses do...while rather than repeat...until which is actually reflected by the use of strict comparison operators). While there is no reason to exchange elements equal to the bound, this change allows tests on the pointers themselves to be omitted, which are otherwise needed to ensure they do not run out of range. Indeed, since at least one instance of the pivot value is present in the range, the first advancement of either pointer cannot pass across this instance if an inclusive test is used; once an exchange is performed, these exchanged elements are now both strictly ahead of the pointer that found them, preventing that pointer from running off. (The latter is true independently of the test used, so it would be possible to use the inclusive test only when looking for the first inversion. However, using an inclusive test throughout also ensures that a division near the middle is found when all elements in the range are equal, which gives an important efficiency gain for sorting arrays with many equal elements.) The risk of producing a non-advancing separation is avoided in a different manner than described by Hoare. Such a separation can only result when no inversions are found, with both pointers advancing to the pivot element at the first iteration (they are then considered to have crossed, and no exchange takes place). The division returned is after the final position of the second pointer, so the case to avoid is where the pivot is the final element of the range and all others are smaller than it. Therefore, the pivot choice must avoid the final element (in Hoare's description it could be any element in the range); this is done here by rounding down the middle position, using the floor function. This illustrates that the argument for correctness of an implementation of the Hoare partition scheme can be subtle, and it is easy to get it wrong.\\nIn pseudocode,\\n// Sorts a (portion of an) array, divides it into partitions, then sorts those\\nalgorithm quicksort(A, lo, hi) is \\n  if lo >= 0 && hi >= 0 && lo < hi then\\n    p := partition(A, lo, hi) \\n    quicksort(A, lo, p) // Note: the pivot is now included\\n    quicksort(A, p + 1, hi) \\n\\n// Divides array into two partitions\\nalgorithm partition(A, lo, hi) is \\n  // Pivot value\\n  pivot := A[ floor((hi - lo)/2) + lo ] // The value in the middle of the array\\n\\n  // Left index\\n  i := lo - 1 \\n\\n  // Right index\\n  j := hi + 1\\n\\n  loop forever \\n    // Move the left index to the right at least once and while the element at\\n    // the left index is less than the pivot\\n    do i := i + 1 while A[i] < pivot\\n    \\n    // Move the right index to the left at least once and while the element at\\n    // the right index is greater than the pivot\\n    do j := j - 1 while A[j] > pivot\\n\\n    // If the indices crossed, return\\n    if i >= j then return j\\n    \\n    // Swap the elements at the left and right indices\\n    swap A[i] with A[j]\\n\\nThe entire array is sorted by quicksort(A, 0, length(A) - 1).\\nHoare's scheme is more efficient than Lomuto's partition scheme because it does three times fewer swaps on average. Also, as mentioned, the implementation given creates a balanced partition even when all values are equal., which Lomuto's scheme does not. Like Lomuto's partition scheme, Hoare's partitioning also would cause Quicksort to degrade to O(n2) for already sorted input, if the pivot was chosen as the first or the last element. With the middle element as the pivot, however, sorted data results with (almost) no swaps in equally sized partitions leading to best case behavior of Quicksort, i.e. O(n log(n)). Like others, Hoare's partitioning doesn't produce a stable sort. In this scheme, the pivot's final location is not necessarily at the index that is returned, as the pivot and elements equal to the pivot can end up anywhere within the partition after a partition step, and may not be sorted until the base case of a partition with a single element is reached via recursion. The next two segments that the main algorithm recurs on are (lo..p) (elements \\u2264 pivot) and (p+1..hi) (elements \\u2265 pivot) as opposed to (lo..p-1) and (p+1..hi) as in Lomuto's scheme.Subsequent recursions (expansion on previous paragraph)\\nLet's expand a little bit on the next two segments that the main algorithm recurs on. Because we are using strict comparators (>, <) in the \\\"do...while\\\" loops to prevent ourselves from running out of range, there's a chance that the pivot itself gets swapped with other elements in the partition function. Therefore, the index returned in the partition function isn't necessarily where the actual pivot is. Consider the example of [5, 2, 3, 1, 0], following the scheme, after the first partition the array becomes [0, 2, 1, 3, 5], the \\\"index\\\" returned is 2, which is the number 1, when the real pivot, the one we chose to start the partition with was the number 3. With this example, we see how it is necessary to include the returned index of the partition function in our subsequent recursions. As a result, we are presented with the choices of either recursing on (lo..p) and (p+1..hi), or (lo..p - 1) and (p..hi). Which of the two options we choose depends on which index (i or j) we return in the partition function when the indices cross, and how we choose our pivot in the partition function (floor v.s. ceiling).\\nLet's first examine the choice of recursing on (lo..p) and (p+1..hi), with the example of sorting an array where multiple identical elements exist [0, 0]. If index i (the \\\"latter\\\" index) is returned after indices cross in the partition function, the index 1 would be returned after the first partition. The subsequent recursion on (lo..p)would be on (0, 1), which corresponds to the exact same array [0, 0]. A non-advancing separation that causes infinite recursion is produced. It is therefore obvious that when recursing on (lo..p) and (p+1..hi), because the left half of the recursion includes the returned index, it is the partition function's job to exclude the \\\"tail\\\" in non-advancing scenarios. Which is to say, index j (the \\\"former\\\" index when indices cross) should be returned instead of i. Going with a similar logic, when considering the example of an already sorted array [0, 1], the choice of pivot needs to be \\\"floor\\\" to ensure that the pointers stop on the \\\"former\\\" instead of the \\\"latter\\\" (with \\\"ceiling\\\" as the pivot, the index 1 would be returned and included in (lo..p) causing infinite recursion). It is for the exact same reason why choice of the last element as pivot must be avoided.\\nThe choice of recursing on (lo..p - 1) and (p..hi) follows the exact same logic as above. Because the right half of the recursion includes the returned index, it is the partition function's job to exclude the \\\"head\\\" in non-advancing scenarios. The index i (the \\\"latter\\\" index after the indices cross) in the partition function needs to be returned, and \\\"ceiling\\\" needs to be chosen as the pivot. The two nuances are clear, again, when considering the examples of sorting an array where multiple identical elements exist ([0, 0]), and an already sorted array [0, 1] respectively. It is noteworthy that with version of recursion, for the same reason, choice of the first element as pivot must be avoided.\\n\\nImplementation issues\\nChoice of pivot\\nIn the very early versions of quicksort, the leftmost element of the partition would often be chosen as the pivot element. Unfortunately, this causes worst-case behavior on already sorted arrays, which is a rather common use-case. The problem was easily solved by choosing either a random index for the pivot, choosing the middle index of the partition or (especially for longer partitions) choosing the median of the first, middle and last element of the partition for the pivot (as recommended by Sedgewick). This \\\"median-of-three\\\" rule counters the case of sorted (or reverse-sorted) input, and gives a better estimate of the optimal pivot (the true median) than selecting any single element, when no information about the ordering of the input is known.\\nMedian-of-three code snippet for Lomuto partition:\\n\\nmid := \\u230a(lo + hi) / 2\\u230b\\nif A[mid] < A[lo]\\n    swap A[lo] with A[mid]\\nif A[hi] < A[lo]\\n    swap A[lo] with A[hi]\\nif A[mid] < A[hi]\\n    swap A[mid] with A[hi]\\npivot := A[hi]\\n\\nIt puts a median into A[hi] first, then that new value of A[hi] is used for a pivot, as in a basic algorithm presented above.\\nSpecifically, the expected number of comparisons needed to sort n elements (see \\u00a7 Analysis of randomized quicksort) with random pivot selection is 1.386 n log n. Median-of-three pivoting brings this down to Cn, 2 \\u2248 1.188 n log n, at the expense of a three-percent increase in the expected number of swaps. An even stronger pivoting rule, for larger arrays, is to pick the ninther, a recursive median-of-three (Mo3), defined as\\nninther(a) = median(Mo3(first 1/3 of a), Mo3(middle 1/3 of a), Mo3(final 1/3 of a))Selecting a pivot element is also complicated by the existence of integer overflow. If the boundary indices of the subarray being sorted are sufficiently large, the na\\u00efve expression for the middle index, (lo + hi)/2, will cause overflow and provide an invalid pivot index. This can be overcome by using, for example, lo + (hi\\u2212lo)/2 to index the middle element, at the cost of more complex arithmetic. Similar issues arise in some other methods of selecting the pivot element.\\n\\nRepeated elements\\nWith a partitioning algorithm such as the Lomuto partition scheme described above (even one that chooses good pivot values), quicksort exhibits poor performance for inputs that contain many repeated elements. The problem is clearly apparent when all the input elements are equal: at each recursion, the left partition is empty (no input values are less than the pivot), and the right partition has only decreased by one element (the pivot is removed). Consequently, the Lomuto partition scheme takes quadratic time to sort an array of equal values. However, with a partitioning algorithm such as the Hoare partition scheme, repeated elements generally results in better partitioning, and although needless swaps of elements equal to the pivot may occur, the running time generally decreases as the number of repeated elements increases (with memory cache reducing the swap overhead). In the case where all elements are equal, Hoare partition scheme needlessly swaps elements, but the partitioning itself is best case, as noted in the Hoare partition section above.\\nTo solve the Lomuto partition scheme problem (sometimes called the Dutch national flag problem), an alternative linear-time partition routine can be used that separates the values into three groups: values less than the pivot, values equal to the pivot, and values greater than the pivot. (Bentley and McIlroy call this a \\\"fat partition\\\" and it was already implemented in the qsort of Version 7 Unix.) The values equal to the pivot are already sorted, so only the less-than and greater-than partitions need to be recursively sorted. In pseudocode, the quicksort algorithm becomes\\n\\nalgorithm quicksort(A, lo, hi) is\\n    if lo < hi then\\n        p := pivot(A, lo, hi)\\n        left, right := partition(A, p, lo, hi)  // note: multiple return values\\n        quicksort(A, lo, left - 1)\\n        quicksort(A, right + 1, hi)\\n\\nThe partition algorithm returns indices to the first ('leftmost') and to the last ('rightmost') item of the middle partition. Every item of the partition is equal to p and is therefore sorted. Consequently, the items of the partition need not be included in the recursive calls to quicksort.\\nThe best case for the algorithm now occurs when all elements are equal (or are chosen from a small set of k \\u226a n elements). In the case of all equal elements, the modified quicksort will perform only two recursive calls on empty subarrays and thus finish in linear time (assuming the partition subroutine takes no longer than linear time).\\n\\nOptimizations\\nTwo other important optimizations, also suggested by Sedgewick and widely used in practice, are:\\nTo make sure at most O(log n) space is used, recur first into the smaller side of the partition, then use a tail call to recur into the other, or update the parameters to no longer include the now sorted smaller side, and iterate to sort the larger side.\\nWhen the number of elements is below some threshold (perhaps ten elements), switch to a non-recursive sorting algorithm such as insertion sort that performs fewer swaps, comparisons or other operations on such small arrays. The ideal 'threshold' will vary based on the details of the specific implementation.\\nAn older variant of the previous optimization: when the number of elements is less than the threshold k, simply stop; then after the whole array has been processed, perform insertion sort on it. Stopping the recursion early leaves the array k-sorted, meaning that each element is at most k positions away from its final sorted position. In this case, insertion sort takes O(kn) time to finish the sort, which is linear if k is a constant.:\\u200a117\\u200a Compared to the \\\"many small sorts\\\" optimization, this version may execute fewer instructions, but it makes suboptimal use of the cache memories in modern computers.\\n\\nParallelization\\nQuicksort's divide-and-conquer formulation makes it amenable to parallelization using task parallelism. The partitioning step is accomplished through the use of a parallel prefix sum algorithm to compute an index for each array element in its section of the partitioned array. Given an array of size n, the partitioning step performs O(n) work in O(log n) time and requires O(n) additional scratch space. After the array has been partitioned, the two partitions can be sorted recursively in parallel. Assuming an ideal choice of pivots, parallel quicksort sorts an array of size n in O(n log n) work in O(log2 n) time using O(n) additional space.\\nQuicksort has some disadvantages when compared to alternative sorting algorithms, like merge sort, which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, it is difficult to parallelize the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.\\nOther more sophisticated parallel sorting algorithms can achieve even better time bounds. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW (concurrent read and concurrent write) PRAM (parallel random-access machine) with n processors by performing partitioning implicitly.\\n\\nFormal analysis\\nWorst-case analysis\\nThe most unbalanced partition occurs when one of the sublists returned by the partitioning routine is of size n \\u2212 1. This may occur if the pivot happens to be the smallest or largest element in the list, or in some implementations (e.g., the Lomuto partition scheme as described above) when all the elements are equal.\\nIf this happens repeatedly in every partition, then each recursive call processes a list of size one less than the previous list. Consequently, we can make n \\u2212 1 nested calls before we reach a list of size 1. This means that the call tree is a linear chain of n \\u2212 1 nested calls. The ith call does O(n \\u2212 i) work to do the partition, and \\n  \\n    \\n      \\n        \\n          \\n            \\u2211\\n            \\n              i\\n              =\\n              0\\n            \\n            \\n              n\\n            \\n          \\n          (\\n          n\\n          \\u2212\\n          i\\n          )\\n          =\\n          O\\n          (\\n          \\n            n\\n            \\n              2\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle \\\\sum _{i=0}^{n}(n-i)=O(n^{2})}\\n  , so in that case quicksort takes O(n2) time.\\n\\nBest-case analysis\\nIn the most balanced case, each time we perform a partition we divide the list into two nearly equal pieces. This means each recursive call processes a list of half the size. Consequently, we can make only log2 n nested calls before we reach a list of size 1. This means that the depth of the call tree is log2 n. But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only O(n) time all together (each call has some constant overhead, but since there are only O(n) calls at each level, this is subsumed in the O(n) factor). The result is that the algorithm uses only O(n log n) time.\\n\\nAverage-case analysis\\nTo sort an array of n distinct elements, quicksort takes O(n log n) time in expectation, averaged over all n! permutations of n elements with equal probability. Alternatively, if the algorithm selects the pivot uniformly at random from the input array, the same analysis can be used to bound the expected running time for any input sequence; the expectation is then take over the random choices made by the algorithm (Cormen et al., Introduction to Algorithms, Section 7.3).\\nWe list here three common proofs to this claim providing different insights into quicksort's workings.\\n\\nUsing percentiles\\nIf each pivot has rank somewhere in the middle 50 percent, that is, between the 25th percentile and the 75th percentile, then it splits the elements with at least 25% and at most 75% on each side. If we could consistently choose such pivots, we would only have to split the list at most \\n  \\n    \\n      \\n        \\n          log\\n          \\n            4\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        \\u2061\\n        n\\n      \\n    \\n    {\\\\displaystyle \\\\log _{4/3}n}\\n   times before reaching lists of size 1, yielding an O(n log n) algorithm.\\nWhen the input is a random permutation, the pivot has a random rank, and so it is not guaranteed to be in the middle 50 percent. However, when we start from a random permutation, in each recursive call the pivot has a random rank in its list, and so it is in the middle 50 percent about half the time. That is good enough. Imagine that a coin is flipped: heads means that the rank of the pivot is in the middle 50 percent, tail means that it isn't. Now imagine that the coin is flipped over and over until it gets k heads. Although this could take a long time, on average only 2k flips are required, and the chance that the coin won't get k heads after 100k flips is highly improbable (this can be made rigorous using Chernoff bounds). By the same argument, Quicksort's recursion will terminate on average at a call depth of only \\n  \\n    \\n      \\n        2\\n        \\n          log\\n          \\n            4\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        \\u2061\\n        n\\n      \\n    \\n    {\\\\displaystyle 2\\\\log _{4/3}n}\\n  . But if its average call depth is O(log n), and each level of the call tree processes at most n elements, the total amount of work done on average is the product, O(n log n). The algorithm does not have to verify that the pivot is in the middle half\\u2014if we hit it any constant fraction of the times, that is enough for the desired complexity.\\n\\nUsing recurrences\\nAn alternative approach is to set up a recurrence relation for the T(n) factor, the time needed to sort a list of size n. In the most unbalanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size 0 and n\\u22121, so the recurrence relation is\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        =\\n        O\\n        (\\n        n\\n        )\\n        +\\n        T\\n        (\\n        0\\n        )\\n        +\\n        T\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        =\\n        O\\n        (\\n        n\\n        )\\n        +\\n        T\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle T(n)=O(n)+T(0)+T(n-1)=O(n)+T(n-1).}\\n  This is the same relation as for insertion sort and selection sort, and it solves to worst case T(n) = O(n2).\\nIn the most balanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size n/2, so the recurrence relation is\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        =\\n        O\\n        (\\n        n\\n        )\\n        +\\n        2\\n        T\\n        \\n          (\\n          \\n            \\n              n\\n              2\\n            \\n          \\n          )\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle T(n)=O(n)+2T\\\\left({\\\\frac {n}{2}}\\\\right).}\\n  The master theorem for divide-and-conquer recurrences tells us that T(n) = O(n log n).\\nThe outline of a formal proof of the O(n log n) expected time complexity follows. Assume that there are no duplicates as duplicates could be handled with linear time pre- and post-processing, or considered cases easier than the analyzed. When the input is a random permutation, the rank of the pivot is uniform random from 0 to n \\u2212 1. Then the resulting parts of the partition have sizes i and n \\u2212 i \\u2212 1, and i is uniform random from 0 to n \\u2212 1. So, averaging over all possible splits and noting that the number of comparisons for the partition is n \\u2212 1, the average number of comparisons over all permutations of the input sequence can be estimated accurately by solving the recurrence relation:\\n\\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n        =\\n        n\\n        \\u2212\\n        1\\n        +\\n        \\n          \\n            1\\n            n\\n          \\n        \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        (\\n        C\\n        (\\n        i\\n        )\\n        +\\n        C\\n        (\\n        n\\n        \\u2212\\n        i\\n        \\u2212\\n        1\\n        )\\n        )\\n        =\\n        n\\n        \\u2212\\n        1\\n        +\\n        \\n          \\n            2\\n            n\\n          \\n        \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        C\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle C(n)=n-1+{\\\\frac {1}{n}}\\\\sum _{i=0}^{n-1}(C(i)+C(n-i-1))=n-1+{\\\\frac {2}{n}}\\\\sum _{i=0}^{n-1}C(i)}\\n  \\n\\n  \\n    \\n      \\n        n\\n        C\\n        (\\n        n\\n        )\\n        =\\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        +\\n        2\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        C\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle nC(n)=n(n-1)+2\\\\sum _{i=0}^{n-1}C(i)}\\n  \\n  \\n    \\n      \\n        n\\n        C\\n        (\\n        n\\n        )\\n        \\u2212\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        C\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        =\\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\u2212\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        (\\n        n\\n        \\u2212\\n        2\\n        )\\n        +\\n        2\\n        C\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle nC(n)-(n-1)C(n-1)=n(n-1)-(n-1)(n-2)+2C(n-1)}\\n  \\n  \\n    \\n      \\n        n\\n        C\\n        (\\n        n\\n        )\\n        =\\n        (\\n        n\\n        +\\n        1\\n        )\\n        C\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        +\\n        2\\n        n\\n        \\u2212\\n        2\\n      \\n    \\n    {\\\\displaystyle nC(n)=(n+1)C(n-1)+2n-2}\\n  \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      )\\n                    \\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      \\u2212\\n                      1\\n                      )\\n                    \\n                    n\\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      (\\n                      n\\n                      +\\n                      1\\n                      )\\n                    \\n                  \\n                \\n                \\u2264\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      \\u2212\\n                      1\\n                      )\\n                    \\n                    n\\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      \\u2212\\n                      2\\n                      )\\n                    \\n                    \\n                      n\\n                      \\u2212\\n                      1\\n                    \\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    n\\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    2\\n                    \\n                      (\\n                      n\\n                      \\u2212\\n                      1\\n                      )\\n                      n\\n                    \\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n                \\u2264\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      \\u2212\\n                      2\\n                      )\\n                    \\n                    \\n                      n\\n                      \\u2212\\n                      1\\n                    \\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    n\\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                 \\n                 \\n                \\u22ee\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      1\\n                      )\\n                    \\n                    2\\n                  \\n                \\n                +\\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    2\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    2\\n                    \\n                      i\\n                      +\\n                      1\\n                    \\n                  \\n                \\n                \\u2264\\n                2\\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                    \\u2212\\n                    1\\n                  \\n                \\n                \\n                  \\n                    1\\n                    i\\n                  \\n                \\n                \\u2248\\n                2\\n                \\n                  \\u222b\\n                  \\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    1\\n                    x\\n                  \\n                \\n                \\n                  d\\n                \\n                x\\n                =\\n                2\\n                ln\\n                \\u2061\\n                n\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\frac {C(n)}{n+1}}&={\\\\frac {C(n-1)}{n}}+{\\\\frac {2}{n+1}}-{\\\\frac {2}{n(n+1)}}\\\\leq {\\\\frac {C(n-1)}{n}}+{\\\\frac {2}{n+1}}\\\\\\\\&={\\\\frac {C(n-2)}{n-1}}+{\\\\frac {2}{n}}-{\\\\frac {2}{(n-1)n}}+{\\\\frac {2}{n+1}}\\\\leq {\\\\frac {C(n-2)}{n-1}}+{\\\\frac {2}{n}}+{\\\\frac {2}{n+1}}\\\\\\\\&\\\\ \\\\ \\\\vdots \\\\\\\\&={\\\\frac {C(1)}{2}}+\\\\sum _{i=2}^{n}{\\\\frac {2}{i+1}}\\\\leq 2\\\\sum _{i=1}^{n-1}{\\\\frac {1}{i}}\\\\approx 2\\\\int _{1}^{n}{\\\\frac {1}{x}}\\\\mathrm {d} x=2\\\\ln n\\\\end{aligned}}}\\n  Solving the recurrence gives C(n) = 2n ln n \\u2248 1.39n log2 n.\\nThis means that, on average, quicksort performs only about 39% worse than in its best case. In this sense, it is closer to the best case than the worst case. A comparison sort cannot use less than log2(n!) comparisons on average to sort n items (as explained in the article Comparison sort) and in case of large n, Stirling's approximation yields log2(n!) \\u2248 n(log2 n \\u2212 log2 e), so quicksort is not much worse than an ideal comparison sort. This fast average runtime is another reason for quicksort's practical dominance over other sorting algorithms.\\n\\nUsing a binary search tree\\nThe following binary search tree (BST) corresponds to each execution of quicksort: the initial pivot is the root node; the pivot of the left half is the root of the left subtree, the pivot of the right half is the root of the right subtree, and so on. The number of comparisons of the execution of quicksort equals the number of comparisons during the construction of the BST by a sequence of insertions. So, the average number of comparisons for randomized quicksort equals the average cost of constructing a BST when the values inserted \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},\\\\ldots ,x_{n})}\\n   form a random permutation.\\nConsider a BST created by insertion of a sequence \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},\\\\ldots ,x_{n})}\\n   of values forming a random permutation. Let C denote the cost of creation of the BST. We have \\n  \\n    \\n      \\n        C\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        \\n          \\u2211\\n          \\n            j\\n            <\\n            i\\n          \\n        \\n        \\n          c\\n          \\n            i\\n            ,\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C=\\\\sum _{i}\\\\sum _{j<i}c_{i,j}}\\n  , where \\n  \\n    \\n      \\n        \\n          c\\n          \\n            i\\n            ,\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{i,j}}\\n   is a binary random variable expressing whether during the insertion of \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   there was a comparison to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}}\\n  .\\nBy linearity of expectation, the expected value \\n  \\n    \\n      \\n        E\\n        \\u2061\\n        [\\n        C\\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {E} [C]}\\n   of C is \\n  \\n    \\n      \\n        E\\n        \\u2061\\n        [\\n        C\\n        ]\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        \\n          \\u2211\\n          \\n            j\\n            <\\n            i\\n          \\n        \\n        Pr\\n        (\\n        \\n          c\\n          \\n            i\\n            ,\\n            j\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {E} [C]=\\\\sum _{i}\\\\sum _{j<i}\\\\Pr(c_{i,j})}\\n  .\\nFix i and j<i. The values \\n  \\n    \\n      \\n        \\n          \\n            x\\n            \\n              1\\n            \\n          \\n          ,\\n          \\n            x\\n            \\n              2\\n            \\n          \\n          ,\\n          \\u2026\\n          ,\\n          \\n            x\\n            \\n              j\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {x_{1},x_{2},\\\\ldots ,x_{j}}}\\n  , once sorted, define j+1 intervals. The core structural observation is that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is compared to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}}\\n   in the algorithm if and only if \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   falls inside one of the two intervals adjacent to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}}\\n  .\\nObserve that since \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},\\\\ldots ,x_{n})}\\n   is a random permutation, \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},\\\\ldots ,x_{j},x_{i})}\\n   is also a random permutation, so the probability that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is adjacent to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}}\\n   is exactly \\n  \\n    \\n      \\n        \\n          \\n            2\\n            \\n              j\\n              +\\n              1\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {2}{j+1}}}\\n  .\\nWe end with a short calculation:\\n\\n  \\n    \\n      \\n        E\\n        \\u2061\\n        [\\n        C\\n        ]\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        \\n          \\u2211\\n          \\n            j\\n            <\\n            i\\n          \\n        \\n        \\n          \\n            2\\n            \\n              j\\n              +\\n              1\\n            \\n          \\n        \\n        =\\n        O\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n              \\n            \\n            log\\n            \\u2061\\n            i\\n          \\n          )\\n        \\n        =\\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {E} [C]=\\\\sum _{i}\\\\sum _{j<i}{\\\\frac {2}{j+1}}=O\\\\left(\\\\sum _{i}\\\\log i\\\\right)=O(n\\\\log n).}\\n\\nSpace complexity\\nThe space used by quicksort depends on the version used.\\nThe in-place version of quicksort has a space complexity of O(log n), even in the worst case, when it is carefully implemented using the following strategies.\\n\\nIn-place partitioning is used. This unstable partition requires O(1) space.\\nAfter partitioning, the partition with the fewest elements is (recursively) sorted first, requiring at most O(log n) space. Then the other partition is sorted using tail recursion or iteration, which doesn't add to the call stack. This idea, as discussed above, was described by R. Sedgewick, and keeps the stack depth bounded by O(log n).Quicksort with in-place and unstable partitioning uses only constant additional space before making any recursive call. Quicksort must store a constant amount of information for each nested recursive call. Since the best case makes at most O(log n) nested recursive calls, it uses O(log n) space. However, without Sedgewick's trick to limit the recursive calls, in the worst case quicksort could make O(n) nested recursive calls and need O(n) auxiliary space.\\nFrom a bit complexity viewpoint, variables such as lo and hi do not use constant space; it takes O(log n) bits to index into a list of n items. Because there are such variables in every stack frame, quicksort using Sedgewick's trick requires O((log n)2) bits of space. This space requirement isn't too terrible, though, since if the list contained distinct elements, it would need at least O(n log n) bits of space.\\nAnother, less common, not-in-place, version of quicksort uses O(n) space for working storage and can implement a stable sort. The working storage allows the input array to be easily partitioned in a stable manner and then copied back to the input array for successive recursive calls. Sedgewick's optimization is still appropriate.\\n\\nRelation to other algorithms\\nQuicksort is a space-optimized version of the binary tree sort. Instead of inserting items sequentially into an explicit tree, quicksort organizes them concurrently into a tree that is implied by the recursive calls. The algorithms make exactly the same comparisons, but in a different order. An often desirable property of a sorting algorithm is stability \\u2013 that is the order of elements that compare equal is not changed, allowing controlling order of multikey tables (e.g. directory or folder listings) in a natural way. This property is hard to maintain for in-place quicksort (that uses only constant additional space for pointers and buffers, and O(log n) additional space for the management of explicit or implicit recursion). For variant quicksorts involving extra memory due to representations using pointers (e.g. lists or trees) or files (effectively lists), it is trivial to maintain stability. The more complex, or disk-bound, data structures tend to increase time cost, in general making increasing use of virtual memory or disk.\\nThe most direct competitor of quicksort is heapsort. Heapsort's running time is O(n log n), but heapsort's average running time is usually considered slower than in-place quicksort. This result is debatable; some publications indicate the opposite. Introsort is a variant of quicksort that switches to heapsort when a bad case is detected to avoid quicksort's worst-case running time. Major programming languages, such as C++ (in the GNU and LLVM implementations), use introsort.Quicksort also competes with merge sort, another O(n log n) sorting algorithm. Standard merge sort is an out-of-place stable sort, unlike standard in-place quicksort and heapsort, and has excellent worst-case performance.  The main disadvantage of mergesort is that, when operating on arrays, efficient implementations require O(n) auxiliary space, whereas the variant of quicksort with in-place partitioning and tail recursion uses only O(log n) space.\\nMergesort works very well on linked lists, requiring only a small, constant amount of auxiliary storage.  Although quicksort can be implemented as a stable sort using linked lists, it will often suffer from poor pivot choices without random access.  Mergesort is also the algorithm of choice for external sorting of very large data sets stored on slow-to-access media such as disk storage or network-attached storage.\\nBucket sort with two buckets is very similar to quicksort; the pivot in this case is effectively the value in the middle of the value range, which does well on average for uniformly distributed inputs.\\n\\nSelection-based pivoting\\nA selection algorithm chooses the kth smallest of a list of numbers; this is an easier problem in general than sorting. One simple but effective selection algorithm works nearly in the same manner as quicksort, and is accordingly known as quickselect. The difference is that instead of making recursive calls on both sublists, it only makes a single tail-recursive call on the sublist that contains the desired element. This change lowers the average complexity to linear or O(n) time, which is optimal for selection, but the selection algorithm is still O(n2) in the worst case.\\nA variant of quickselect, the median of medians algorithm, chooses pivots more carefully, ensuring that the pivots are near the middle of the data (between the 30th and 70th percentiles), and thus has guaranteed linear time \\u2013 O(n). This same pivot strategy can be used to construct a variant of quicksort (median of medians quicksort) with O(n log n) time. However, the overhead of choosing the pivot is significant, so this is generally not used in practice.\\nMore abstractly, given an O(n) selection algorithm, one can use it to find the ideal pivot (the median) at every step of quicksort and thus produce a sorting algorithm with O(n log n) running time. Practical implementations of this variant are considerably slower on average, but they are of theoretical interest because they show an optimal selection algorithm can yield an optimal sorting algorithm.\\n\\nVariants\\nMulti-pivot quicksort\\nInstead of partitioning into two subarrays using a single pivot, multi-pivot quicksort (also multiquicksort) partitions its input into some s number of subarrays using s \\u2212 1 pivots. While the dual-pivot case (s = 3) was considered by Sedgewick and others already in the mid-1970s, the resulting algorithms were not faster in practice than the \\\"classical\\\" quicksort. A 1999 assessment of a multiquicksort with a variable number of pivots, tuned to make efficient use of processor caches, found it to increase the instruction count by some 20%, but simulation results suggested that it would be more efficient on very large inputs. A version of dual-pivot quicksort developed by Yaroslavskiy in 2009 turned out to be fast enough to warrant implementation in Java 7, as the standard algorithm to sort arrays of primitives (sorting arrays of objects is done using Timsort). The performance benefit of this algorithm was subsequently found to be mostly related to cache performance, and experimental results indicate that the three-pivot variant may perform even better on modern machines.\\n\\nExternal quicksort\\nFor disk files, an external sort based on partitioning similar to quicksort is possible. It is slower than external merge sort, but doesn't require extra disk space. 4 buffers are used, 2 for input, 2 for output. Let N = number of records in the file, B = the number of records per buffer, and M = N/B = the number of buffer segments in the file. Data is read (and written) from both ends of the file inwards. Let X represent the segments that start at the beginning of the file and Y represent segments that start at the end of the file. Data is read into the X and Y read buffers. A pivot record is chosen and the records in the X and Y buffers other than the pivot record are copied to the X write buffer in ascending order and Y write buffer in descending order based comparison with the pivot record. Once either X or Y buffer is filled, it is written to the file and the next X or Y buffer is read from the file. The process continues until all segments are read and one write buffer remains. If that buffer is an X write buffer, the pivot record is appended to it and the X buffer written. If that buffer is a Y write buffer, the pivot record is prepended to the Y buffer and the Y buffer written. This constitutes one partition step of the file, and the file is now composed of two subfiles. The start and end positions of each subfile are pushed/popped to a stand-alone stack or the main stack via recursion. To limit stack space to O(log2(n)), the smaller subfile is processed first. For a stand-alone stack, push the larger subfile parameters onto the stack, iterate on the smaller subfile. For recursion, recurse on the smaller subfile first, then iterate to handle the larger subfile. Once a sub-file is less than or equal to 4 B records, the subfile is sorted in-place via quicksort and written. That subfile is now sorted and in place in the file. The process is continued until all sub-files are sorted and in place. The average number of passes on the file is approximately 1 + ln(N+1)/(4 B), but worst case pattern is N passes (equivalent to O(n^2) for worst case internal sort).\\n\\nThree-way radix quicksort\\nThis algorithm is a combination of radix sort and quicksort. Pick an element from the array (the pivot) and consider the first character (key) of the string (multikey). Partition the remaining elements into three sets: those whose corresponding character is less than, equal to, and greater than the pivot's character. Recursively sort the \\\"less than\\\" and \\\"greater than\\\" partitions on the same character. Recursively sort the \\\"equal to\\\" partition by the next character (key). Given we sort using bytes or words of length W bits, the best case is O(KN) and the worst case O(2KN) or at least O(N2) as for standard quicksort, given for unique keys N<2K, and K is a hidden constant in all standard comparison sort algorithms including quicksort. This is a kind of three-way quicksort in which the middle partition represents a (trivially) sorted subarray of elements that are exactly equal to the pivot.\\n\\nQuick radix sort\\nAlso developed by Powers as an O(K) parallel PRAM algorithm. This is again a combination of radix sort and quicksort but the quicksort left/right partition decision is made on successive bits of the key, and is thus O(KN) for N K-bit keys. All comparison sort algorithms implicitly assume the transdichotomous model with K in \\u0398(log N), as if K is smaller we can sort in O(N) time using a hash table or integer sorting.  If K \\u226b log N but elements are unique within O(log N) bits, the remaining bits will not be looked at by either quicksort or quick radix sort.  Failing that, all comparison sorting algorithms will also have the same overhead of looking through O(K) relatively useless bits but quick radix sort will avoid the worst case O(N2) behaviours of standard quicksort and radix quicksort, and will be faster even in the best case of those comparison algorithms under these conditions of uniqueprefix(K) \\u226b log N. See Powers for further discussion of the hidden overheads in comparison, radix and parallel sorting.\\n\\nBlockQuicksort\\nIn any comparison-based sorting algorithm, minimizing the number of comparisons requires maximizing the amount of information gained from each comparison, meaning that the comparison results are unpredictable.  This causes frequent branch mispredictions, limiting performance. BlockQuicksort rearranges the computations of quicksort to convert unpredictable branches to data dependencies.  When partitioning, the input is divided into moderate-sized blocks (which fit easily into the data cache), and two arrays are filled with the positions of elements to swap.  (To avoid conditional branches, the position is unconditionally stored at the end of the array, and the index of the end is incremented if a swap is needed.) A second pass exchanges the elements at the positions indicated in the arrays.  Both loops have only one conditional branch, a test for termination, which is usually taken.\\nThe BlockQuicksort technique is incorporated into LLVM's C++ STL implementation, libcxx, providing a 50% improvement on random integer sequences. Pattern-defeating quicksort (pdqsort), a version of introsort, also incorporates this technique.\\n\\nPartial and incremental quicksort\\nSeveral variants of quicksort exist that separate the k smallest or largest elements from the rest of the input.\\n\\nGeneralization\\nRichard Cole and David C. Kandathil, in 2004, discovered a one-parameter family of sorting algorithms, called partition sorts, which on average (with all input orderings equally likely) perform at most \\n  \\n    \\n      \\n        n\\n        log\\n        \\u2061\\n        n\\n        +\\n        \\n          O\\n        \\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle n\\\\log n+{O}(n)}\\n   comparisons (close to the information theoretic lower bound) and \\n  \\n    \\n      \\n        \\n          \\u0398\\n        \\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\Theta }(n\\\\log n)}\\n   operations; at worst they perform \\n  \\n    \\n      \\n        \\n          \\u0398\\n        \\n        (\\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\Theta }(n\\\\log ^{2}n)}\\n   comparisons (and also operations); these are in-place, requiring only additional \\n  \\n    \\n      \\n        \\n          O\\n        \\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle {O}(\\\\log n)}\\n   space. Practical efficiency and smaller variance in performance were demonstrated against optimised quicksorts (of Sedgewick and Bentley-McIlroy).\\n\\nTradeoffs\\nQuicksort has a few tradeoffs that should be taken into consideration during implementation. For example, quicksort requires a lot of comparisons and it\\u2019s not stable since it swaps non-adjacent elements. Additionally, if the input is already sorted or reverse sorted, the pivot element will be one of the extreme elements causing the performance to degrade to \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          N\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(N^{2})}\\n  . As such, algorithms like Mergesort are a good choice if stability is important or the input is already partially sorted.\\n\\nUpdates on the latest research\\nThere is a new Quicksort algorithm which improves the worst time complexity from \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          N\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(N^{2})}\\n   to \\n  \\n    \\n      \\n        O\\n        (\\n        N\\n        l\\n        o\\n        g\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle O(NlogN)}\\n   by avoiding picking the maximum or minimum value as the pivot. \\nCode implemented in python:    quicksort(arr, lo, hi)\\n\\n   if len(arr) <= 100:\\n       insertion_sort(arr, hi-lo+1)\\n       return\\n   if len(arr) >= 500:\\n       arrange_element(arr, lo, hi)\\n       arrange_element(arr, lo, ((hi+lo)/2)-1)\\n       arrange_element(arr, (hi+lo)/2, hi)\\n  \\n   start = (hi-lo+1)*0.25+lo\\n   end = hi-(hi-lo+1)*0.25\\n \\n   pivot = arr[(start-end+1)/2]\\n   i = lo-1\\n   j = lo\\n   while j < hi:\\n       if arr[j] < pivot:\\n           i += 1\\n           arr[i], arr[j] = arr[j], arr[i]\\n       j += 1\\n   arr[i+1], arr[hi] = arr[hi], arr[i+1]\\n   pi = i+1\\n   quick_sort(arr, lo, pi-1)\\n   quick_sort(arr, pi+1, hi)\\n\\narrange_element(arr, lo, hi)\\n\\n   if (hi-lo+1) % 2 == 0:\\n       mid = (hi-lo+1)/2\\n       i, j = lo, mid\\n       while i < mid:\\n           if arr[i] > arr[j]:\\n               arr[i], arr[j] = arr[j], arr[i]\\n               i += 1\\n               j += 1\\n   else:\\n       mid = (hi-lo+1)/2\\n       i, j = lo, mid\\n       while i < mid:\\n           if arr[i] > arr[j]:\\n               arr[i], arr[j] = arr[j], arr[i]\\n               i += 1\\n               j += 1\\n       if arr[lo] > arr[hi]:\\n           arr[lo], arr[hi] = arr[hi], arr[lo]\\n\\ninsertion_sort(arr, n):\\n\\n   i = 1\\n   while i < n:\\n       key = arr[i]\\n       j = i-1\\n       while (j >= 0) and (arr[j] > key):\\n           arr[j+1] = arr[j]\\n           j -=1\\n       arr[j+1] = key\\n       i += 1\\n\\nSee also\\nIntrosort \\u2013 Hybrid sorting algorithm\\n\\nNotes\\nReferences\\nSedgewick, R. (1978). \\\"Implementing Quicksort programs\\\". Comm. ACM. 21 (10): 847\\u2013857. doi:10.1145/359619.359631. S2CID 10020756.\\nDean, B. C. (2006). \\\"A simple expected running time analysis for randomized 'divide and conquer' algorithms\\\". Discrete Applied Mathematics. 154: 1\\u20135. doi:10.1016/j.dam.2005.07.005.\\nHoare, C. A. R. (1961). \\\"Algorithm 63: Partition\\\". Comm. ACM. 4 (7): 321. doi:10.1145/366622.366642. S2CID 52800011.\\nHoare, C. A. R. (1961). \\\"Algorithm 65: Find\\\". Comm. ACM. 4 (7): 321\\u2013322. doi:10.1145/366622.366647.\\nHoare, C. A. R. (1962). \\\"Quicksort\\\". Comput. J. 5 (1): 10\\u201316. doi:10.1093/comjnl/5.1.10. (Reprinted in Hoare and Jones: Essays in computing science, 1989.)\\nMusser, David R. (1997). \\\"Introspective Sorting and Selection Algorithms\\\". Software: Practice and Experience. 27 (8): 983\\u2013993. doi:10.1002/(SICI)1097-024X(199708)27:8<983::AID-SPE117>3.0.CO;2-#.\\nDonald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Pages 113\\u2013122 of section 5.2.2: Sorting by Exchanging.\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Chapter 7: Quicksort, pp. 145\\u2013164.\\nFaron Moller. Analysis of Quicksort. CS 332: Designing Algorithms. Department of Computer Science, Swansea University.\\nMart\\u00ednez, C.; Roura, S. (2001). \\\"Optimal Sampling Strategies in Quicksort and Quickselect\\\". SIAM J. Comput. 31 (3): 683\\u2013705. CiteSeerX 10.1.1.17.4954. doi:10.1137/S0097539700382108.\\nBentley, J. L.; McIlroy, M. D. (1993). \\\"Engineering a sort function\\\". Software: Practice and Experience. 23 (11): 1249\\u20131265. CiteSeerX 10.1.1.14.8162. doi:10.1002/spe.4380231105. S2CID 8822797.\\n\\nExternal links\\n\\n\\\"Animated Sorting Algorithms: Quick Sort\\\". Archived from the original on 2 March 2015. Retrieved 25 November 2008. \\u2013 graphical demonstration\\n\\\"Animated Sorting Algorithms: Quick Sort (3-way partition)\\\". Archived from the original on 6 March 2015. Retrieved 25 November 2008.\\nOpen Data Structures \\u2013 Section 11.1.2 \\u2013 Quicksort, Pat Morin\\nInteractive illustration of Quicksort, with code walkthrough\"}, {\"Radix sort\": \"In computer science, radix sort is a non-comparative sorting algorithm. It avoids comparison by creating and distributing elements into buckets according to their radix. For elements with more than one significant digit, this bucketing process is repeated for each digit, while preserving the ordering of the prior step, until all digits have been considered. For this reason, radix sort has also been called bucket sort and digital sort.\\nRadix sort can be applied to data that can be sorted lexicographically, be they integers, words, punch cards, playing cards, or the mail.\\n\\nHistory\\nRadix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines. Radix sorting algorithms came into common use as a way to sort punched cards as early as 1923.The first memory-efficient computer algorithm for this sorting method was developed in 1954 at MIT by Harold H. Seward. Computerized radix sorts had previously been dismissed as impractical because of the perceived need for variable allocation of buckets of unknown size. Seward's innovation was to use a linear scan to determine the required bucket sizes and offsets beforehand, allowing for a single static allocation of auxiliary memory. The linear scan is closely related to Seward's other algorithm \\u2014 counting sort.\\nIn the modern era, radix sorts are most commonly applied to collections of binary strings and integers. It has been shown in some benchmarks to be faster than other more general-purpose sorting algorithms, sometimes 50% to three times faster.\\n\\nDigit order\\nRadix sorts can be implemented to start at either the most significant digit (MSD) or least significant digit (LSD). For example, with 1234, one could start with 1 (MSD) or 4 (LSD).\\nLSD radix sorts typically use the following sorting order: short keys come before longer keys, and then keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, like the sequence [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. LSD sorts are generally stable sorts.\\nMSD radix sorts are most suitable for sorting strings or fixed-length integer representations. A sequence like [b, c, e, d, f, g, ba] would be sorted as [b, ba, c, d, e, f, g]. If lexicographic ordering is used to sort variable-length integers in base 10, then numbers from 1 to 10 would be output as [1, 10, 2, 3, 4, 5, 6, 7, 8, 9], as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key. MSD sorts are not necessarily stable if the original ordering of duplicate keys must always be maintained.\\nOther than the traversal order, MSD and LSD sorts differ in their handling of variable length input.\\nLSD sorts can group by length, radix sort each group, then concatenate the groups in size order. MSD sorts must effectively 'extend' all shorter keys to the size of the largest key and sort them accordingly, which can be more complicated than the grouping required by LSD.\\nHowever, MSD sorts are more amenable to subdivision and recursion. Each bucket created by an MSD step can itself be radix sorted using the next most significant digit, without reference to any other buckets created in the previous step. Once the last digit is reached, concatenating the buckets is all that is required to complete the sort.\\n\\nExamples\\nLeast significant digit\\nInput list:\\n\\n[170, 45, 75, 90, 2, 802, 2, 66]Starting from the rightmost (last) digit, sort the numbers based on that digit:\\n\\n[{170, 90}, {2, 802, 2}, {45, 75}, {66}]Sorting by the next left digit:\\n\\n[{02, 802, 02}, {45}, {66}, {170, 75}, {90}]Notice that an implicit digit 0 is prepended for the two 2s so that 802 maintains its position between them.And finally by the leftmost digit:\\n\\n[{002, 002, 045, 066, 075, 090}, {170}, {802}]Notice that a 0 is prepended to all of the 1- or 2-digit numbers.Each step requires just a single pass over the data, since each item can be placed in its bucket without comparison with any other element.\\nSome radix sort implementations allocate space for buckets by first counting the number of keys that belong in each bucket before moving keys into those buckets. The number of times that each digit occurs is stored in an array.\\nAlthough it is always possible to pre-determine the bucket boundaries using counts, some implementations opt to use dynamic memory allocation instead.\\n\\nMost significant digit, forward recursive\\nInput list, fixed width numeric strings with leading zeros:\\n\\n[170, 045, 075, 025, 002, 024, 802, 066]First digit, with brackets indicating buckets:\\n\\n[{045, 075, 025, 002, 024, 066}, {170}, {802}]Notice that 170 and 802 are already complete because they are all that remain in their buckets, so no further recursion is neededNext digit:\\n\\n[{ {002}, {025, 024}, {045}, {066}, {075} }, 170, 802]Final digit:\\n\\n[ 002, { {024}, {025} }, 045, 066, 075 , 170, 802]All that remains is concatenation:\\n\\n[002, 024, 025, 045, 066, 075, 170, 802]\\n\\nComplexity and performance\\nRadix sort operates in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        w\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nw)}\\n   time, where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of keys, and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   is the key length. LSD variants can achieve a lower bound for \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   of 'average key length' when splitting variable length keys into groups as discussed above.\\nOptimized radix sorts can be very fast when working in a domain that suits them.\\nThey are constrained to lexicographic data, but for many practical applications this is not a limitation. Large key sizes can hinder LSD implementations when the induced number of passes becomes the bottleneck.\\n\\nSpecialized variants\\nIn-place MSD radix sort implementations\\nBinary MSD radix sort, also called binary quicksort, can be implemented in-place by splitting the input array into two bins - the 0s bin and the 1s bin. The 0s bin is grown from the beginning of the array, whereas the 1s bin is grown from the end of the array. The 0s bin boundary is placed before the first array element. The 1s bin boundary is placed after the last array element. The most significant bit of the first array element is examined. If this bit is a 1, then the first element is swapped with the element in front of the 1s bin boundary (the last element of the array), and the 1s bin is grown by one element by decrementing the 1s boundary array index. If this bit is a 0, then the first element remains at its current location, and the 0s bin is grown by one element. The next array element examined is the one in front of the 0s bin boundary (i.e. the first element that is not in the 0s bin or the 1s bin). This process continues until the 0s bin and the 1s bin reach each other. The 0s bin and the 1s bin are then sorted recursively based on the next bit of each array element. Recursive processing continues until the least significant bit has been used for sorting. Handling signed two's complement integers requires treating the most significant bit with the opposite sense, followed by unsigned treatment of the rest of the bits.\\nIn-place MSD binary-radix sort can be extended to larger radix and retain in-place capability. Counting sort is used to determine the size of each bin and their starting index. Swapping is used to place the current element into its bin, followed by expanding the bin boundary. As the array elements are scanned the bins are skipped over and only elements between bins are processed, until the entire array has been processed and all elements end up in their respective bins. The number of bins is the same as the radix used - e.g. 16 bins for 16-radix. Each pass is based on a single digit (e.g. 4-bits per digit in the case of 16-radix), starting from the most significant digit. Each bin is then processed recursively using the next digit, until all digits have been used for sorting.Neither in-place binary-radix sort nor n-bit-radix sort, discussed in paragraphs above, are stable algorithms.\\n\\nStable MSD radix sort implementations\\nMSD radix sort can be implemented as a stable algorithm, but requires the use of a memory buffer of the same size as the input array. This extra memory allows the input buffer to be scanned from the first array element to last, and move the array elements to the destination bins in the same order. Thus, equal elements will be placed in the memory buffer in the same order they were in the input array. The MSD-based algorithm uses the extra memory buffer as the output on the first level of recursion, but swaps the input and output on the next level of recursion, to avoid the overhead of copying the output result back to the input buffer. Each of the bins are recursively processed, as is done for the in-place MSD radix sort. After the sort by the last digit has been completed, the output buffer is checked to see if it is the original input array, and if it's not, then a single copy is performed. If the digit size is chosen such that the key size divided by the digit size is an even number, the copy at the end is avoided.\\n\\nHybrid approaches\\nRadix sort, such as the two-pass method where counting sort is used during the first pass of each level of recursion, has a large constant overhead. Thus, when the bins get small, other sorting algorithms should be used, such as insertion sort. A good implementation of insertion sort is fast for small arrays, stable, in-place, and can significantly speed up radix sort.\\n\\nApplication to parallel computing\\nThis recursive sorting algorithm has particular application to parallel computing, as each of the bins can be sorted independently. In this case, each bin is passed to the next available processor. A single processor would be used at the start (the most significant digit). By the second or third digit, all available processors would likely be engaged. Ideally, as each subdivision is fully sorted, fewer and fewer processors would be utilized. In the worst case, all of the keys will be identical or nearly identical to each other, with the result that there will be little to no advantage to using parallel computing to sort the keys.\\nIn the top level of recursion, opportunity for parallelism is in the counting sort portion of the algorithm. Counting is highly parallel, amenable to the parallel_reduce pattern, and splits the work well across multiple cores until reaching memory bandwidth limit. This portion of the algorithm has data-independent parallelism. Processing each bin in subsequent recursion levels is data-dependent, however. For example, if all keys were of the same value, then there would be only a single bin with any elements in it, and no parallelism would be available. For random inputs all bins would be near equally populated and a large amount of parallelism opportunity would be available.There are faster parallel sorting algorithms available, for example optimal complexity O(log(n)) are those of the Three Hungarians and Richard Cole and Batcher's bitonic merge sort has an algorithmic complexity of O(log2(n)), all of which have a lower algorithmic time complexity to radix sort on a CREW-PRAM. The fastest known PRAM sorts were described in 1991 by David Powers with a parallelized quicksort that can operate in O(log(n)) time on a CRCW-PRAM with n processors by performing partitioning implicitly, as well as a radixsort that operates using the same trick in O(k), where k is the maximum keylength. However, neither the PRAM architecture or a single sequential processor can actually be built in a way that will scale without the number of constant fan-out gate delays per cycle increasing as O(log(n)), so that in effect a pipelined version of Batcher's bitonic mergesort and the O(log(n)) PRAM sorts are all O(log2(n)) in terms of clock cycles, with Powers acknowledging that Batcher's would have lower constant in terms of gate delays than his Parallel quicksort and radix sort, or Cole's merge sort, for a keylength-independent sorting network of O(nlog2(n)).\\n\\nTree-based radix sort\\nRadix sorting can also be accomplished by building a tree (or radix tree) from the input set, and doing a pre-order traversal. This is similar to the relationship between heapsort and the heap data structure. This can be useful for certain data types, see burstsort.\\n\\nSee also\\nIBM 80 series Card Sorters\\nOther distribution sorts\\nKirkpatrick-Reisch sorting\\nPrefix sum\\n\\nReferences\\nExternal links\\n\\nExplanation, Pseudocode and implementation in C and Java\\nHigh Performance Implementation of LSD Radix sort in JavaScript\\nHigh Performance Implementation of LSD & MSD Radix sort in C# with source in GitHub\\nVideo tutorial of MSD Radix Sort\\nDemonstration and comparison of Radix sort with Bubble sort, Merge sort and Quicksort implemented in JavaScript\\nArticle about Radix sorting IEEE floating-point numbers with implementation.\\nFaster Floating Point Sorting and Multiple Histogramming with implementation in C++\\nPointers to radix sort visualizations\\nUSort library contains tuned implementations of radix sort for most numerical C types (C99)\\nDonald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Section 5.2.5: Sorting by Distribution, pp. 168\\u2013179.\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 8.3: Radix sort, pp. 170\\u2013173.\\nBRADSORT v1.50 source code\\nEfficient Trie-Based Sorting of Large Sets of Strings, by Ranjan Sinha and Justin Zobel. This paper describes a method of creating tries of buckets which figuratively burst into sub-tries when the buckets hold more than a predetermined capacity of strings, hence the name, \\\"Burstsort\\\".\\nOpen Data Structures - Java Edition - Section 11.2 - Counting Sort and Radix Sort, Pat Morin\\nOpen Data Structures - C++ Edition - Section 11.2 - Counting Sort and Radix Sort, Pat Morin\"}, {\"Run of a sequence\": \"In computer science, a run of a sequence is a non-decreasing range of the sequence that cannot be extended.  The number of runs of a sequence is the number of increasing subsequences of the sequence. This is a measure of presortedness, and in particular measures how many subsequences must be merged to sort a sequence.\\n\\nDefinition\\nLet \\n  \\n    \\n      \\n        X\\n        =\\n        \\u27e8\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle X=\\\\langle x_{1},\\\\dots ,x_{n}\\\\rangle }\\n   be a sequence of elements from a totally ordered set. A run of \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   is a maximal increasing sequence \\n  \\n    \\n      \\n        \\u27e8\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            i\\n            +\\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            j\\n            \\u2212\\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle \\\\langle x_{i},x_{i+1},\\\\dots ,x_{j-1},x_{j}\\\\rangle }\\n  . That is, \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n            \\u2212\\n            1\\n          \\n        \\n        >\\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i-1}>x_{i}}\\n   and \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n        >\\n        \\n          x\\n          \\n            j\\n            +\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}>x_{j+1}}\\n   assuming that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n            \\u2212\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i-1}}\\n   and \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n            +\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j+1}}\\n   exists. For example, if \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is a natural number, the sequence \\n  \\n    \\n      \\n        \\u27e8\\n        n\\n        +\\n        1\\n        ,\\n        n\\n        +\\n        2\\n        ,\\n        \\u2026\\n        ,\\n        2\\n        n\\n        ,\\n        1\\n        ,\\n        2\\n        ,\\n        \\u2026\\n        ,\\n        n\\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle \\\\langle n+1,n+2,\\\\dots ,2n,1,2,\\\\dots ,n\\\\rangle }\\n   has the two runs \\n  \\n    \\n      \\n        \\u27e8\\n        n\\n        +\\n        1\\n        ,\\n        \\u2026\\n        ,\\n        2\\n        n\\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle \\\\langle n+1,\\\\dots ,2n\\\\rangle }\\n   and \\n  \\n    \\n      \\n        \\u27e8\\n        1\\n        ,\\n        \\u2026\\n        ,\\n        n\\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle \\\\langle 1,\\\\dots ,n\\\\rangle }\\n  .\\nLet \\n  \\n    \\n      \\n        \\n          \\n            r\\n            u\\n            n\\n            s\\n          \\n        \\n        (\\n        X\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {runs}}(X)}\\n   be defined as the number of positions \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   such that \\n  \\n    \\n      \\n        1\\n        \\u2264\\n        i\\n        <\\n        n\\n      \\n    \\n    {\\\\displaystyle 1\\\\leq i<n}\\n   and \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n            +\\n            1\\n          \\n        \\n        <\\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i+1}<x_{i}}\\n  . It is equivalently defined as the number of runs of \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   minus one. This definition ensure that \\n  \\n    \\n      \\n        \\n          \\n            r\\n            u\\n            n\\n            s\\n          \\n        \\n        (\\n        \\u27e8\\n        1\\n        ,\\n        2\\n        ,\\n        \\u2026\\n        ,\\n        n\\n        \\u27e9\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {runs}}(\\\\langle 1,2,\\\\dots ,n\\\\rangle )=0}\\n  , that is, the \\n  \\n    \\n      \\n        \\n          \\n            r\\n            u\\n            n\\n            s\\n          \\n        \\n        (\\n        X\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {runs}}(X)=0}\\n   if, and only if, the sequence \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   is sorted. As another example, \\n  \\n    \\n      \\n        \\n          \\n            r\\n            u\\n            n\\n            s\\n          \\n        \\n        (\\n        \\u27e8\\n        n\\n        ,\\n        n\\n        \\u2212\\n        1\\n        ,\\n        \\u2026\\n        ,\\n        1\\n        \\u27e9\\n        )\\n        =\\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {runs}}(\\\\langle n,n-1,\\\\dots ,1\\\\rangle )=n-1}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            r\\n            u\\n            n\\n            s\\n          \\n        \\n        (\\n        \\u27e8\\n        2\\n        ,\\n        1\\n        ,\\n        4\\n        ,\\n        3\\n        ,\\n        \\u2026\\n        ,\\n        2\\n        n\\n        ,\\n        2\\n        n\\n        \\u2212\\n        1\\n        \\u27e9\\n        )\\n        =\\n        n\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {runs}}(\\\\langle 2,1,4,3,\\\\dots ,2n,2n-1\\\\rangle )=n}\\n  .\\n\\nSorting sequences with a low number of runs\\nThe function \\n  \\n    \\n      \\n        \\n          \\n            r\\n            u\\n            n\\n            s\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {runs}}}\\n   is a measure of presortedness. The natural merge sort is \\n  \\n    \\n      \\n        \\n          \\n            r\\n            u\\n            n\\n            s\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {runs}}}\\n  -optimal.  That is, if it is known that a sequence has a low number of runs, it can be efficiently sorted using the natural merge sort.\\n\\nLong runs\\nA long run is defined similarly to a run, except that the sequence can be either non-decreasing or non-increasing. The number of long runs is not a measure of presortedness. A sequence with a small number of long runs can be sorted efficiently by first reversing the decreasing runs and then using a natural merge sort.\\n\\nReferences\\nPowers, David M. W.; McMahon, Graham B. (1983). \\\"A compendium of interesting prolog programs\\\". DCS Technical Report 8313 (Report). Department of Computer Science, University of New South Wales.\\nMannila, H (1985). \\\"Measures of Presortedness and Optimal Sorting Algorithms\\\". IEEE Trans. Comput. (C-34): 318\\u2013325.\"}, {\"Samplesort\": \"Samplesort is a sorting algorithm that is a divide and conquer algorithm often used in parallel processing systems. Conventional divide and conquer sorting algorithms partitions the array into sub-intervals or buckets. The buckets are then sorted individually and then concatenated together. However, if the array is non-uniformly distributed, the performance of these sorting algorithms can be significantly throttled. Samplesort addresses this issue by selecting a sample of size s from the n-element sequence, and determining the range of the buckets by sorting the sample and choosing p\\u22121 < s elements from the result. These elements (called splitters) then divide the array into p approximately equal-sized buckets. Samplesort is described in the 1970 paper, \\\"Samplesort: A Sampling Approach to Minimal Storage Tree Sorting\\\", by W. D. Frazer and A. C. McKellar.\\n\\nAlgorithm\\nSamplesort is a generalization of quicksort. Where quicksort partitions its input into two parts at each step, based on a single value called the pivot, samplesort instead takes a larger sample from its input and divides its data into buckets accordingly. Like quicksort, it then recursively sorts the buckets.\\nTo devise a samplesort implementation, one needs to decide on the number of buckets p. When this is done, the actual algorithm operates in three phases:\\nSample p\\u22121 elements from the input (the splitters). Sort these; each pair of adjacent splitters then defines a bucket.\\nLoop over the data, placing each element in the appropriate bucket. (This may mean: send it to a processor, in a multiprocessor system.)\\nSort each of the buckets.The full sorted output is the concatenation of the buckets.\\nA common strategy is to set p equal to the number of processors available. The data is then distributed among the processors, which perform the sorting of buckets using some other, sequential, sorting algorithm.\\n\\nPseudocode\\nThe following listing shows the above mentioned three step algorithm as pseudocode and shows how the algorithm works in principle. In the following, A is the unsorted data, k is the oversampling factor, discussed later, and p is the number of splitters.\\n\\nfunction sampleSort(A[1..n], k, p)\\n    // if average bucket size is below a threshold switch to e.g. quicksort\\n    if n / k < threshold then smallSort(A) \\n    /* Step 1 */\\n    select S = [S1, ..., S(p\\u22121)k] randomly from // select samples\\n    sort S // sort sample\\n    [s0, s1, ..., sp\\u22121, sp] <- [-\\u221e, Sk, S2k, ..., S(p\\u22121)k, \\u221e] // select splitters\\n    /* Step 2 */\\n    for each a in A\\n        find j such that sj\\u22121 < a <= sj\\n        place a in bucket bj\\n    /* Step 3 and concatenation */\\n    return concatenate(sampleSort(b1), ..., sampleSort(bk))\\n\\nThe pseudo code is different from the original Frazer and McKellar algorithm. In the pseudo code, samplesort is called recursively. Frazer and McKellar called samplesort just once and used quicksort in all following iterations.\\n\\nComplexity\\nThe complexity, given in Big O notation, for a parallelized implementation with \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   processors:\\nFind the splitters.\\n\\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            \\n              \\n                n\\n                p\\n              \\n            \\n            +\\n            log\\n            \\u2061\\n            (\\n            p\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left({\\\\frac {n}{p}}+\\\\log(p)\\\\right)}\\n  Send to buckets.\\n\\n  \\n    \\n      \\n        O\\n        (\\n        p\\n        )\\n      \\n    \\n    {\\\\displaystyle O(p)}\\n   for reading all nodes\\n\\n  \\n    \\n      \\n        O\\n        (\\n        log\\n        \\u2061\\n        (\\n        p\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle O(\\\\log(p))}\\n   for broadcasting\\n\\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            \\n              \\n                n\\n                p\\n              \\n            \\n            log\\n            \\u2061\\n            (\\n            p\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left({\\\\frac {n}{p}}\\\\log(p)\\\\right)}\\n   for binary search for all keys\\n\\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            \\n              n\\n              p\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left({\\\\frac {n}{p}}\\\\right)}\\n   to send keys to bucketSort buckets. \\n\\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            c\\n            \\n              (\\n              \\n                \\n                  n\\n                  p\\n                \\n              \\n              )\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left(c\\\\left({\\\\frac {n}{p}}\\\\right)\\\\right)}\\n   where \\n  \\n    \\n      \\n        c\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle c(n)}\\n   is the complexity of the underlying sequential sorting method. Often \\n  \\n    \\n      \\n        c\\n        (\\n        n\\n        )\\n        =\\n        n\\n        log\\n        \\u2061\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle c(n)=n\\\\log(n)}\\n  .The number of comparisons, performed by this algorithm, approaches the information theoretical optimum \\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        !\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}(n!)}\\n   for big input sequences. In experiments, conducted by Frazer and McKellar, the algorithm needed 15% fewer comparisons than quicksort.\\n\\nSampling the data\\nThe data may be sampled through different methods. Some methods include:\\n\\nPick evenly spaced samples.\\nPick randomly selected samples.\\n\\nOversampling\\nThe oversampling ratio determines how many times more data elements to pull as samples, before determining the splitters. The goal is to get a good representation of the distribution of the data. If the data values are widely distributed, in that there are not many duplicate values, then a small sampling ratio is sufficient. In other cases where there are many duplicates in the distribution, a larger oversampling ratio will be necessary. In the ideal case, after step 2, each bucket contains \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\displaystyle n/p}\\n   elements. In this case, no bucket takes longer to sort than the others, because all buckets are of equal size.\\nAfter pulling \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   times more samples than necessary, the samples are sorted. Thereafter, the splitters used as bucket boundaries are the samples at position \\n  \\n    \\n      \\n        k\\n        ,\\n        2\\n        k\\n        ,\\n        3\\n        k\\n        ,\\n        \\u2026\\n        ,\\n        (\\n        p\\n        \\u2212\\n        1\\n        )\\n        k\\n      \\n    \\n    {\\\\displaystyle k,2k,3k,\\\\dots ,(p-1)k}\\n   of the sample sequence (together with \\n  \\n    \\n      \\n        \\u2212\\n        \\u221e\\n      \\n    \\n    {\\\\displaystyle -\\\\infty }\\n   and \\n  \\n    \\n      \\n        \\u221e\\n      \\n    \\n    {\\\\displaystyle \\\\infty }\\n   as left and right boundaries for the left most and right most buckets respectively). This provides a better heuristic for good splitters than just selecting \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   splitters randomly.\\n\\nBucket size estimate\\nWith the resulting sample size, the expected bucket size and especially the probability of a bucket exceeding a certain size can be estimated. The following will show that for an oversampling factor of \\n  \\n    \\n      \\n        S\\n        \\u2208\\n        \\u0398\\n        \\n          (\\n          \\n            \\n              \\n                \\n                  log\\n                  \\u2061\\n                  n\\n                \\n                \\n                  \\u03f5\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle S\\\\in \\\\Theta \\\\left({\\\\dfrac {\\\\log n}{\\\\epsilon ^{2}}}\\\\right)}\\n   the probability that no bucket has more than \\n  \\n    \\n      \\n        (\\n        1\\n        +\\n        \\u03f5\\n        )\\n        \\u22c5\\n        \\n          \\n            \\n              n\\n              p\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (1+\\\\epsilon )\\\\cdot {\\\\dfrac {n}{p}}}\\n   elements is larger than \\n  \\n    \\n      \\n        1\\n        \\u2212\\n        \\n          \\n            \\n              1\\n              n\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 1-{\\\\dfrac {1}{n}}}\\n  .\\nTo show this let \\n  \\n    \\n      \\n        \\u27e8\\n        \\n          e\\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          e\\n          \\n            n\\n          \\n        \\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle \\\\langle e_{1},\\\\dots ,e_{n}\\\\rangle }\\n   be the input as a sorted sequence. For a processor to get more than \\n  \\n    \\n      \\n        (\\n        1\\n        +\\n        \\u03f5\\n        )\\n        \\u22c5\\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\displaystyle (1+\\\\epsilon )\\\\cdot n/p}\\n   elements, there has to exist a subsequence of the input of length \\n  \\n    \\n      \\n        (\\n        1\\n        +\\n        \\u03f5\\n        )\\n        \\u22c5\\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\displaystyle (1+\\\\epsilon )\\\\cdot n/p}\\n  , of which a maximum of S samples are picked. These cases constitute the probability \\n  \\n    \\n      \\n        \\n          P\\n          \\n            fail\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P_{\\\\text{fail}}}\\n  . This can be represented as the random variable:\\n\\nFor the expected value of \\n  \\n    \\n      \\n        \\n          X\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X_{i}}\\n   holds:\\n\\nThis will be used to estimate \\n  \\n    \\n      \\n        \\n          P\\n          \\n            fail\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle P_{\\\\text{fail}}}\\n  :\\n\\nUsing the Chernoff bound now, it can be shown:\\n\\nMany identical keys\\nIn case of many identical keys, the algorithm goes through many recursion levels where sequences are sorted, because the whole sequence consists of identical keys. This can be counteracted by introducing equality buckets. Elements equal to a pivot are sorted into their respective equality bucket, which can be implemented with only one additional conditional branch. Equality buckets are not further sorted. This works, since keys occurring more than \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        k\\n      \\n    \\n    {\\\\displaystyle n/k}\\n   times are likely to become pivots.\\n\\nUses in parallel systems\\nSamplesort is often used in parallel systems, including distributed systems such as bulk synchronous parallel machines. Due to the variable amount of splitters (in contrast to only one pivot in Quicksort), Samplesort is very well suited and intuitive for parallelization and scaling. Furthermore Samplesort is also more cache-efficient than implementations of e.g. quicksort.\\nParallelization is implemented by splitting the sorting for each processor or node, where the number of buckets is equal to the number of processors \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  . Samplesort is efficient in parallel systems because each processor receives approximately the same bucket size \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\displaystyle n/p}\\n  . Since the buckets are sorted concurrently, the processors will complete the sorting at approximately the same time, thus not having a processor wait for others.\\nOn distributed systems, the splitters are chosen by taking \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   elements on each processor, sorting the resulting \\n  \\n    \\n      \\n        k\\n        p\\n      \\n    \\n    {\\\\displaystyle kp}\\n   elements with a distributed sorting algorithm, taking every \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  -th element and broadcasting the result to all processors. This costs \\n  \\n    \\n      \\n        \\n          T\\n          \\n            sort\\n          \\n        \\n        (\\n        k\\n        p\\n        ,\\n        p\\n        )\\n      \\n    \\n    {\\\\displaystyle T_{\\\\text{sort}}(kp,p)}\\n   for sorting the \\n  \\n    \\n      \\n        k\\n        p\\n      \\n    \\n    {\\\\displaystyle kp}\\n   elements on \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   processors, as well as  \\n  \\n    \\n      \\n        \\n          T\\n          \\n            allgather\\n          \\n        \\n        (\\n        p\\n        ,\\n        p\\n        )\\n      \\n    \\n    {\\\\displaystyle T_{\\\\text{allgather}}(p,p)}\\n   for distributing the \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   chosen splitters to \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   processors.\\nWith the resulting splitters, each processor places its own input data into local buckets. This takes \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        (\\n        n\\n        \\n          /\\n        \\n        p\\n        log\\n        \\u2061\\n        p\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}(n/p\\\\log p)}\\n   with binary search. Thereafter, the local buckets are redistributed to the processors. Processor \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   gets the local buckets \\n  \\n    \\n      \\n        \\n          b\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle b_{i}}\\n   of all other processors and sorts these locally. The distribution takes \\n  \\n    \\n      \\n        \\n          T\\n          \\n            all-to-all\\n          \\n        \\n        (\\n        N\\n        ,\\n        p\\n        )\\n      \\n    \\n    {\\\\displaystyle T_{\\\\text{all-to-all}}(N,p)}\\n   time, where \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   is the size of the biggest bucket. The local sorting takes \\n  \\n    \\n      \\n        \\n          T\\n          \\n            localsort\\n          \\n        \\n        (\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle T_{\\\\text{localsort}}(N)}\\n  .\\nExperiments performed in the early 1990s on Connection Machine supercomputers showed samplesort to be particularly good at sorting large datasets on these machines, because its incurs little interprocessor communication overhead. On latter-day GPUs, the algorithm may be less effective than its alternatives.\\n\\nEfficient Implementation of Samplesort\\nAs described above, the samplesort algorithm splits the elements according to the selected splitters. An efficient implementation strategy is proposed in the paper \\\"Super Scalar Sample Sort\\\". The implementation proposed in the paper uses two arrays of size \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   (the original array containing the input data and a temporary one) for an efficient implementation. Hence, this version of the implementation is not an in-place algorithm.\\nIn each recursion step, the data gets copied to the other array in a partitioned fashion. If the data is in the temporary array in the last recursion step, then the data is copied back to the original array.\\n\\nDetermining buckets\\nIn a comparison based sorting algorithm the comparison operation is the most performance critical part. In Samplesort this corresponds to determining the bucket for each element. This needs \\n  \\n    \\n      \\n        log\\n        \\u2061\\n        k\\n      \\n    \\n    {\\\\displaystyle \\\\log k}\\n   time for each element.\\nSuper Scalar Sample Sort uses a balanced search tree which is implicitly stored in an array t. The root is stored at 0, the left successor of \\n  \\n    \\n      \\n        \\n          t\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle t_{i}}\\n   is stored at \\n  \\n    \\n      \\n        \\n          t\\n          \\n            2\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle t_{2i}}\\n   and the right successor is stored at \\n  \\n    \\n      \\n        \\n          t\\n          \\n            2\\n            i\\n            +\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle t_{2i+1}}\\n  . Given the search tree t, the algorithm calculates the bucket number j of element \\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{i}}\\n   as follows (assuming \\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n        >\\n        \\n          t\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{i}>t_{j}}\\n   evaluates to 1 if it is true and 0 otherwise):\\n\\nj := 1\\nrepeat log2(p) times\\n    j := 2j + (a > tj)\\nj := j \\u2212 p + 1\\n\\nSince the number of buckets k is known at compile time, this loop can be unrolled by the compiler. The comparison operation is implemented with predicated instructions. Thus, there occur no branch mispredictions, which would slow down the comparison operation significantly.\\n\\nPartitioning\\nFor an efficient partitioning of the elements, the algorithm needs to know the sizes of the buckets in advance. To partition the elements of the sequence and put them into the array, we need to know the size of the buckets in advance. A naive algorithm could count the number of elements of each bucket. Then the elements could be inserted to the other array at the right place. Using this, one has to determine the bucket for each elements twice (one time for counting the number of elements in a bucket, and one time for inserting them).\\nTo avoid this doubling of comparisons, Super Scalar Sample Sort uses an additional array \\n  \\n    \\n      \\n        o\\n      \\n    \\n    {\\\\displaystyle o}\\n   (called oracle) which assigns each index of the elements to a bucket. First, the algorithm determines the contents of \\n  \\n    \\n      \\n        o\\n      \\n    \\n    {\\\\displaystyle o}\\n   by determining the bucket for each element and the bucket sizes, and then placing the elements into the bucket determined by \\n  \\n    \\n      \\n        o\\n      \\n    \\n    {\\\\displaystyle o}\\n  . The array \\n  \\n    \\n      \\n        o\\n      \\n    \\n    {\\\\displaystyle o}\\n   also incurs cost in storage space, but as it only needs to store \\n  \\n    \\n      \\n        n\\n        \\u22c5\\n        log\\n        \\u2061\\n        k\\n      \\n    \\n    {\\\\displaystyle n\\\\cdot \\\\log k}\\n   bits, these cost are small compared to the space of the input array.\\n\\nIn-place samplesort\\nA key disadvantage of the efficient Samplesort implementation shown above is that it is not in-place and requires a second temporary array of the same size as the input sequence during sorting. Efficient implementations of e.g. quicksort are in-place and thus more space efficient. However, Samplesort can be implemented in-place as well.The in-place algorithm is separated into four phases:\\n\\nSampling which is equivalent to the sampling in the above mentioned efficient implementation.\\nLocal Classification on each processor, which groups the input into blocks such that all elements in each block belong to the same bucket, but buckets are not necessarily continuous in memory.\\nBlock permutation brings the blocks into the globally correct order.\\nCleanup moves some elements on the edges of the buckets.One obvious disadvantage of this algorithm is that it reads and writes every element twice, once in the classification phase and once in the block permutation phase. However, the algorithm performs up to three times faster than other state of the art in-place competitors and up to 1.5 times faster than other state of the art sequential competitors. As sampling was already discussed above, the three later stages will be further detailed in the following.\\n\\nLocal classification\\nIn a first step, the input array is split up into \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   stripes of blocks of equal size, one for each processor. Each processor additionally allocates \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   buffers that are of equal size to the blocks, one for each bucket. Thereafter, each processor scans its stripe and moves the elements into the buffer of the according bucket. If a buffer is full, the buffer is written into the processors stripe, beginning at the front. There is always at least one buffer size of empty memory, because for a buffer to be written (i.e. buffer is full), at least a whole buffer size of elements more than elements written back had to be scanned. Thus, every full block contains elements of the same bucket. While scanning, the size of each bucket is kept track of.\\n\\nBlock permutation\\nFirstly, a prefix sum operation is performed that calculates the boundaries of the buckets. However, since only full blocks are moved in this phase, the boundaries are rounded up to a multiple of the block size and a single overflow buffer is allocated. Before starting the block permutation, some empty blocks might have to be moved to the end of its bucket. Thereafter, a write pointer \\n  \\n    \\n      \\n        \\n          w\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle w_{i}}\\n   is set to the start of the bucket \\n  \\n    \\n      \\n        \\n          b\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle b_{i}}\\n   subarray for each bucket and a read pointer \\n  \\n    \\n      \\n        \\n          r\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{i}}\\n   is set to the last non empty block in the bucket \\n  \\n    \\n      \\n        \\n          b\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle b_{i}}\\n   subarray for each bucket.\\nTo limit work contention, each processor is assigned a different primary bucket \\n  \\n    \\n      \\n        \\n          b\\n          \\n            p\\n            r\\n            i\\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle b_{prim}}\\n   and two swap buffers that can each hold a block. In each step, if both swap buffers are empty, the processor decrements the read pointer \\n  \\n    \\n      \\n        \\n          r\\n          \\n            p\\n            r\\n            i\\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{prim}}\\n   of its primary bucket and reads the block at \\n  \\n    \\n      \\n        \\n          r\\n          \\n            p\\n            r\\n            i\\n            m\\n            \\u2212\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle r_{prim-1}}\\n   and places it in one of its swap buffers. After determining the destination bucket \\n  \\n    \\n      \\n        \\n          b\\n          \\n            d\\n            e\\n            s\\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle b_{dest}}\\n   of the block by classifying the first element of the block, it increases the write pointer \\n  \\n    \\n      \\n        \\n          w\\n          \\n            d\\n            e\\n            s\\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle w_{dest}}\\n  , reads the block at \\n  \\n    \\n      \\n        \\n          w\\n          \\n            d\\n            e\\n            s\\n            t\\n            \\u2212\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle w_{dest-1}}\\n   into the other swap buffer and writes the block into its destination bucket. If \\n  \\n    \\n      \\n        \\n          w\\n          \\n            d\\n            e\\n            s\\n            t\\n          \\n        \\n        >\\n        \\n          r\\n          \\n            d\\n            e\\n            s\\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle w_{dest}>r_{dest}}\\n  , the swap buffers are empty again. Otherwise the block remaining in the swap buffers has to be inserted into its destination bucket.\\nIf all blocks in the subarray of the primary bucket of a processor are in the correct bucket, the next bucket is chosen as the primary bucket. If a processor chose all buckets as primary bucket once, the processor is finished.\\n\\nCleanup\\nSince only whole blocks were moved in the block permutation phase, some elements might still be incorrectly placed around the bucket boundaries. Since there has to be enough space in the array for each element, those incorrectly placed elements can be moved to empty spaces from left to right, lastly considering the overflow buffer.\\n\\nSee also\\nFlashsort\\nQuicksort\\n\\nReferences\\nExternal links\\nFrazer and McKellar's samplesort and derivatives:\\n\\nFrazer and McKellar's original paper\\nDOI.org\\nDOI.orgAdapted for use on parallel computers:\\n\\nhttp://citeseer.ist.psu.edu/91922.html\\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.49.214\"}, {\"Schwartzian transform\": \"In computer programming, the Schwartzian transform is a technique used to improve the efficiency of sorting a list of items. This idiom is appropriate for comparison-based sorting when the ordering is actually based on the ordering of a certain property (the key) of the elements, where computing that property is an intensive operation that should be performed a minimal number of times. The Schwartzian transform is notable in that it does not use named temporary arrays.\\nThe Schwartzian transform is a version of a Lisp idiom known as decorate-sort-undecorate, which avoids recomputing the sort keys by temporarily associating them with the input items. This approach is similar to memoization, which avoids repeating the calculation of the key corresponding to a specific input value. By comparison, this idiom assures that each input item's key is calculated exactly once, which may still result in repeating some calculations if the input data contains duplicate items.\\nThe idiom is named after Randal L. Schwartz, who first demonstrated it in Perl shortly after the release of Perl 5 in 1994. The term \\\"Schwartzian transform\\\" applied solely to Perl programming for a number of years, but it has later been adopted by some users of other languages, such as Python, to refer to similar idioms in those languages. However, the algorithm was already in use in other languages (under no specific name) before it was popularized among the Perl community in the form of that particular idiom by Schwartz. The term \\\"Schwartzian transform\\\" indicates a specific idiom, and not the algorithm in general.\\nFor example, to sort the word list (\\\"aaaa\\\",\\\"a\\\",\\\"aa\\\") according to word length: first build the list ([\\\"aaaa\\\",4],[\\\"a\\\",1],[\\\"aa\\\",2]), then sort it according to the numeric values getting ([\\\"a\\\",1],[\\\"aa\\\",2],[\\\"aaaa\\\",4]), then strip off the numbers and you get (\\\"a\\\",\\\"aa\\\",\\\"aaaa\\\"). That was the algorithm in general, so it does not count as a transform. To make it a true Schwartzian transform, it would be done in Perl like this:\\n\\nThe Perl idiom\\nThe general form of the Schwartzian transform is:\\n\\nHere foo($_) represents an expression that takes $_ (each item of the list in turn) and produces the corresponding value that is to be compared in its stead.\\nReading from right to left (or from the bottom to the top):\\n\\nthe original list @unsorted is fed into a map operation that wraps each item into a (reference to an anonymous 2-element) array consisting of itself and the calculated value that will determine its sort order (list of item becomes a list of [item, value]);\\nthen the list of lists produced by map is fed into sort, which sorts it according to the values previously calculated (list of [item, value] \\u21d2 sorted list of [item, value]);\\nfinally, another map operation unwraps the values (from the anonymous array) used for the sorting, producing the items of the original list in the sorted order (sorted list of [item, value] \\u21d2 sorted list of item).The use of anonymous arrays ensures that memory will be reclaimed by the Perl garbage collector immediately after the sorting is done.\\n\\nEfficiency analysis\\nWithout the Schwartzian transform, the sorting in the example above would be written in Perl like this:\\n\\nWhile it is shorter to code, the naive approach here could be much less efficient if the key function (called foo in the example above) is expensive to compute. This is because the code inside the brackets is evaluated each time two elements need to be compared. An optimal comparison sort performs O(n log n) comparisons (where n is the length of the list), with 2 calls to foo every comparison, resulting in O(n log n) calls to foo. In comparison, using the Schwartzian transform, we only make 1 call to foo per element, at the beginning map stage, for a total of n calls to foo.\\nHowever, if the function foo is relatively simple, then the extra overhead of the Schwartzian transform may be unwarranted.\\n\\nExample\\nFor example, to sort a list of files by their modification times, a naive approach might be as follows:\\n\\n function naiveCompare(file a, file b) {\\n     return modificationTime(a) < modificationTime(b)\\n }\\n \\n // Assume that sort(list, comparisonPredicate) sorts the given list using\\n // the comparisonPredicate to compare two elements.\\n sortedArray := sort(filesArray, naiveCompare)\\n\\nUnless the modification times are memoized for each file, this method requires re-computing them every time a file is compared in the sort. Using the Schwartzian transform, the modification time is calculated only once per file.\\nA Schwartzian transform involves the functional idiom described above, which does not use temporary arrays.\\nThe same algorithm can be written procedurally to better illustrate how it works, but this requires using temporary arrays, and is not a Schwartzian transform. The following example pseudo-code implements the algorithm in this way:\\n\\n for each file in filesArray\\n     insert array(file, modificationTime(file)) at end of transformedArray\\n \\n function simpleCompare(array a, array b) {\\n     return a[2] < b[2]\\n }\\n \\n transformedArray := sort(transformedArray, simpleCompare)\\n \\n for each file in transformedArray\\n     insert file[1] at end of sortedArray\\n\\nHistory\\nThe first known online appearance of the Schwartzian transform is a December 16, 1994 posting by Randal Schwartz to a thread in comp.unix.shell Usenet newsgroup, crossposted to comp.lang.perl. (The current version of the Perl Timeline is incorrect and refers to a later date in 1995.) The thread began with a question about how to sort a list of lines by their \\\"last\\\" word:\\n\\nadjn:Joshua Ng\\nadktk:KaLap Timothy Kwong\\nadmg:Mahalingam Gobieramanan\\nadmln:Martha L. Nangalama\\n\\nSchwartz responded with:\\n\\nThis code produces the result:\\n\\nadmg:Mahalingam Gobieramanan\\nadktk:KaLap Timothy Kwong\\nadmln:Martha L. Nangalama\\nadjn:Joshua Ng\\n\\nSchwartz noted in the post that he was \\\"Speak[ing] with a lisp in Perl\\\", a reference to the idiom's Lisp origins.\\nThe term \\\"Schwartzian transform\\\" itself was coined by Tom Christiansen in a follow-up reply. Later posts by Christiansen made it clear that he had not intended to name the construct, but merely to refer to it from the original post: his attempt to finally name it \\\"The Black Transform\\\" did not take hold (\\\"Black\\\" here being a pun on \\\"schwar[t]z\\\", which means black in German).\\n\\nComparison to other languages\\nSome other languages provide a convenient interface to the same optimization as the Schwartzian transform:\\n\\nIn Python 2.4 and above, both the sorted() function and the in-place list.sort() method take a key= parameter that allows the user to provide a \\\"key function\\\" (like foo in the examples above). In Python 3 and above, use of the key function is the only way to specify a custom sort order (the previously supported cmp= parameter that allowed the user to provide a \\\"comparison function\\\" was removed). Before Python 2.4, developers would use the lisp-originated decorate\\u2013sort\\u2013undecorate (DSU) idiom, usually by wrapping the objects in a (sortkey, object) tuple.\\nIn Ruby 1.8.6 and above, the Enumerable abstract class (which includes Arrays) contains a sort_by method, which allows specifying the \\\"key function\\\" (like foo in the examples above) as a code block.\\nIn D 2 and above, the schwartz Sort function is available. It might require less temporary data and be faster than the Perl idiom or the decorate\\u2013sort\\u2013undecorate idiom present in Python and Lisp. This is because sorting is done in-place, and only minimal extra data (one array of transformed elements) is created.\\nRacket's core sort function accepts a #:key keyword argument with a function that extracts a key, and an additional #:cache-keys? requests that the resulting values are cached during sorting. For example, a convenient way to shuffle a list is (sort l < #:key (\\u03bb (_) (random)) #:cache-keys? #t).\\nIn PHP 5.3 and above the transform can be implemented by use of array_walk, e.g. to work around the limitations of the unstable sort algorithms in PHP.\\nIn Elixir, the Enum.sort_by/2 and Enum.sort_by/3 methods allow users to perform a Schwartzian transform for any module that implements the Enumerable protocol.\\nIn Raku, one needs to supply a comparator lambda that only takes 1 argument to perform a Schwartzian transform under the hood:  would sort on the string representation using a Schwartzian transform,  would do the same converting the elements to compare just before each comparison.\\nIn Rust, somewhat confusingly, the slice::sort_by_key method does *not* perform a Schwartzian transform as it will not allocate additional storage for the key, it will call the key function for each value for each comparison. The slice::sort_by_cached_key method will compute the keys once per element.\\n\\nReferences\\nExternal links\\n\\nSorting with the Schwartzian Transform by Randal L. Schwartz\\nMark-Jason Dominus explains the Schwartzian Transform\\nhttp://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/52234\\nPython Software Foundation (2005). 1.5.2   I want to do a complicated sort: can you do a Schwartzian Transform in Python?. Retrieved June 22, 2005.\\nMemoize Perl module - making expensive functions faster by caching their results.\"}, {\"Selection sort\": \"In computer science, selection sort is an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.\\nThe algorithm divides the input list into two parts: a sorted sublist of items which is built up from left to right at the front (left) of the list and a sublist of the remaining unsorted items that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right. \\nThe time efficiency of selection sort is quadratic, so there are a number of sorting techniques which have better time complexity than selection sort.\\n\\nExample\\nHere is an example of this sort algorithm sorting five elements:\\n\\n(Nothing appears changed on these last two lines because the last two numbers were already in order.)\\nSelection sort can also be used on list structures that make add and remove efficient, such as a linked list. In this case it is more common to remove the minimum element from the remainder of the list, and then insert it at the end of the values sorted so far. For example:\\n\\narr[] = 64 25 12 22 11\\n\\n// Find the minimum element in arr[0...4]\\n// and place it at beginning\\n11 25 12 22 64\\n\\n// Find the minimum element in arr[1...4]\\n// and place it at beginning of arr[1...4]\\n11 12 25 22 64\\n\\n// Find the minimum element in arr[2...4]\\n// and place it at beginning of arr[2...4]\\n11 12 22 25 64\\n\\n// Find the minimum element in arr[3...4]\\n// and place it at beginning of arr[3...4]\\n11 12 22 25 64\\n\\nImplementations\\nBelow is an implementation in C.\\n\\nComplexity\\nSelection sort is not difficult to analyze compared to other sorting algorithms, since none of the loops depend on the data in the array. Selecting the minimum requires scanning \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements (taking \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   comparisons) and then swapping it into the first position. Finding the next lowest element requires scanning the remaining \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        2\\n      \\n    \\n    {\\\\displaystyle n-2}\\n   elements and so on. Therefore, the total number of comparisons is\\n\\n  \\n    \\n      \\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        +\\n        (\\n        n\\n        \\u2212\\n        2\\n        )\\n        +\\n        .\\n        .\\n        .\\n        +\\n        1\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        i\\n      \\n    \\n    {\\\\displaystyle (n-1)+(n-2)+...+1=\\\\sum _{i=1}^{n-1}i}\\n  By arithmetic progression,\\n\\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        i\\n        =\\n        \\n          \\n            \\n              (\\n              n\\n              \\u2212\\n              1\\n              )\\n              +\\n              1\\n            \\n            2\\n          \\n        \\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        =\\n        \\n          \\n            1\\n            2\\n          \\n        \\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        =\\n        \\n          \\n            1\\n            2\\n          \\n        \\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        \\u2212\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i=1}^{n-1}i={\\\\frac {(n-1)+1}{2}}(n-1)={\\\\frac {1}{2}}n(n-1)={\\\\frac {1}{2}}(n^{2}-n)}\\n  which is of complexity \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   in terms of number of comparisons. Each of these scans requires one swap for \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   elements (the final element is already in place).\\n\\nComparison to other sorting algorithms\\nAmong quadratic sorting algorithms (sorting algorithms with a simple average-case of \\u0398(n2)), selection sort almost always outperforms bubble sort and gnome sort. Insertion sort is very similar in that after the kth iteration, the first \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   elements in the array are in sorted order. Insertion sort's advantage is that it only scans as many elements as it needs in order to place the \\n  \\n    \\n      \\n        k\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle k+1}\\n  st element, while selection sort must scan all remaining elements to find the \\n  \\n    \\n      \\n        k\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle k+1}\\n  st element.\\nSimple calculation shows that insertion sort will therefore usually perform about half as many comparisons as selection sort, although it can perform just as many or far fewer depending on the order the array was in prior to sorting. It can be seen as an advantage for some real-time applications that selection sort will perform identically regardless of the order of the array, while insertion sort's running time can vary considerably. However, this is more often an advantage for insertion sort in that it runs much more efficiently if the array is already sorted or \\\"close to sorted.\\\"\\nWhile selection sort is preferable to insertion sort in terms of number of writes (\\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   swaps versus up to \\n  \\n    \\n      \\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle n(n-1)/2}\\n   swaps, with each swap being two writes), this is roughly twice the theoretical minimum achieved by cycle sort, which performs at most n writes.  This can be important if writes are significantly more expensive than reads, such as with EEPROM or Flash memory, where every write lessens the lifespan of the memory.\\nSelection sort can be implemented without unpredictable branches for the benefit of CPU branch predictors, by finding the location of the minimum with branch-free code and then performing the swap unconditionally.\\nFinally, selection sort is greatly outperformed on larger arrays by \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n\\\\log n)}\\n   divide-and-conquer algorithms such as mergesort. However, insertion sort or selection sort are both typically faster for small arrays (i.e. fewer than 10\\u201320 elements). A useful optimization in practice for the recursive algorithms is to switch to insertion sort or selection sort for \\\"small enough\\\" sublists.\\n\\nVariants\\nHeapsort greatly improves the basic algorithm by using an implicit heap data structure to speed up finding and removing the lowest datum. If implemented correctly, the heap will allow finding the next lowest element in \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (\\\\log n)}\\n   time instead of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n)}\\n   for the inner loop in normal selection sort, reducing the total running time to \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n\\\\log n)}\\n  .\\nA bidirectional variant of selection sort (called double selection sort or sometimes cocktail sort due to its similarity to cocktail shaker sort) finds both the minimum and maximum values in the list in every pass. This requires three comparisons per two items (a pair of elements is compared, then the greater is compared to the maximum and the lesser is compared to the minimum) rather than regular selection sort's one comparison per item, but requires only half as many passes, a net 25% savings.\\nSelection sort can be implemented as a stable sort if, rather than swapping in step 2, the minimum value is inserted into the first position and the intervening values shifted up. However, this modification either requires a data structure that supports efficient insertions or deletions, such as a linked list, or it leads to performing \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n^{2})}\\n   writes.\\nIn the bingo sort variant, items are sorted by repeatedly looking through the remaining items to find the greatest value and moving all items with that value to their final location. Like counting sort, this is an efficient variant if there are many duplicate values: selection sort does one pass through the remaining items for each item moved, while Bingo sort does one pass for each value.  After an initial pass to find the greatest value, subsequent passes move every item with that value to its final location while finding the next value as in the following pseudocode (arrays are zero-based and the for-loop includes both the top and bottom limits, as in Pascal):\\n\\nThus, if on average there are more than two items with the same value, bingo sort can be expected to be faster because it executes the inner loop fewer times than selection sort.\\n\\nSee also\\nSelection algorithm\\n\\nReferences\\nExternal links\\n\\nAnimated Sorting Algorithms: Selection Sort at the Wayback Machine (archived 7 March 2015) \\u2013 graphical demonstration\"}, {\"Shellsort\": \"Shellsort, also known as Shell sort or Shell's method, is an in-place comparison sort. It can be seen as either a generalization of sorting by exchange (bubble sort) or sorting by insertion (insertion sort). The method starts by sorting pairs of elements far apart from each other, then progressively reducing the gap between elements to be compared. By starting with far apart elements, it can move some out-of-place elements into position faster than a simple nearest neighbor exchange. Donald Shell published the first version of this sort in 1959. The running time of Shellsort is heavily dependent on the gap sequence it uses. For many practical variants, determining their time complexity remains an open problem.\\n\\nDescription\\nShellsort is an optimization of insertion sort that allows the exchange of items that are far apart. The idea is to arrange the list of elements so that, starting anywhere, taking every hth element produces a sorted list. Such a list is said to be h-sorted. It can also be thought of as h interleaved lists, each individually sorted. Beginning with large values of h allows elements to move long distances in the original list, reducing large amounts of disorder quickly, and leaving less work for smaller h-sort steps to do. If the list is then k-sorted for some smaller integer k, then the list remains h-sorted. Following this idea for a decreasing sequence of h values ending in 1 is guaranteed to leave a sorted list in the end.In simplistic terms, this means if we have an array of 1024 numbers, our first gap (h) could be 512. We then run through the list comparing each element in the first half to the element in the second half. Our second gap (k) is 256, which breaks the array into four sections (starting at 0,256,512,768), and we make sure the first items in each section are sorted relative to each other, then the second item in each section, and so on. In practice the gap sequence could be anything, but the last gap is always 1 to finish the sort (effectively finishing with an ordinary insertion sort).\\nAn example run of Shellsort with gaps 5, 3 and 1 is shown below.\\n\\nThe first pass, 5-sorting, performs insertion sort on five  separate subarrays (a1, a6, a11), (a2, a7, a12), (a3, a8), (a4, a9), (a5, a10). For instance, it changes the subarray (a1, a6, a11) from (62, 17, 25) to (17, 25, 62). The next pass, 3-sorting, performs insertion sort on the three subarrays (a1, a4, a7, a10), (a2, a5, a8, a11), (a3, a6, a9, a12). The last pass, 1-sorting, is an ordinary insertion sort of the entire array (a1,..., a12).\\nAs the example illustrates, the subarrays that Shellsort operates on are initially short; later they are longer but almost ordered. In both cases insertion sort works efficiently.\\nShellsort is not stable: it may change the relative order of elements with equal values. It is an adaptive sorting algorithm in that it executes faster when the input is partially sorted.\\n\\nPseudocode\\nUsing Marcin Ciura's gap sequence, with an inner insertion sort.\\n\\nGap sequences\\nThe question of deciding which gap sequence to use is difficult. Every gap sequence that contains 1 yields a correct sort (as this makes the final pass an ordinary insertion sort); however, the properties of thus obtained versions of Shellsort may be very different. Too few gaps slows down the passes, and too many gaps produces an overhead.\\nThe table below compares most proposed gap sequences published so far. Some of them have decreasing elements that depend on the size of the sorted array (N). Others are increasing infinite sequences, whose elements less than N should be used in reverse order.\\n\\nWhen the binary representation of N contains many consecutive zeroes, Shellsort using Shell's original gap sequence makes \\u0398(N2) comparisons in the worst case. For instance, this case occurs for N equal to a power of two when elements greater and smaller than the median occupy odd and even positions respectively, since they are compared only in the last pass.\\nAlthough it has higher complexity than the O(N log N) that is optimal for comparison sorts, Pratt's version lends itself to sorting networks and has the same asymptotic gate complexity as Batcher's bitonic sorter.\\nGonnet and Baeza-Yates observed that Shellsort makes the fewest comparisons on average when the ratios of successive gaps are roughly equal to 2.2. This is why their sequence with ratio 2.2 and Tokuda's sequence with ratio 2.25 prove efficient. However, it is not known why this is so. Sedgewick recommends using gaps which have low greatest common divisors or are pairwise coprime.With respect to the average number of comparisons, Ciura's sequence has the best known performance; gaps from 701 were not determined but the sequence can be further extended according to the recursive formula \\n  \\n    \\n      \\n        \\n          h\\n          \\n            k\\n          \\n        \\n        =\\n        \\u230a\\n        2.25\\n        \\n          h\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle h_{k}=\\\\lfloor 2.25h_{k-1}\\\\rfloor }\\n  .\\nTokuda's sequence, defined by the simple formula \\n  \\n    \\n      \\n        \\n          h\\n          \\n            k\\n          \\n        \\n        =\\n        \\u2308\\n        \\n          h\\n          \\n            k\\n          \\n          \\u2032\\n        \\n        \\u2309\\n      \\n    \\n    {\\\\displaystyle h_{k}=\\\\lceil h'_{k}\\\\rceil }\\n  , where \\n  \\n    \\n      \\n        \\n          h\\n          \\n            k\\n          \\n          \\u2032\\n        \\n        =\\n        2.25\\n        \\n          h\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n          \\u2032\\n        \\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle h'_{k}=2.25h'_{k-1}+1}\\n  , \\n  \\n    \\n      \\n        \\n          h\\n          \\n            1\\n          \\n          \\u2032\\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle h'_{1}=1}\\n  , can be recommended for practical applications.\\nIf the maximum input size is small, as may occur if Shellsort is used on small subarrays by another recursive sorting algorithm such as quicksort or merge sort, then it is possible to tabulate an optimal sequence for each input size.\\n\\nComputational complexity\\nThe following property holds: after h2-sorting of any h1-sorted array, the array remains h1-sorted. Every h1-sorted and h2-sorted array is also (a1h1+a2h2)-sorted, for any nonnegative integers a1 and a2. The worst-case complexity of Shellsort is therefore connected with the Frobenius problem: for given integers h1,..., hn with gcd = 1, the Frobenius number g(h1,..., hn) is the greatest integer that cannot be represented as a1h1+ ... +anhn with nonnegative integer a1,..., an. Using known formulae for Frobenius numbers, we can determine the worst-case complexity of Shellsort for several classes of gap sequences. Proven results are shown in the above table.\\nMark Allen Weiss proved that Shellsort runs in O(N log N) time when the input array is in reverse order.With respect to the average number of operations, none of the proven results concerns a practical gap sequence. For gaps that are powers of two, Espelid computed this average as \\n  \\n    \\n      \\n        0.5349\\n        N\\n        \\n          \\n            N\\n          \\n        \\n        \\u2212\\n        0.4387\\n        N\\n        \\u2212\\n        0.097\\n        \\n          \\n            N\\n          \\n        \\n        +\\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle 0.5349N{\\\\sqrt {N}}-0.4387N-0.097{\\\\sqrt {N}}+O(1)}\\n  . Knuth determined the average complexity of sorting an N-element array with two gaps (h, 1) to be \\n  \\n    \\n      \\n        \\n          \\n            \\n              2\\n              \\n                N\\n                \\n                  2\\n                \\n              \\n            \\n            h\\n          \\n        \\n        +\\n        \\n          \\n            \\u03c0\\n            \\n              N\\n              \\n                3\\n              \\n            \\n            h\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {2N^{2}}{h}}+{\\\\sqrt {\\\\pi N^{3}h}}}\\n  . It follows that a two-pass Shellsort with h = \\u0398(N1/3) makes on average O(N5/3) comparisons/inversions/running time. Yao found the average complexity of a three-pass Shellsort. His result was refined by Janson and Knuth: the average number of comparisons/inversions/running time made during a Shellsort with three gaps (ch, cg, 1), where h and g are coprime, is \\n  \\n    \\n      \\n        \\n          \\n            \\n              N\\n              \\n                2\\n              \\n            \\n            \\n              4\\n              c\\n              h\\n            \\n          \\n        \\n        +\\n        O\\n        (\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {N^{2}}{4ch}}+O(N)}\\n   in the first pass, \\n  \\n    \\n      \\n        \\n          \\n            1\\n            \\n              8\\n              g\\n            \\n          \\n        \\n        \\n          \\n            \\n              \\u03c0\\n              \\n                c\\n                h\\n              \\n            \\n          \\n        \\n        (\\n        h\\n        \\u2212\\n        1\\n        )\\n        \\n          N\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        O\\n        (\\n        h\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {1}{8g}}{\\\\sqrt {\\\\frac {\\\\pi }{ch}}}(h-1)N^{3/2}+O(hN)}\\n   in the second pass and \\n  \\n    \\n      \\n        \\u03c8\\n        (\\n        h\\n        ,\\n        g\\n        )\\n        N\\n        +\\n        \\n          \\n            1\\n            8\\n          \\n        \\n        \\n          \\n            \\n              \\u03c0\\n              c\\n            \\n          \\n        \\n        (\\n        c\\n        \\u2212\\n        1\\n        )\\n        \\n          N\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        O\\n        \\n          (\\n          \\n            (\\n            c\\n            \\u2212\\n            1\\n            )\\n            g\\n            \\n              h\\n              \\n                1\\n                \\n                  /\\n                \\n                2\\n              \\n            \\n            N\\n          \\n          )\\n        \\n        +\\n        O\\n        \\n          (\\n          \\n            \\n              c\\n              \\n                2\\n              \\n            \\n            \\n              g\\n              \\n                3\\n              \\n            \\n            \\n              h\\n              \\n                2\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\psi (h,g)N+{\\\\frac {1}{8}}{\\\\sqrt {\\\\frac {\\\\pi }{c}}}(c-1)N^{3/2}+O\\\\left((c-1)gh^{1/2}N\\\\right)+O\\\\left(c^{2}g^{3}h^{2}\\\\right)}\\n   in the third pass. \\u03c8(h, g) in the last formula is a complicated function asymptotically equal to \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\u03c0\\n                h\\n              \\n              128\\n            \\n          \\n        \\n        g\\n        +\\n        O\\n        \\n          (\\n          \\n            \\n              g\\n              \\n                \\u2212\\n                1\\n                \\n                  /\\n                \\n                2\\n              \\n            \\n            \\n              h\\n              \\n                1\\n                \\n                  /\\n                \\n                2\\n              \\n            \\n          \\n          )\\n        \\n        +\\n        O\\n        \\n          (\\n          \\n            g\\n            \\n              h\\n              \\n                \\u2212\\n                1\\n                \\n                  /\\n                \\n                2\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {\\\\frac {\\\\pi h}{128}}}g+O\\\\left(g^{-1/2}h^{1/2}\\\\right)+O\\\\left(gh^{-1/2}\\\\right)}\\n  . In particular, when h = \\u0398(N7/15) and g = \\u0398(N1/5), the average time of sorting is O(N23/15).\\nBased on experiments, it is conjectured that Shellsort with Hibbard's gap sequence runs in O(N5/4) average time, and that Gonnet and Baeza-Yates's sequence requires on average 0.41N ln N (ln ln N + 1/6) element moves. Approximations of the average number of operations formerly put forward for other sequences fail when sorted arrays contain millions of elements.\\nThe graph below shows the average number of element comparisons in various variants of Shellsort, divided by the theoretical lower bound, i.e. log2N!, where the sequence 1, 4, 10, 23, 57, 132, 301, 701 has been extended according to the formula \\n  \\n    \\n      \\n        \\n          h\\n          \\n            k\\n          \\n        \\n        =\\n        \\u230a\\n        2.25\\n        \\n          h\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle h_{k}=\\\\lfloor 2.25h_{k-1}\\\\rfloor }\\n  .\\n\\nApplying the theory of Kolmogorov complexity, Jiang, Li, and Vit\\u00e1nyi \\n proved the following lower bound for the order of the average number of operations/running time in a p-pass Shellsort: \\u03a9(pN1+1/p) when p \\u2264 log2N and \\u03a9(pN) when p > log2N.\\nTherefore, Shellsort has prospects of running in an average time that asymptotically grows like N logN only when using gap sequences whose number of gaps grows in proportion to the logarithm of the array size. It is, however, unknown whether Shellsort can reach this asymptotic order of average-case complexity, which is optimal for comparison sorts. The lower bound was improved by Vit\\u00e1nyi for every number of passes \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   to  \\n\\n  \\n    \\n      \\n        \\u03a9\\n        (\\n        N\\n        \\n          \\u2211\\n          \\n            k\\n            =\\n            1\\n          \\n          \\n            p\\n          \\n        \\n        \\n          h\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        \\n          /\\n        \\n        \\n          h\\n          \\n            k\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Omega (N\\\\sum _{k=1}^{p}h_{k-1}/h_{k})}\\n  \\nwhere \\n  \\n    \\n      \\n        \\n          h\\n          \\n            0\\n          \\n        \\n        =\\n        N\\n      \\n    \\n    {\\\\displaystyle h_{0}=N}\\n  . This result implies for example the Jiang-Li-Vit\\u00e1nyi lower bound for all \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  -pass increment sequences and improves that lower bound for particular increment sequences. In fact all bounds (lower and upper) currently known for the average case are precisely matched by this lower bound. For example, this gives the new result that the Janson-Knuth upper bound is matched by the resulting lower bound for the used increment sequence, showing that three pass Shellsort for this increment sequence uses \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        \\n          N\\n          \\n            23\\n            \\n              /\\n            \\n            15\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (N^{23/15})}\\n   comparisons/inversions/running time.\\nThe formula allows us to search for increment sequences that yield lower bounds which are unknown; for example an increment sequence for four passes which has a lower bound greater than\\n\\n  \\n    \\n      \\n        \\u03a9\\n        (\\n        p\\n        \\n          n\\n          \\n            1\\n            +\\n            1\\n            \\n              /\\n            \\n            p\\n          \\n        \\n        )\\n        =\\n        \\u03a9\\n        (\\n        \\n          n\\n          \\n            5\\n            \\n              /\\n            \\n            4\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Omega (pn^{1+1/p})=\\\\Omega (n^{5/4})}\\n   for the increment sequence\\n\\n  \\n    \\n      \\n        \\n          h\\n          \\n            1\\n          \\n        \\n        =\\n        \\n          n\\n          \\n            11\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle h_{1}=n^{11/16},}\\n   \\n  \\n    \\n      \\n        \\n          h\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          n\\n          \\n            7\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle h_{2}=n^{7/16},}\\n   \\n  \\n    \\n      \\n        \\n          h\\n          \\n            3\\n          \\n        \\n        =\\n        \\n          n\\n          \\n            3\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle h_{3}=n^{3/16},}\\n   \\n  \\n    \\n      \\n        \\n          h\\n          \\n            4\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle h_{4}=1}\\n  . The lower bound becomes\\n\\n  \\n    \\n      \\n        T\\n        =\\n        \\u03a9\\n        (\\n        n\\n        \\u22c5\\n        (\\n        \\n          n\\n          \\n            1\\n            \\u2212\\n            11\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        +\\n        \\n          n\\n          \\n            11\\n            \\n              /\\n            \\n            16\\n            \\u2212\\n            7\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        +\\n        \\n          n\\n          \\n            7\\n            \\n              /\\n            \\n            16\\n            \\u2212\\n            3\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        +\\n        \\n          n\\n          \\n            3\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        )\\n        =\\n        \\u03a9\\n        (\\n        \\n          n\\n          \\n            1\\n            +\\n            5\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        )\\n        =\\n        \\u03a9\\n        (\\n        \\n          n\\n          \\n            21\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle T=\\\\Omega (n\\\\cdot (n^{1-11/16}+n^{11/16-7/16}+n^{7/16-3/16}+n^{3/16})=\\\\Omega (n^{1+5/16})=\\\\Omega (n^{21/16}).}\\n  \\nThe worst-case complexity of any version of Shellsort is of higher order: Plaxton, Poonen, and Suel showed that it grows at least as rapidly as \\n  \\n    \\n      \\n        \\u03a9\\n        \\n          (\\n          \\n            N\\n            \\n              \\n                (\\n                \\n                  \\n                    \\n                      log\\n                      \\u2061\\n                      N\\n                    \\n                    \\n                      log\\n                      \\u2061\\n                      log\\n                      \\u2061\\n                      N\\n                    \\n                  \\n                \\n                )\\n              \\n              \\n                2\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega \\\\left(N\\\\left({\\\\log N \\\\over \\\\log \\\\log N}\\\\right)^{2}\\\\right)}\\n  .\\nRobert Cypher proved a stronger lower bound: \\n  \\n    \\n      \\n        \\u03a9\\n        \\n          (\\n          \\n            N\\n            \\n              \\n                \\n                  (\\n                  log\\n                  \\u2061\\n                  N\\n                  \\n                    )\\n                    \\n                      2\\n                    \\n                  \\n                \\n                \\n                  log\\n                  \\u2061\\n                  log\\n                  \\u2061\\n                  N\\n                \\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega \\\\left(N{{(\\\\log N)^{2}} \\\\over {\\\\log \\\\log N}}\\\\right)}\\n   when \\n  \\n    \\n      \\n        \\n          h\\n          \\n            s\\n            +\\n            1\\n          \\n        \\n        >\\n        \\n          h\\n          \\n            s\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle h_{s+1}>h_{s}}\\n   for all \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  .\\n\\nApplications\\nShellsort performs more operations and has higher cache miss ratio than quicksort. However, since it can be implemented using little code and does not use the call stack, some implementations of the qsort function in the C standard library targeted at embedded systems use it instead of quicksort. Shellsort is, for example, used in the uClibc library. For similar reasons, in the past, Shellsort was used in the Linux kernel.Shellsort can also serve as a sub-algorithm of introspective sort, to sort short subarrays and to prevent a slowdown when the recursion depth exceeds a given limit. This principle is employed, for instance, in the bzip2 compressor.\\n\\nSee also\\nComb sort\\n\\nReferences\\nBibliography\\nKnuth, Donald E. (1997). \\\"Shell's method\\\". The Art of Computer Programming. Volume 3: Sorting and Searching (2nd ed.). Reading, Massachusetts: Addison-Wesley. pp. 83\\u201395. ISBN 978-0-201-89685-5.\\nAnalysis of Shellsort and Related Algorithms, Robert Sedgewick, Fourth European Symposium on Algorithms, Barcelona, September 1996.\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Shell Sort at the Wayback Machine (archived 10 March 2015) \\u2013 graphical demonstration\\nShellsort with gaps 5, 3, 1 as a Hungarian folk dance\"}, {\"Slowsort\": \"Slowsort is a sorting algorithm. It is of humorous nature and not useful. It is a reluctant algorithm based on the principle of multiply and surrender (a parody formed by taking the opposites of divide and conquer). It was published in 1984 by Andrei Broder and Jorge Stolfi in their paper Pessimal Algorithms and Simplexity Analysis (a parody of optimal algorithms and complexity analysis).\\n\\nAlgorithm\\nSlowsort is a recursive algorithm.\\n\\nIt sorts in-place.\\nIt is a stable sort. (It does not change the order of equal-valued keys.)This is an implementation in pseudocode:\\n\\nSort the first half, recursively. (1.1)\\nSort the second half, recursively. (1.2)\\nFind the maximum of the whole array by comparing the results of 1.1 and 1.2, and place it at the end of the list. (1.3)\\nSort the entire list (except for the maximum now at the end), recursively. (2)An unoptimized implementation in Haskell (purely functional) may look as follows:\\n\\nComplexity\\nThe runtime \\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle T(n)}\\n   for Slowsort is \\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        =\\n        2\\n        T\\n        (\\n        n\\n        \\n          /\\n        \\n        2\\n        )\\n        +\\n        T\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle T(n)=2T(n/2)+T(n-1)+1}\\n  .\\nA lower asymptotic bound for \\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle T(n)}\\n   in Landau notation is \\n  \\n    \\n      \\n        \\u03a9\\n        \\n          (\\n          \\n            n\\n            \\n              \\n                \\n                  \\n                    log\\n                    \\n                      2\\n                    \\n                  \\n                  \\u2061\\n                  (\\n                  n\\n                  )\\n                \\n                \\n                  (\\n                  2\\n                  +\\n                  \\u03f5\\n                  )\\n                \\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega \\\\left(n^{\\\\frac {\\\\log _{2}(n)}{(2+\\\\epsilon )}}\\\\right)}\\n   for any \\n  \\n    \\n      \\n        \\u03f5\\n        >\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\epsilon >0}\\n  .\\nSlowsort is therefore not in polynomial time. Even the best case is worse than Bubble sort.\\n\\n\\n== References ==\"}, {\"Smoothsort\": \"In computer science, smoothsort is a comparison-based sorting algorithm. A variant of heapsort, it was invented and published by Edsger Dijkstra in 1981. Like heapsort, smoothsort is an in-place algorithm with an upper bound of O(n log n), but it is not a stable sort.  The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n log n) regardless of the initial sorted state.\\n\\nOverview\\nLike heapsort, smoothsort organizes the input into a priority queue and then repeatedly extracts the maximum.  Also like heapsort, the priority queue is an implicit heap data structure (a heap-ordered implicit binary tree), which occupies a prefix of the array.  Each extraction shrinks the prefix and adds the extracted element to a growing sorted suffix.  When the prefix has shrunk to nothing, the array is completely sorted.\\nHeapsort maps the binary tree to the array using a top-down breadth-first traversal of the tree; the array begins with the root of the tree, then its two children, then four grandchildren, and so on.  Every element has a well-defined depth below the root of the tree, and every element except the root has its parent earlier in the array.  Its height above the leaves, however, depends on the size of the array.  This has the disadvantage that every element must be moved as part of the sorting process: it must pass through the root before being moved to its final location.\\nSmoothsort uses a different mapping, a bottom-up depth-first post-order traversal.  A left child is followed by the subtree rooted at its sibling, and a right child is followed by its parent.  Every element has a well-defined height above the leaves, and every non-leaf element has its children earlier in the array.  Its depth below the root, however, depends on the size of the array.  The algorithm is organized so the root is at the end of the heap, and at the moment that an element is extracted from the heap it is already in its final location and does not need to be moved.  Also, a sorted array is already a valid heap, and many sorted intervals are valid heap-ordered subtrees.\\nMore formally, every position i is the root of a unique subtree, whose nodes occupy a contiguous interval that ends at i.  An initial prefix of the array (including the whole array), might be such an interval corresponding to a subtree, but in general decomposes as a union of a number of successive such subtree intervals, which Dijkstra calls \\\"stretches\\\". Any subtree without a parent (i.e. rooted at a position whose parent lies beyond the prefix under consideration) gives a stretch in the decomposition of that interval, which decomposition is therefore unique. When a new node is appended to the prefix, one of two cases occurs: either the position is a leaf and adds a stretch of length 1 to the decomposition, or it combines with the last two stretches, becoming the parent of their respective roots, thus replacing the two stretches by a new stretch containing their union plus the new (root) position.\\nDijkstra noted that the obvious rule would be to combine stretches if and only if they have equal size, in which case all subtrees would be perfect binary trees of size 2k\\u22121.  However, he chose a different rule, which gives more possible tree sizes.  This has the same asymptotic efficiency, but gains a small constant factor in efficiency by requiring fewer stretches to cover each interval.\\nThe rule Dijkstra uses is that the last two stretches are combined if and only if their sizes are consecutive Leonardo numbers L(i+1) and L(i) (in that order), which numbers are recursively defined, in a manner very similar to the Fibonacci numbers, as:\\n\\nL(0) = L(1) = 1\\nL(k+2) = L(k+1) + L(k) + 1As a consequence, the size of any subtree is a Leonardo number. The sequence of stretch sizes decomposing the first n positions, for any n, can be found in a greedy manner: the first size is the largest Leonardo number not exceeding n, and the remainder (if any) is decomposed recursively. The sizes of stretches are decreasing, strictly so except possibly for two final sizes 1, and avoiding successive Leonardo numbers except possibly for the final two sizes.\\nIn addition to each stretch being a heap-ordered tree, the roots of the trees are maintained in sorted order.  This effectively adds a third child (which Dijkstra calls a \\\"stepson\\\") to each root linking it to the preceding root.  This combines all of the trees together into one global heap. with the global maximum at the end.\\nAlthough the location of each node's stepson is fixed, the link only exists for tree roots, meaning that links are removed whenever trees are merged.  This is different from ordinary children, which are linked as long as the parent exists.\\nIn the first (heap growing) phase of sorting, an increasingly large initial part of the array is reorganized so that the subtree for each of its stretches is a max-heap: the entry at any non-leaf position is at least as large as the entries at the positions that are its children.  In addition, all roots are at least as large as their stepsons.\\nIn the second (heap shrinking) phase, the maximal node is detached from the end of the array (without needing to move it) and the heap invariants are re-established among its children.  (Specifically, among the newly created stepsons.)\\nPractical implementation frequently needs to compute Leonardo numbers L(k).  Dijkstra provides clever code which uses a fixed number of integer variables to efficiently compute the values needed at the time they are needed.  Alternatively, if there is a finite bound N on the size of arrays to be sorted, a precomputed table of Leonardo numbers can be stored in O(log N) space.\\n\\nOperations\\nWhile the two phases of the sorting procedure are opposite to each other as far as the evolution of the sequence-of-heaps structure is concerned, they are implemented using one core primitive, equivalent to the \\n\\\"sift down\\\" operation in a binary max-heap.\\n\\nSifting down\\nThe core sift-down operation (which Dijkstra calls \\\"trinkle\\\") restores the heap invariant when it is possibly violated only at the root node.  If the root node is less than any of its children, it is swapped with its greatest child and the process repeated with the root node in its new subtree.\\nThe difference between smoothsort and a binary max-heap is that the root of each stretch must be ordered with respect to a third \\\"stepson\\\": the root of the preceding stretch.  So the sift-down procedure starts with a series of four-way comparisons (the root node and three children) until the stepson is not the maximal element, then a series of three-way comparisons (the root plus two children) until the root node finds its final home and the invariants are re-established.\\nEach tree is a full binary tree: each node has two children or none. There is no need to deal with the special case of one child which occurs in a standard implicit binary heap.  (But the special case of stepson links more than makes up for this saving.)\\nBecause there are O(log n) stretches, each of which is a tree of depth O(log n), the time to perform each sifting-down operation is bounded by O(log n).\\n\\nGrowing the heap region by incorporating an element to the right\\nWhen an additional element is considered for incorporation into the sequence of stretches (list of disjoint heap structures) it either forms a new one-element stretch, or it combines the two rightmost stretches by becoming the parent of both their roots and forming a new stretch that replaces the two in the sequence. Which of the two happens depends only on the sizes of the stretches currently present (and ultimately only on the index of the element added); Dijkstra stipulated that stretches are combined if and only if their sizes are L(k+1) and L(k) for some k, i.e., consecutive Leonardo numbers; the new stretch will have size L(k+2).\\nIn either case, the new element must be sifted down to its correct place in the heap structure.  Even if the new node is a one-element stretch, it must still be sorted relative to the preceding stretch's root.\\n\\nOptimization\\nDijkstra's algorithm saves work by observing that the full heap invariant is required at the end of the growing phase, but it is not required at every intermediate step.  In particular, the requirement that an element be greater than its stepson is only important for the elements which are the final tree roots.\\nTherefore, when an element is added, compute the position of its future parent.  If this is within the range of remaining values to be sorted, act as if there is no stepson and only sift down within the current tree.\\n\\nShrinking the heap region by separating the rightmost element from it\\nDuring this phase, the form of the sequence of stretches goes through the changes of the growing phase in reverse. No work at all is needed when separating off a leaf node, but for a non-leaf node its two children become roots of new stretches, and need to be moved to their proper place in the sequence of roots of stretches. This can be obtained by applying sift-down twice: first for the left child, and then for the right child (whose stepson was the left child).\\nBecause half of all nodes in a full binary tree are leaves, this performs an average of one sift-down operation per node.\\n\\nOptimization\\nIt is already known that the newly exposed roots are correctly ordered with respect to their normal children; it is only the ordering relative to their stepsons which is in question.  Therefore, while shrinking the heap, the first step of sifting down can be simplified to a single comparison with the stepson.  If a swap occurs, subsequent steps must do the full four-way comparison.\\n\\nAnalysis\\nSmoothsort takes O(n) time to process a presorted array, O(n log  n) in the worst case, and achieves nearly-linear performance on many nearly-sorted inputs.  However, it does not handle all nearly-sorted sequences optimally.  Using the count of inversions as a measure of un-sortedness (the number of pairs of indices i and j with i < j and A[i] > A[j]; for randomly sorted input this is approximately n2/4), there are possible input sequences with O(n log n) inversions which cause it to take \\u03a9(n log n) time, whereas other adaptive sorting algorithms can solve these cases in O(n log log n) time.The smoothsort algorithm needs to be able to hold in memory the sizes of all of the trees in the Leonardo heap.  Since they are sorted by order and all orders are distinct, this is usually done using a bit vector indicating which orders are present.  Moreover, since the largest order is at most O(log n), these bits can be encoded in O(1) machine words, assuming a transdichotomous machine model.\\nNote that O(1) machine words is not the same thing as one machine word.  A 32-bit vector would only suffice for sizes less than L(32) = 7049155.  A 64-bit vector will do for sizes less than L(64) = 34335360355129 \\u2248 245.  In general, it takes 1/log2(\\u03c6) \\u2248 1.44 bits of vector per bit of size.\\n\\nPoplar sort\\nA simpler algorithm inspired by smoothsort is poplar sort.  Named after the rows of trees of decreasing size often seen in Dutch polders, it performs fewer comparisons than smoothsort for inputs that are not mostly sorted, but cannot achieve linear time for sorted inputs.\\nThe significant change made by poplar sort in that the roots of the various trees are not kept in sorted order; there are no \\\"stepson\\\" links tying them together into a single heap.  Instead, each time the heap is shrunk in the second phase, the roots are searched to find the maximum entry.\\nBecause there are n shrinking steps, each of which must search O(log n) tree roots for the maximum, the best-case run time for poplar sort is  O(n log n).\\nThe authors also suggest using perfect binary trees rather than Leonardo trees to provide further simplification, but this is a less significant change.\\nThe same structure has been proposed as a general-purpose priority queue under the name post-order heap, achieving O(1) amortized insertion time in a structure simpler than an implicit binomial heap.\\n\\nApplications\\nThe musl C library uses smoothsort for its implementation of qsort().\\n\\nReferences\\nExternal links\\nCommented transcription of EWD796a, 16-Aug-1981\\nDetailed modern explanation of Smoothsort\\nwikibooks:Algorithm Implementation/Sorting/Smoothsort\\nDescription and example implementation of Poplar heap\\nNoshita, Kohei; Nakatani, Yoshinobu (April 1985). \\\"On the Nested Heap Structure in Smoothsort\\\". Mathematical Foundations of Computer Science and Their Applications (Japanese: \\u6570\\u7406\\u89e3\\u6790\\u7814\\u7a76\\u6240\\u8b1b\\u7a76\\u9332). 556: 1\\u201316.\"}, {\"Sort (C++)\": \"sort is a generic function in the C++ Standard Library for doing comparison sorting. The function originated in the Standard Template Library (STL).\\nThe specific sorting algorithm is not mandated by the language standard and may vary across implementations, but the worst-case asymptotic complexity of the function is specified: a call to sort must perform no more than O(N log N) comparisons when applied to a range of N elements.\\n\\nUsage\\nThe sort function is included from the <algorithm> header of the C++ Standard Library, and carries three arguments: RandomAccessIterator first, RandomAccessIterator last, Compare comp. Here, RandomAccessIterator is a templated type that must be a random access iterator, and first and last must define a sequence of values, i.e., last must be reachable from first by repeated application of the increment operator to first. The third argument, also of a templated type, denotes a comparison predicate. This comparison predicate must define a strict weak ordering on the elements of the sequence to be sorted. The third argument is optional; if not given, the \\\"less-than\\\" (<) operator is used, which may be overloaded in C++.\\nThis code sample sorts a given array of integers (in ascending order) and prints it out.\\n\\nThe same functionality using a vector container, using its begin and end methods to obtain iterators:\\n\\nGenericity\\nsort is specified generically, so that it can work on any random-access container and any way of determining that an element x of such a container should be placed before another element y.\\nAlthough generically specified, sort is not easily applied to all sorting problems. A particular problem that has been the subject of some study is the following:\\n\\nLet A and B be two arrays, where there exists some relation between the element A[i] and the element B[i] for all valid indices i.\\nSort A while maintaining the relation with B, i.e., apply the same permutation to B that sorts A.\\nDo the previous without copying the elements of A and B into a new array of pairs, sorting, and moving the elements back into the original arrays (which would require O(n) temporary space).A solution to this problem was suggested by A. Williams in 2002, who implemented a custom iterator type for pairs of arrays and analyzed some of the difficulties in correctly implementing such an iterator type. Williams's solution was studied and refined by K. \\u00c5hlander.\\n\\nComplexity and implementations\\nThe C++ standard requires that a call to sort performs O(N log N) comparisons when applied to a range of N elements.\\nIn previous versions of C++, such as C++03, only average complexity was required to be O(N log N). This was to allow the use of algorithms like (median-of-3) quicksort, which are fast in the average case, indeed significantly faster than other algorithms like heap sort with optimal worst-case complexity, and where the worst-case quadratic complexity rarely occurs. The introduction of hybrid algorithms such as introsort allowed both fast average performance and optimal worst-case performance, and thus the complexity requirements were tightened in later standards.\\nDifferent implementations use different algorithms. The GNU Standard C++ library, for example, uses a 3-part hybrid sorting algorithm: introsort is performed first (introsort itself being a hybrid of quicksort and heap sort), to a maximum depth given by 2\\u00d7log2 n, where n is the number of elements, followed by an insertion sort on the result.\\n\\nOther types of sorting\\nsort is not stable: equivalent elements that are ordered one way before sorting may be ordered differently after sorting. stable_sort ensures stability of result at expense of worse performance (in some cases), requiring only quasilinear time with exponent 2 \\u2013 O(n log2 n) \\u2013 if additional memory is not available, but linearithmic time O(n log n) if additional memory is available. This allows the use of in-place merge sort for in-place stable sorting and regular merge sort for stable sorting with additional memory.\\nPartial sorting is implemented by partial_sort, which takes a range of n elements and an integer m < n, and reorders the range so that the smallest m elements are in the first m positions in sorted order (leaving the remaining n \\u2212 m in the remaining positions, in some unspecified order). Depending on design this may be considerably faster than complete sort. Historically, this was commonly implemented using a heap-based algorithm that takes \\u0398(n + m log n) worst-case time. A better algorithm called quickselsort is used in the Copenhagen STL implementation, bringing the complexity down to \\u0398(n + m log m).Selection of the nth element is implemented by nth_element, which actually implements an in-place partial sort: it correctly sorts the nth element, and also ensures that this element partitions so elements before it are less than it, and elements after it are greater than it. There is the requirement that this takes linear time on average, but there is no worst-case requirement; these requirements are exactly met by quickselect, for any choice of pivot strategy.\\nSome containers, among them list, provide specialised version of sort as a member function. This is because linked lists don't have random access (and therefore can't use the regular sort function); and the specialised version also preserves the values list iterators point to.\\n\\nComparison to qsort\\nAside from sort, the C++ standard library also includes the qsort function from the C standard library. Compared to qsort, the templated sort is more type-safe since it does not require access to data items through unsafe void pointers, as qsort does. Also, qsort accesses the comparison function using a function pointer, necessitating large numbers of repeated function calls, whereas in sort, comparison functions may be inlined into the custom object code generated for a template instantiation. In practice, C++ code using sort is often considerably faster at sorting simple data like integers than equivalent C code using qsort.\\n\\nReferences\\nExternal links\\nC++ reference for std::sort\\nAnother C++ reference for std::sort\"}, {\"Sort (Unix)\": \"In computing, sort is a standard command line program of Unix and Unix-like operating systems, that prints the lines of its input or concatenation of all files listed in its argument list in sorted order. Sorting is done based on one or more sort keys extracted from each line of input. By default, the entire input is taken as sort key. Blank space is the default field separator. The command supports a number of command-line options that can vary by implementation. For instance the \\\"-r\\\" flag will reverse the sort order.\\n\\nHistory\\nA sort command that invokes a general sort facility was first implemented within Multics. Later, it appeared in Version 1 Unix. This version was originally written by Ken Thompson at AT&T Bell Laboratories. By Version 4 Thompson had modified it to use pipes, but sort retained an option to name the output file because it was used to sort a file in place. In Version 5, Thompson invented \\\"-\\\" to represent standard input.The version of sort bundled in GNU coreutils was written by Mike Haertel and Paul Eggert. This implementation employs the merge sort algorithm.\\nSimilar commands are available on many other operating systems, for example a sort command is part of ASCII's MSX-DOS2 Tools for MSX-DOS version 2.The sort command has also been ported to the IBM i operating system.\\n\\nSyntax\\nsort [OPTION]... [FILE]...\\n\\nWith no FILE, or when FILE is -, the command reads from standard input.\\n\\nParameters\\nExamples\\nSort a file in alphabetical order\\nSort by number\\nThe -n option makes the program sort according to numerical value. The du command produces output that starts with a number, the file size, so its output can be piped to sort to produce a list of files sorted by (ascending) file size:\\n\\nThe find command with the ls option prints file sizes in the 7th field, so a list of the LaTeX files sorted by file size is produced by:\\n\\nColumns or fields\\nUse the -k option to sort on a certain column. For example, use \\\"-k 2\\\" to sort on the second column. In old versions of sort, the +1 option made the program sort on the second column of data (+2 for the third, etc.). This usage is deprecated.\\n\\nSort on multiple fields\\nThe -k m,n option lets you sort on a key that is potentially composed of multiple fields (start at column m, end at column n):\\n\\nHere the first sort is done using column 2. -k2,2n specifies sorting on the key starting and ending with column 2, and sorting numerically.  If -k2 is used instead, the sort key would begin at column 2 and extend to the end of the line, spanning all the fields in between. -k1,1 dictates breaking ties using the value in column 1, sorting alphabetically by default. Note that bob, and chad have the same quota and are sorted alphabetically in the final output.\\n\\nSorting a pipe delimited file\\nSorting a tab delimited file\\nSorting a file with tab separated values requires a tab character to be specified as the column delimiter. This illustration uses the shell's dollar-quote notation\\nto specify the tab as a C escape sequence.\\n\\nSort in reverse\\nThe -r option just reverses the order of the sort:\\n\\nSort in random\\nThe GNU implementation has a -R --random-sort option based on hashing; this is not a full random shuffle because it will sort identical lines together. A true random sort is provided by the Unix utility shuf.\\n\\nSort by version\\nThe GNU implementation has a -V --version-sort option which is a natural sort of (version) numbers within text. Two text strings that are to be compared are split into blocks of letters and blocks of digits. Blocks of letters are compared alpha-numerically, and blocks of digits are compared numerically (i.e., skipping leading zeros, more digits means larger, otherwise the leftmost digits that differ determine the result). Blocks are compared left-to-right and the first non-equal block in that loop decides which text is larger. This happens to work for IP addresses, Debian package version strings and similar tasks where numbers of variable length are embedded in strings.\\n\\nSee also\\nCollation\\nList of Unix commands\\nuniq\\nshuf\\n\\nReferences\\nFurther reading\\nShotts (Jr), William E. (2012). The Linux Command Line: A Complete Introduction. No Starch Press. ISBN 978-1593273897.\\nMcElhearn, Kirk (2006). The Mac OS X Command Line: Unix Under the Hood. John Wiley & Sons. ISBN 978-0470113851.\\n\\nExternal links\\n\\nOriginal Sort manpage The original BSD Unix program's manpage\\nsort(1) \\u2013 Linux User Manual \\u2013 User Commands\\nsort(1) \\u2013 Plan 9 Programmer's Manual, Volume 1\\nsort(1) \\u2013 Inferno General commands Manual\\nFurther details about sort at Softpanorama\"}, {\"Sorting\": \"Sorting refers to ordering data in an increasing or decreasing manner according to some linear relationship among the data items.\\n\\nordering: arranging items in a sequence ordered by some criterion;\\ncategorizing: grouping items with similar properties.Ordering items is the combination of categorizing them based on equivalent order, and ordering the categories themselves.\\n\\nSorting information or data\\nIn computer science, arranging in an ordered sequence is called \\\"sorting\\\". Sorting is a common operation in many applications, and efficient algorithms to perform it have been developed.\\nThe most common uses of sorted sequences are:\\n\\nmaking lookup or search efficient;\\nmaking merging of sequences efficient.\\nenable processing of data in a defined order.The opposite of sorting, rearranging a sequence of items in a random or meaningless order, is called shuffling.\\nFor sorting, either a weak order, \\\"should not come after\\\", can be specified, or a strict weak order, \\\"should come before\\\" (specifying one defines also the other, the two are the complement of the inverse of each other, see operations on binary relations). For the sorting to be unique, these two are restricted to a total order and a strict total order, respectively.\\nSorting n-tuples (depending on context also called e.g. records consisting of fields) can be done based on one or more of its components. More generally objects can be sorted based on a property. Such a component or property is called a sort key.\\nFor example, the items are books, the sort key is the title, subject or author, and the order is alphabetical.\\nA new sort key can be created from two or more sort keys by lexicographical order. The first is then called the primary sort key, the second the secondary sort key, etc.\\nFor example, addresses could be sorted using the city as primary sort key, and the street as secondary sort key.\\nIf the sort key values are totally ordered, the sort key defines a weak order of the items: items with the same sort key are equivalent with respect to sorting. See also stable sorting. If different items have different sort key values then this defines a unique order of the items.\\n\\nA standard order is often called ascending (corresponding to the fact that the standard order of numbers is ascending, i.e. A to Z, 0 to 9), the reverse order descending (Z to A, 9 to 0).  For dates and times, ascending means that earlier values precede later ones e.g. 1/1/2000 will sort ahead of 1/1/2001.\\n\\nCommon sorting algorithms\\nBubble/Shell sort: Exchange two adjacent elements if they are out of order. Repeat until array is sorted.\\nInsertion sort: Scan successive elements for an out-of-order item, then insert the item in the proper place.\\nSelection sort: Find the smallest (or biggest) element in the array, and put it in the proper place. Swap it with the value in the first position. Repeat until array is sorted.\\nQuick sort: Partition the array into two segments. In the first segment, all elements are less than or equal to the pivot value. In the second segment, all elements are greater than or equal to the pivot value. Finally, sort the two segments recursively.\\nMerge sort: Divide the list of elements in two parts, sort the two parts individually and then merge it.\\n\\nPhysical sorting processes\\nVarious sorting tasks are essential in industrial processes. For example, during the extraction of gold from ore, a device called a shaker table uses gravity, vibration, and flow to separate gold from lighter materials in the ore (sorting by size and weight). Sorting is also a naturally occurring process that results in the concentration of ore or sediment. Sorting results from the application of some criterion or differential stressors to a mass to separate it into its components based on some variable quality. Materials that are different, but only slightly so, such as the isotopes of uranium, are very difficult to separate.\\nOptical sorting is an automated process of sorting solid products using cameras and/or lasers and has widespread use in the food industry. Sensor-based sorting is used in mineral processing.\\n\\nSee also\\nHelp:Sorting in Wikipedia tables. For sorting of categories, see Wikipedia:Categorization#Sort keys and for sorting of article sections, see WP:ORDER\\nCollation\\nIBM mainframe sort/merge\\nUnicode collation algorithm\\nKnolling\\n5S (methodology)\\n\\nReferences\\nExternal links\\n\\nDemonstration of Sorting Algorithms (includes bubble and quicksort)\\nAnimated video explaining bubble sort and quick sort and compares their performance.\"}, {\"Sorting network\": \"In computer science, comparator networks are abstract devices built up of a fixed number of \\\"wires\\\", carrying values, and comparator modules that connect pairs of wires, swapping the values on the wires if they are not in a desired order. Such networks are typically designed to perform sorting on fixed numbers of values, in which case they are called sorting networks.\\nSorting networks differ from general comparison sorts in that they are not capable of handling arbitrarily large inputs, and in that their sequence of comparisons is set in advance, regardless of the outcome of previous comparisons. In order to sort larger amounts of inputs, new sorting networks must be constructed. This independence of comparison sequences is useful for parallel execution and for implementation in hardware. Despite the simplicity of sorting nets, their theory is surprisingly deep and complex. Sorting networks were first studied circa 1954 by Armstrong, Nelson and O'Connor, who subsequently patented the idea.Sorting networks can be implemented either in hardware or in software. Donald Knuth describes how the comparators for binary integers can be implemented as simple, three-state electronic devices. Batcher, in 1968, suggested using them to construct switching networks for computer hardware, replacing both buses and the faster, but more expensive, crossbar switches. Since the 2000s, sorting nets (especially bitonic mergesort) are used by the GPGPU community for constructing sorting algorithms to run on graphics processing units.\\n\\nIntroduction\\nA sorting network consists of two types of items: comparators and wires. The wires are thought of as running from left to right, carrying values (one per wire) that traverse the network all at the same time. Each comparator connects two wires. When a pair of values, traveling through a pair of wires, encounter a comparator, the comparator swaps the values if and only if the top wire's value is greater or equal to the bottom wire's value.\\nIn a formula, if the top wire carries x and the bottom wire carries y, then after hitting a comparator the wires carry \\n  \\n    \\n      \\n        \\n          x\\n          \\u2032\\n        \\n        =\\n        min\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle x'=\\\\min(x,y)}\\n   and \\n  \\n    \\n      \\n        \\n          y\\n          \\u2032\\n        \\n        =\\n        max\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle y'=\\\\max(x,y)}\\n  , respectively, so the pair of values is sorted.:\\u200a635\\u200a A network of wires and comparators that will correctly sort all possible inputs into ascending order is called a sorting network or Kruskal hub. By reflecting the network, it is also possible to sort all inputs into descending order.\\nThe full operation of a simple sorting network is shown below. It is evident why this sorting network will correctly sort the inputs; note that the first four comparators will \\\"sink\\\" the largest value to the bottom and \\\"float\\\" the smallest value to the top. The final comparator sorts out the middle two wires.\\n\\nDepth and efficiency\\nThe efficiency of a sorting network can be measured by its total size, meaning the number of comparators in the network, or by its depth, defined (informally) as the largest number of comparators that any input value can encounter on its way through the network. Noting that sorting networks can perform certain comparisons in parallel (represented in the graphical notation by comparators that lie on the same vertical line), and assuming all comparisons to take unit time, it can be seen that the depth of the network is equal to the number of time steps required to execute it.:\\u200a636\\u2013637\\n\\nInsertion and Bubble networks\\nWe can easily construct a network of any size recursively using the principles of insertion and selection. Assuming we have a sorting network of size n, we can construct a network of size n + 1 by \\\"inserting\\\" an additional number into the already sorted subnet (using the principle behind insertion sort). We can also accomplish the same thing by first \\\"selecting\\\" the lowest value from the inputs and then sort the remaining values recursively (using the principle behind bubble sort).\\nThe structure of these two sorting networks are very similar. A construction of the two different variants, which collapses together comparators that can be performed simultaneously shows that, in fact, they are identical.The insertion network (or equivalently, bubble network) has a depth of 2n - 3, where n is the number of values. This is better than the O(n log n) time needed by random-access machines, but it turns out that there are much more efficient sorting networks with a depth of just O(log2 n), as described below.\\n\\nZero-one principle\\nWhile it is easy to prove the validity of some sorting networks (like the insertion/bubble sorter), it is not always so easy. There are n! permutations of numbers in an n-wire network, and to test all of them would take a significant amount of time, especially when n is large.  The number of test cases can be reduced significantly, to 2n, using the so-called zero-one principle.  While still exponential, this is smaller than n! for all n \\u2265 4, and the difference grows quite quickly with increasing n.\\nThe zero-one principle states that, if a sorting network can correctly sort all 2n sequences of zeros and ones, then it is also valid for arbitrary ordered inputs. This not only drastically cuts down on the number of tests needed to ascertain the validity of a network, it is of great use in creating many constructions of sorting networks as well.\\nThe principle can be proven by first observing the following fact about comparators: when a monotonically increasing function f is applied to the inputs, i.e., x and y are replaced by f(x) and f(y), then the comparator produces min(f(x), f(y)) = f(min(x, y)) and max(f(x), f(y)) = f(max(x, y)). By induction on the depth of the network, this result can be extended to a lemma stating that if the network transforms the sequence a1, ..., an into b1, ..., bn, it will transform f(a1), ..., f(an) into f(b1), ..., f(bn). Suppose that some input a1, ..., an contains two items ai < aj, and the network incorrectly swaps these in the output. Then it will also incorrectly sort f(a1), ..., f(an) for the function\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            {\\n            \\n              \\n                \\n                  1\\n                   \\n                \\n                \\n                  \\n                    \\n                      if \\n                    \\n                  \\n                  x\\n                  >\\n                  \\n                    a\\n                    \\n                      i\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                  0\\n                   \\n                \\n                \\n                  \\n                    \\n                      otherwise.\\n                    \\n                  \\n                \\n              \\n            \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)={\\\\begin{cases}1\\\\ &{\\\\mbox{if }}x>a_{i}\\\\\\\\0\\\\ &{\\\\mbox{otherwise.}}\\\\end{cases}}}\\n  This function is monotonic, so we have the zero-one principle as the contrapositive.:\\u200a640\\u2013641\\n\\nConstructing sorting networks\\nVarious algorithms exist to construct sorting networks of depth O(log2 n) (hence size O(n log2 n)) such as Batcher odd\\u2013even mergesort, bitonic sort, Shell sort, and the Pairwise sorting network. These networks are often used in practice.\\nIt is also possible to construct networks of depth O(log n) (hence size O(n log n)) using a construction called the AKS network, after its discoverers Ajtai, Koml\\u00f3s, and Szemer\\u00e9di. While an important theoretical discovery, the AKS network has very limited practical application because of the large linear constant hidden by the Big-O notation.:\\u200a653\\u200a These are partly due to a construction of an expander graph.\\nA simplified version of the AKS network was described by Paterson in 1990, who noted that \\\"the constants obtained for the depth bound still prevent the construction being of practical value\\\".A more recent construction called the zig-zag sorting network of size O(n log n) was discovered by Goodrich in 2014. While its size is much smaller than that of AKS networks, its depth O(n log n) makes it unsuitable for a parallel implementation.\\n\\nOptimal sorting networks\\nFor small, fixed numbers of inputs n, optimal sorting networks can be constructed, with either minimal depth (for maximally parallel execution) or minimal size (number of comparators). These networks can be used to increase the performance of larger sorting networks resulting from the recursive constructions of, e.g., Batcher, by halting the recursion early and inserting optimal nets as base cases. The following table summarizes the optimality results for small networks for which the optimal depth is known:\\n\\nFor larger networks neither the optimal depth nor the optimal size are currently known. The bounds known so far are provided in the table below:\\n\\nThe first sixteen depth-optimal networks are listed in Knuth's Art of Computer Programming, and have been since the 1973 edition; however, while the optimality of the first eight was established by Floyd and Knuth in the 1960s, this property wasn't proven for the final six until 2014 (the cases nine and ten having been decided in 1991).\\nFor one to twelve inputs, minimal (i.e. size-optimal) sorting networks are known, and for higher values, lower bounds on their sizes S(n) can be derived inductively using a lemma due to Van Voorhis (p. 240): S(n) \\u2265 S(n \\u2212 1) + \\u2308log2n\\u2309. The first ten optimal networks have been known since 1969, with the first eight again being known as optimal since the work of Floyd and Knuth, but optimality of the cases n = 9 and n = 10 took until 2014 to be resolved.\\nThe optimality of the smallest known sorting networks for n = 11 and n = 12 was resolved in 2020.Some work in designing optimal sorting network has been done using genetic algorithms: D. Knuth mentions that the smallest known sorting network for n = 13 was found by Hugues Juill\\u00e9 in 1995 \\\"by simulating an evolutionary process of genetic breeding\\\" (p. 226), and that the minimum depth sorting networks for n = 9 and n = 11 were found by Loren Schwiebert in 2001 \\\"using genetic methods\\\" (p. 229).\\n\\nComplexity of testing sorting networks\\nUnless P=NP, the problem of testing whether a candidate network is a sorting network is likely to remain difficult for networks of large sizes, due to the problem being co-NP-complete.\\n\\nReferences\\nAngel, O.; Holroyd, A. E.; Romik, D.; Vir\\u00e1g, B. (2007). \\\"Random sorting networks\\\". Advances in Mathematics. 215 (2): 839\\u2013868. arXiv:math/0609538. doi:10.1016/j.aim.2007.05.019.\\n\\nExternal links\\nList of smallest sorting networks for given number of inputs\\nSorting Networks\\nCHAPTER 28: SORTING NETWORKS\\nSorting Networks\\nTool for generating and graphing sorting networks\\nSorting networks and the END algorithm\\nLipton, Richard J.; Regan, Ken (24 April 2014). \\\"Galactic Sorting Networks\\\". G\\u00f6del\\u2019s Lost Letter and P=NP.\\nSorting Networks validity\"}, {\"Spaghetti sort\": \"Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items, introduced by A. K. Dewdney in his Scientific American column. This algorithm sorts a sequence of items requiring O(n) stack space in a stable manner. It requires a parallel processor.\\n\\nAlgorithm\\nFor simplicity, assume we are sorting a list of natural numbers. The sorting method is illustrated using uncooked rods of spaghetti:\\n\\nFor each number x in the list, obtain a rod of length x. (One practical way of choosing the unit is to let the largest number m in the list correspond to one full rod of spaghetti. In this case, the full rod equals m spaghetti units. To get a rod of length x, break a rod in two so that one piece is of length x units; discard the other piece.)\\nOnce you have all your spaghetti rods, take them loosely in your fist and lower them to the table, so that they all stand upright, resting on the table surface. Now, for each rod, lower your other hand from above until it meets with a rod\\u2014this one is clearly the longest. Remove this rod and insert it into the front of the (initially empty) output list (or equivalently, place it in the last unused slot of the output array). Repeat until all rods have been removed.\\n\\nAnalysis\\nPreparing the n rods of spaghetti takes linear time. Lowering the rods on the table takes constant time, O(1). This is possible because the hand, the spaghetti rods and the table work as a fully parallel computing device. There are then n rods to remove so, assuming each contact-and-removal operation takes constant time, the worst-case time complexity of the algorithm is O(n).\\n\\nReferences\\nExternal links\\nA. K. Dewdney's homepage\\nImplementations of a model of physical sorting, Boole Centre for Research in Informatics\\nClassical/Quantum Computing, IFF-Institute\"}, {\"Splaysort\": \"In computer science, splaysort is an adaptive comparison sorting algorithm based on the splay tree data structure.\\n\\nAlgorithm\\nThe steps of the algorithm are:\\n\\nInitialize an empty splay tree\\nFor each data item in the input order, insert it into the splay tree\\nTraverse the splay tree in inorder to find the sorted order of the dataThus, the algorithm may be seen as a form of insertion sort or tree sort, using a splay tree to speed up each insertion.\\n\\nAnalysis\\nBased on the amortized analysis of splay trees, the worst case running time of splaysort, on an input with n data items, is O(n log n), matching the time bounds for efficient non-adaptive algorithms such as quicksort, heap sort, and merge sort.\\nFor an input sequence in which most items are placed close to their predecessor in the sorted order, or are out of order with only a small number of other items, splaysort can be faster than O(n log n), showing that it is an adaptive sort. To quantify this, let dx be the number of positions in the input that separate x from its predecessor, and let ix be the number of items that appear on one side of x in the input and on the other side of x in the output (the number of inversions that involve x). Then it follows from the dynamic finger theorem for splay trees that the total time for splaysort is bounded by\\n\\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            x\\n          \\n        \\n        log\\n        \\u2061\\n        \\n          d\\n          \\n            x\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{x}\\\\log d_{x}}\\n  and by\\n\\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            x\\n          \\n        \\n        log\\n        \\u2061\\n        \\n          i\\n          \\n            x\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{x}\\\\log i_{x}}\\n  .Splaysort can also be shown to be adaptive to the entropy of the input sequence.\\n\\nExperimental results\\nIn experiments by Moffat, Eddy & Petersson (1996), splaysort was slower than quicksort on tables of random numbers by a factor of 1.5 to 2, and slower than mergesort by smaller factors. For data consisting of larger records, again in a random order, the additional amount of data movement performed by quicksort significantly slowed it down compared to pointer-based algorithms, and the times for splaysort and mergesort were very close to each other. However, for nearly presorted input sequences (measured in terms of the number of contiguous monotone subsequences in the data, the number of inversions, the number of items that must be removed to make a sorted subsequence, or the number of non-contiguous monotone subsequences into which the input can be partitioned) splaysort became significantly more efficient than the other algorithms.Elmasry & Hammad (2005) compared splaysort to several other algorithms that are adaptive to the total number of inversions in the input, as well as to quicksort. They found that, on the inputs that had few enough inversions to make an adaptive algorithm faster than quicksort, splaysort was the fastest algorithm.\\n\\nVariations\\nSaikkonen & Soisalon-Soininen (2012) modify splaysort to be more strongly adaptive to the number of contiguous monotone subsequences in the input, and report on experiments showing that the resulting algorithm is faster on inputs that are nearly presorted according to this measure.\\n\\n\\n== References ==\"}, {\"Spreadsort\": \"Spreadsort is a sorting algorithm invented by Steven J. Ross in 2002. It combines concepts from distribution-based sorts, such as radix sort and bucket sort, with partitioning concepts from comparison sorts such as quicksort and mergesort. In experimental results it was shown to be highly efficient, often outperforming traditional algorithms such as quicksort, particularly on distributions exhibiting structure and string sorting. There is an open-source implementation with performance analysis and benchmarks, and HTML documentation\\n.Quicksort identifies a pivot element in the list and then partitions the list into two sublists, those elements less than the pivot and those greater than the pivot. Spreadsort generalizes this idea by partitioning the list into n/c partitions at each step, where n is the total number of elements in the list and c is a small constant (in practice usually between 4 and 8 when comparisons are slow, or much larger in situations where they are fast). It uses distribution-based techniques to accomplish this, first locating the minimum and maximum value in the list, and then dividing the region between them into n/c equal-sized bins.\\nWhere caching is an issue, it can help to have a maximum number of bins in each recursive division step, causing this division process to take multiple steps.  Though this causes more iterations, it reduces cache misses and can make the algorithm run faster overall.\\nIn the case where the number of bins is at least the number of elements, spreadsort degenerates to bucket sort and the sort completes. Otherwise, each bin is sorted recursively. The algorithm uses heuristic tests to determine whether each bin would be more efficiently sorted by spreadsort or some other classical sort algorithm, then recursively sorts the bin.\\nLike other distribution-based sorts, spreadsort has the weakness that the programmer is required to provide a means of converting each element into a numeric key, for the purpose of identifying which bin it falls in. Although it is possible to do this for arbitrary-length elements such as strings by considering each element to be followed by an infinite number of minimum values, and indeed for any datatype possessing a total order, this can be more difficult to implement correctly than a simple comparison function, especially on complex structures. Poor implementation of this value function can result in clustering that harms the algorithm's relative performance.\\n\\nPerformance\\nThe worst-case performance of spreadsort is O(n log n) for small data sets, as it uses introsort as a fallback. In the case of distributions where the size of the key in bits k times 2 is roughly the square of the log of the list size n or smaller (2k < (log n)2), it does better in the worst case, achieving O(n \\u221ak - log n) worst-case time for the originally published version, and O(n\\u00b7((k/s) + s)) for the cache aware version.  For many real sorting problems with over 1000 items, including string sorting, this asymptotic worst-case is better than O(n log n).\\nExperiments were done comparing an optimized version of spreadsort to the highly optimized C++ std::sort, implemented with introsort. On lists of integers and floats spreadsort shows a roughly 2\\u20137\\u00d7 runtime improvement for random data on various operating systems.[1]\\nIn space performance, spreadsort is worse than most in-place algorithms: in its simplest form, it is not an in-place algorithm, using O(n) extra space; in experiments, about 20% more than quicksort using a c of 4\\u20138.  With a cache-aware form (as included in Boost.Sort), less memory is used and there is an upper bound on memory usage of the maximum bin count times the maximum number of recursions, which ends up being a few kilobytes times the size of the key in bytes.  Although it uses asymptotically more space than the O(log n) overhead of quicksort or the O(1) overhead of heapsort, it uses considerably less space than the basic form of mergesort, which uses auxiliary space equal to the space occupied by the list.\\n\\nImplementation\\nTwo Levels are as Good as Any\\nAn interesting result for algorithms of this general type (splitting based on the radix, then comparison-based sorting) is that they are O(n) for any bounded and integrable probability density function.  This result can be obtained by forcing Spreadsort to always iterate one more time if the bin size after the first iteration is above some constant value.  If the key density function is known to be Riemann integrable and bounded, this modification of Spreadsort can attain some performance improvement over the basic algorithm, and will have better worst-case performance.  If this restriction cannot usually be depended on, this change will add a little extra runtime overhead to the algorithm and gain little.  Other similar algorithms are Flashsort (which is simpler) and Adaptive Left Radix.  Adaptive Left Radix is apparently quite similar, the main difference being recursive behavior, with Spreadsort checking for worst-case situations and using std::sort to avoid performance problems where necessary, and Adaptive Left Radix recursing continuously until done or the data is small enough to use insertion sort.\\n\\n\\n== References ==\"}, {\"Stooge sort\": \"Stooge sort is a recursive sorting algorithm. It is notable for its exceptionally bad time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...).\\nThe running time of the algorithm is thus slower compared to reasonable sorting algorithms, and is slower than bubble sort, a canonical example of a fairly inefficient sort. It is however more efficient than Slowsort. The name comes from The Three Stooges.The algorithm is defined as follows:\\n\\nIf the value at the start is larger than the value at the end, swap them.\\nIf there are 3 or more elements in the list, then:\\nStooge sort the initial 2/3 of the list\\nStooge sort the final 2/3 of the list\\nStooge sort the initial 2/3 of the list againIt is important to get the integer sort size used in the recursive calls by rounding the 2/3 upwards, e.g. rounding 2/3 of 5 should give 4 rather than 3, as otherwise the sort can fail on certain data.\\n\\nImplementation\\nReferences\\nSources\\nBlack, Paul E. \\\"stooge sort\\\". Dictionary of Algorithms and Data Structures. National Institute of Standards and Technology. Retrieved 18 June 2011.\\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001) [1990]. \\\"Problem 7-3\\\". Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. pp. 161\\u2013162. ISBN 0-262-03293-7.\\n\\nExternal links\\nSorting Algorithms (including Stooge sort)\\nStooge sort \\u2013 implementation and comparison\"}, {\"Strand sort\": \"Strand sort is a recursive sorting algorithm that sorts items of a list into increasing order. It has O(n2) worst time complexity which occurs when the input list is reverse sorted. It has a best case time complexity of O(n) which occurs when the input is a list that is already sorted.The algorithm first moves the first element of a list into a sub-list. It then compares the last element in the sub-list to each subsequent element in the original list. Once there is an element in the original list that is greater than the last element in the sub-list, the element is removed from the original list and added to the sub-list. This process continues until the last element in the sub-list is compared to the remaining elements in the original list. The sub-list is then merged into a new list. Repeat this process and merge all sub-lists until all elements are sorted. This algorithm is called strand sort because there are strands of sorted elements within the unsorted elements that are removed one at a time. This algorithm is also used in J Sort for fewer than 40 elements.\\n\\nExample\\nThis example is based on the description of the algorithm provided in the book, IT Enabled Practices and Emerging Management Paradigms.Step 1: Start with a list of numbers: {5, 1, 4, 2, 0, 9, 6, 3, 8, 7 }\\nStep 2: Next move the first element of the list into a new sub-list:  sub-list contains {5}\\nStep 3: Then iterate through the original list and compare each number to 5 until there is a number greater than 5.\\n\\n1 < 5 so 1 is not added to the sub-list.\\n4 < 5 so 4 is not added to the sub-list.\\n2 < 5 so 2 is not added to the sub-list.\\n0 < 5 so 0  is not added to the sub-list.\\n9 > 5 so 9 is added to the sub-list and removed from the original list.Step 4: Now compare 9 with the remaining elements in the original list until there is a number greater than 9.  \\n\\n6 < 9 so 6 is not added to the sub-list.\\n3 < 9 so 3 is not added to the sub-list.\\n8 < 9 so 8 is not added to the sub-list.\\n7 < 9 so 7 is not added to the sub-list.Step 5: Now there are no more elements to compare 9 to so merge the sub-list into a new list, called solution-list.\\nAfter step 5, the original list contains {1, 4, 2, 0, 6, 3, 8, 7}\\nThe sub-list is empty, and the solution list contains {5, 9}\\nStep 6: Move the first element of the original list into sub-list: sub-list contains {1}\\nStep 7: Iterate through the original list and compare each number to 1 until there is a number greater than 1.\\n\\n4 > 1 so 4 is added to the sub-list and 4 is removed from the original list.Step 8: Now compare 4 with the remaining elements in the original list until there is a number greater than 4.\\n\\n2 < 4 so 2 is not added to the sub-list.\\n0 < 4 so 0 is not added to the sub-list.\\n6 > 4 so 6 is added to the sub-list and is removed from the original list.Step 9: Now compare 6 with the remaining elements in the original list until there is a number greater than 6.  \\n\\n3 < 6 so 3 is not added to the sub-list.\\n8 > 6 so 8 is added to the sub-list and is removed from the original list.Step 10: Now compare 8 with the remaining elements in the original list until there is a number greater than 8.\\n\\n7 < 8 so 7 is not added to the sub-list.Step 11: Since there are no more elements in the original list to compare {8} to, the sub-list is merged with the solution list. Now the original list contains {2, 0, 3, 7}, the sub-list is empty and the solution-list contains: {1, 4, 5, 6, 8, 9}.\\nStep 12:  Move the first element of the original list into sub-list. Sub-list contains {2}\\nStep 13: Iterate through the original list and compare each number to 2 until there is a number greater than 2.\\n\\n0 < 2 so 0 is not added to the sub-list.\\n3 > 2 so 3 is added to the sub-list and is removed from the original list.Step 14: Now compare 3 with the remaining elements in the original list until there is a number greater than 3.\\n\\n7 > 3 so 7 is added to the sub-list and is removed from the original list.Step 15: Since there are no more elements in the original list to compare {7} to, the sub-list is merged with the solution list. The original list now contains {0}, the sub-list is empty, and solution list contains: {1, 2, 3, 4, 5, 6, 7, 8, 9}.\\nStep 16:  Move the first element of the original list into sub-list. Sub-list contains {0}.\\nStep 17:  Since the original list is now empty, the sub-list is merged with the solution list. The solution list now contains: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}. There are now no more elements in the original list, and all of the elements in the solution list have successfully been sorted into increasing numerical order.\\n\\nImplementation\\nSince Strand Sort requires many insertions and deletions, it is best to use a linked list when implementing the algorithm. Linked lists require constant time for both insertions and removals of elements using iterators. The time to traverse through the linked list is directly related to the input size of the list. The following implementation is done in Java 8 and is based on the description of the algorithm from the book, IT Enabled Practices and Emerging Management Paradigms.\\n\\n\\n== References ==\"}, {\"Stupid sort\": \"Stupid sort may refer to:\\n\\nBogosort, based on the generate and test paradigm\\nGnome sort, similar to insertion sort\"}, {\"Timsort\": \"Timsort is a hybrid, stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. It was implemented by Tim Peters in 2002 for use in the Python programming language. The algorithm finds subsequences of the data that are already ordered (runs) and uses them to sort the remainder more efficiently. This is done by merging runs until certain criteria are fulfilled. Timsort has been Python's standard sorting algorithm since version 2.3. It is also used to sort arrays of non-primitive type in Java SE 7, on the Android platform, in GNU Octave, on V8, Swift, and Rust.It uses techniques from Peter McIlroy's 1993 paper \\\"Optimistic Sorting and Information Theoretic Complexity\\\".\\n\\nOperation\\nTimsort was designed to take advantage of runs of consecutive ordered elements that already exist in most real-world data, natural runs. It iterates over the data collecting elements into runs and simultaneously putting those runs in a stack. Whenever the runs on the top of the stack match a merge criterion, they are merged. This goes on until all data is traversed; then, all runs are merged two at a time and only one sorted run remains. The advantage of merging ordered runs instead of merging fixed size sub-lists (as done by traditional mergesort) is that it decreases the total number of comparisons needed to sort the entire list.\\nEach run has a minimum size, which is based on the size of the input and is defined at the start of the algorithm. If a run is smaller than this minimum run size, insertion sort is used to add more elements to the run until the minimum run size is reached.\\n\\nMerge criteria\\nTimsort is a stable sorting algorithm (order of elements with same key is kept) and strives to perform balanced merges (a merge thus merges runs of similar sizes).\\nIn order to achieve sorting stability, only consecutive runs are merged. Between two non-consecutive runs, there can be an element with the same key inside the runs. Merging those two runs would change the order of equal keys. Example of this situation ([] are ordered runs):  [1 2 2] 1 4 2 [0 1 2]\\nIn pursuit of balanced merges, Timsort considers three runs on the top of the stack, X, Y, Z, and maintains the invariants:\\n\\nIf any of these invariants is violated, Y is merged with the smaller of X or Z and the invariants are checked again. Once the invariants hold, the search for a new run in the data can start. These invariants maintain merges as being approximately balanced while maintaining a compromise between delaying merging for balance, exploiting fresh occurrence of runs in cache memory and making merge decisions relatively simple.\\n\\nMerge space overhead\\nThe original merge sort implementation is not in-place and it has a space overhead of N (data size). In-place merge sort implementations exist, but have a high time overhead. In order to achieve a middle term, Timsort performs a merge sort with a small time overhead and smaller space overhead than N.\\nFirst, Timsort performs a binary search to find the location where the first element of the second run would be inserted in the first ordered run, keeping it ordered. Then, it performs the same algorithm to find the location where the last element of the first run would be inserted in the second ordered run, keeping it ordered. Elements before and after these locations are already in their correct place and do not need to be merged. Then, the smaller of the remaining elements of the two runs is copied into temporary memory, and elements are merged with the larger run into the now free space.  If the first run is smaller, the merge starts at the beginning; if the second is smaller, the merge starts at the end. This optimization reduces the number of required element movements, the running time and the temporary space overhead in the general case.\\nExample: two runs [1, 2, 3, 6, 10] and [4, 5, 7, 9, 12, 14, 17] must be merged. Note that both runs are already sorted individually. The smallest element of the second run is 4 and it would have to be added at the fourth position of the first run in order to preserve its order (assuming that the first position of a run is 1). The largest element of the first run is 10 and it would have to be added at the fifth position of the second run in order to preserve its order. Therefore, [1, 2, 3] and [12, 14, 17] are already in their final positions and the runs in which elements movements are required are [6, 10] and [4, 5, 7, 9]. With this knowledge, we only need to allocate a temporary buffer of size 2 instead of 4.\\n\\nMerge direction\\nMerging can be done in both directions: left-to-right, as in the traditional mergesort, or right-to-left.\\n\\nGalloping mode during merge\\nAn individual merge of runs R1 and R2 keeps the count of consecutive elements selected from a run. When this number reaches the minimum galloping threshold (min_gallop), Timsort considers that it is likely that many consecutive elements may still be selected from that run and switches to the galloping mode. Let us assume that R1 is responsible for triggering it. In this mode, the algorithm performs an exponential search, also known as galloping search, for the next element x of the run R2 in the run R1. This is done in two stages: the first one finds the range (2k \\u2212 1, 2k+1 - 1) where x is. The second stage performs a binary search for the element x in the range found in the first stage. The galloping mode is an attempt to adapt the merge algorithm to the pattern of intervals between elements in runs.\\n\\nGalloping is not always efficient. In some cases galloping mode requires more comparisons than a simple linear search. According to benchmarks done by the developer, galloping is beneficial only when the initial element of one run is not one of the first seven elements of the other run. This implies an initial threshold of 7. To avoid the drawbacks of galloping mode, two actions are taken: (1) When galloping is found to be less efficient than binary search, galloping mode is exited. (2)\\nThe success or failure of galloping is used to adjust min_gallop. If the selected element is from the same array that returned an element previously, min_gallop is reduced by one, thus encouraging the return to galloping mode. Otherwise, the value is incremented by one, thus discouraging a return to galloping mode. In the case of random data, the value of min_gallop becomes so large that galloping mode never recurs.\\n\\nDescending runs\\nIn order to also take advantage of data sorted in descending order, Timsort reverses strictly descending runs when it finds them and adds them to the stack of runs. Since descending runs are later blindly reversed, excluding runs with equal elements maintains the algorithm's stability; i.e., equal elements won't be reversed.\\n\\nMinimum run size\\nBecause merging is most efficient when the number of runs is equal to, or slightly less than, a power of two, and notably less efficient when the number of runs is slightly more than a power of two, Timsort chooses minrun to try to ensure the former condition.Minrun is chosen from the range 32 to 64 inclusive, such that the size of the data, divided by minrun, is equal to, or slightly less than, a power of two.  The final algorithm takes the six most significant bits of the size of the array, adds one if any of the remaining bits are set, and uses that result as the minrun.  This algorithm works for all arrays, including those smaller than 64; for arrays of size 63 or less, this sets minrun equal to the array size and Timsort reduces to an insertion sort.\\n\\nAnalysis\\nIn the worst case, Timsort takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   comparisons to sort an array of n elements. In the best case, which occurs when the input is already sorted, it runs in linear time, meaning that it is an adaptive sorting algorithm.It is superior to Quicksort for sorting object references or pointers because these require expensive memory indirection to access data and perform comparisons and Quicksort's cache coherence benefits are greatly reduced.\\n\\nFormal verification\\nIn 2015, Dutch and German researchers in the EU FP7 ENVISAGE project found a bug in the standard implementation of Timsort. It was fixed in 2015 in Python, Java and Android.\\nSpecifically, the invariants on stacked run sizes ensure a tight upper bound on the maximum size of the required stack.  The implementation preallocated a stack sufficient to sort 264 bytes of input, and avoided further overflow checks.\\nHowever, the guarantee requires the invariants to apply to every group of three consecutive runs, but the implementation only checked it for the top three.  Using the KeY tool for formal verification of Java software, the researchers found that this check is not sufficient, and they were able to find run lengths (and inputs which generated those run lengths) which would result in the invariants being violated deeper in the stack after the top of the stack was merged.As a consequence, for certain inputs the allocated size is not sufficient to hold all unmerged runs. In Java, this generates for those inputs an array-out-of-bound exception. The smallest input that triggers this exception in Java and Android v7 is of size 67108864 (226). (Older Android versions already triggered this exception for certain inputs of size 65536 (216))\\nThe Java implementation was corrected by increasing the size of the preallocated stack based on an updated worst-case analysis.  The article also showed by formal methods how to establish the intended invariant by checking that the four topmost runs in the stack satisfy the two rules above.  This approach was adopted by Python and Android.\\n\\nReferences\\nFurther reading\\nAuger, Nicolas; Nicaud, Cyril; Pivoteau, Carine (2015). \\\"Merge Strategies: from Merge Sort to TimSort\\\". hal-01212839.\\nAuger, Jug\\u00e9, Nicaud, Pivoteau (2018). \\\"On the Worst-Case Complexity of TimSort\\\". ESA 2018.\\nSam Buss, Alexander Knop. \\\"Strategies for Stable Merge Sorting.\\\" SODA 2019.\\n\\nExternal links\\ntimsort.txt \\u2013 original explanation by Tim Peters\"}, {\"Topological sorting\": \"In computer science, a topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. Precisely, a topological sort is a graph traversal in which each node v is visited only after all its dependencies are visited. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time. Topological sorting has many applications especially in ranking problems such as feedback arc set. Topological sorting is possible even when the DAG has disconnected components.\\n\\nExamples\\nThe canonical application of topological sorting is in scheduling a sequence of jobs or tasks based on their dependencies. The jobs are represented by vertices, and there is an edge from x to y if job x must be completed before job y can be started (for example, when washing clothes, the washing machine must finish before we put the clothes in the dryer). Then, a topological sort gives an order in which to perform the jobs.  A closely related application of topological sorting algorithms was first studied in the early 1960s in the context of the PERT technique for scheduling in project management. In this application, the vertices of a graph represent the milestones of a project, and the edges represent tasks that must be performed between one milestone and another. Topological sorting forms the basis of linear-time algorithms for finding the critical path of the project, a sequence of milestones and tasks that controls the length of the overall project schedule.\\nIn computer science, applications of this type arise in instruction scheduling, ordering of formula cell evaluation when recomputing formula values in spreadsheets, logic synthesis, determining the order of compilation tasks to perform in makefiles, data serialization, and resolving symbol dependencies in linkers. It is also used to decide in which order to load tables with foreign keys in databases.\\n\\nAlgorithms\\nThe usual algorithms for topological sorting have running time linear in the number of nodes plus the number of edges, asymptotically, \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          |\\n          \\n            V\\n          \\n          |\\n        \\n        +\\n        \\n          |\\n          \\n            E\\n          \\n          |\\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle O(\\\\left|{V}\\\\right|+\\\\left|{E}\\\\right|).}\\n\\nKahn's algorithm\\nOne of these algorithms, first described by Kahn (1962), works by choosing vertices in the same order as the eventual topological sort. First, find a list of \\\"start nodes\\\" which have no incoming edges and insert them into a set S; at least one such node must exist in a non-empty acyclic graph. Then:\\n\\nL \\u2190 Empty list that will contain the sorted elements\\nS \\u2190 Set of all nodes with no incoming edge\\n\\nwhile S is not empty do\\n    remove a node n from S\\n    add n to L\\n    for each node m with an edge e from n to m do\\n        remove edge e from the graph\\n        if m has no other incoming edges then\\n            insert m into S\\n\\nif graph has edges then\\n    return error   (graph has at least one cycle)\\nelse \\n    return L   (a topologically sorted order)\\n\\nIf the graph is a DAG, a solution will be contained in the list L (the solution is not necessarily unique). Otherwise, the graph must have at least one cycle and therefore a topological sort is impossible.\\nReflecting the non-uniqueness of the resulting sort, the structure S can be simply a set or a queue or a stack. Depending on the order that nodes n are removed from set S, a different solution is created. A variation of Kahn's algorithm that breaks ties lexicographically forms a key component of the Coffman\\u2013Graham algorithm for parallel scheduling and layered graph drawing.\\n\\nDepth-first search\\nAn alternative algorithm for topological sorting is based on depth-first search. The algorithm loops through each node of the graph, in an arbitrary order, initiating a depth-first search that terminates when it hits any node that has already been visited since the beginning of the topological sort or the node has no outgoing edges (i.e. a leaf node):\\n\\nL \\u2190 Empty list that will contain the sorted nodes\\nwhile exists nodes without a permanent mark do\\n    select an unmarked node n\\n    visit(n)\\n\\nfunction visit(node n)\\n    if n has a permanent mark then\\n        return\\n    if n has a temporary mark then\\n        stop   (graph has at least one cycle)\\n\\n    mark n with a temporary mark\\n\\n    for each node m with an edge from n to m do\\n        visit(m)\\n\\n    remove temporary mark from n\\n    mark n with a permanent mark\\n    add n to head of L\\n\\nEach node n gets prepended to the output list L only after considering all other nodes which depend on n (all descendants of n in the graph).  Specifically, when the algorithm adds node n, we are guaranteed that all nodes which depend on n are already in the output list L: they were added to L either by the recursive call to visit() which ended before the call to visit n, or by a call to visit() which started even before the call to visit n.  Since each edge and node is visited once, the algorithm runs in linear time. This depth-first-search-based algorithm is the one described by Cormen et al. (2001); it seems to have been first described in print by Tarjan in 1976.\\n\\nParallel algorithms\\nOn a parallel random-access machine, a topological ordering can be constructed in O(log2 n) time using a polynomial number of processors, putting the problem into the complexity class NC2.\\nOne method for doing this is to repeatedly square the adjacency matrix of the given graph, logarithmically many times, using min-plus matrix multiplication with maximization in place of minimization. The resulting matrix describes the longest path distances in the graph. Sorting the vertices by the lengths of their longest incoming paths produces a topological ordering.An algorithm for parallel topological sorting on distributed memory machines parallelizes the algorithm of Kahn for a DAG \\n  \\n    \\n      \\n        G\\n        =\\n        (\\n        V\\n        ,\\n        E\\n        )\\n      \\n    \\n    {\\\\displaystyle G=(V,E)}\\n  . On a high level, the algorithm of Kahn repeatedly removes the vertices of indegree 0 and adds them to the topological sorting in the order in which they were removed. Since the outgoing edges of the removed vertices are also removed, there will be a new set of vertices of indegree 0, where the procedure is repeated until no vertices are left. This algorithm performs \\n  \\n    \\n      \\n        D\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle D+1}\\n   iterations, where D is the longest path in G. Each iteration can be parallelized, which is the idea of the following algorithm.\\nIn the following it is assumed that the graph partition is stored on p processing elements (PE) which are labeled \\n  \\n    \\n      \\n        0\\n        ,\\n        \\u2026\\n        ,\\n        p\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle 0,\\\\dots ,p-1}\\n  . Each PE i initializes a set of local vertices \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            i\\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{i}^{1}}\\n   with indegree 0, where the upper index represents the current iteration. Since all vertices in the local sets \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            0\\n          \\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          Q\\n          \\n            p\\n            \\u2212\\n            1\\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{0}^{1},\\\\dots ,Q_{p-1}^{1}}\\n   have indegree 0, i.e. they are not adjacent, they can be given in an arbitrary order for a valid topological sorting. To assign a global index to each vertex, a prefix sum is calculated over the sizes of \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            0\\n          \\n          \\n            1\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          Q\\n          \\n            p\\n            \\u2212\\n            1\\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{0}^{1},\\\\dots ,Q_{p-1}^{1}}\\n  . So each step, there are \\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            p\\n            \\u2212\\n            1\\n          \\n        \\n        \\n          |\\n        \\n        \\n          Q\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n      \\n    \\n    {\\\\textstyle \\\\sum _{i=0}^{p-1}|Q_{i}|}\\n   vertices added to the topological sorting.\\n\\nIn the first step, PE j assigns the indices \\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            j\\n            \\u2212\\n            1\\n          \\n        \\n        \\n          |\\n        \\n        \\n          Q\\n          \\n            i\\n          \\n          \\n            1\\n          \\n        \\n        \\n          |\\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                0\\n              \\n              \\n                j\\n              \\n            \\n            \\n              |\\n            \\n            \\n              Q\\n              \\n                i\\n              \\n              \\n                1\\n              \\n            \\n            \\n              |\\n            \\n          \\n          )\\n        \\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\textstyle \\\\sum _{i=0}^{j-1}|Q_{i}^{1}|,\\\\dots ,\\\\left(\\\\sum _{i=0}^{j}|Q_{i}^{1}|\\\\right)-1}\\n   to the local vertices in \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            j\\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{j}^{1}}\\n  . These vertices in \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            j\\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{j}^{1}}\\n   are removed, together with their corresponding outgoing edges. For each outgoing edge \\n  \\n    \\n      \\n        (\\n        u\\n        ,\\n        v\\n        )\\n      \\n    \\n    {\\\\displaystyle (u,v)}\\n   with endpoint v in another PE \\n  \\n    \\n      \\n        l\\n        ,\\n        j\\n        \\u2260\\n        l\\n      \\n    \\n    {\\\\displaystyle l,j\\\\neq l}\\n  , the message \\n  \\n    \\n      \\n        (\\n        u\\n        ,\\n        v\\n        )\\n      \\n    \\n    {\\\\displaystyle (u,v)}\\n   is posted to PE l. After all vertices in \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            j\\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{j}^{1}}\\n   are removed, the posted messages are sent to their corresponding PE. Each message \\n  \\n    \\n      \\n        (\\n        u\\n        ,\\n        v\\n        )\\n      \\n    \\n    {\\\\displaystyle (u,v)}\\n   received updates the indegree of the local vertex v. If the indegree drops to zero, v is added to \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            j\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{j}^{2}}\\n  . Then the next iteration starts.\\nIn step k, PE j assigns the indices \\n  \\n    \\n      \\n        \\n          a\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        +\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            j\\n            \\u2212\\n            1\\n          \\n        \\n        \\n          |\\n        \\n        \\n          Q\\n          \\n            i\\n          \\n          \\n            k\\n          \\n        \\n        \\n          |\\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        +\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                0\\n              \\n              \\n                j\\n              \\n            \\n            \\n              |\\n            \\n            \\n              Q\\n              \\n                i\\n              \\n              \\n                k\\n              \\n            \\n            \\n              |\\n            \\n          \\n          )\\n        \\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\textstyle a_{k-1}+\\\\sum _{i=0}^{j-1}|Q_{i}^{k}|,\\\\dots ,a_{k-1}+\\\\left(\\\\sum _{i=0}^{j}|Q_{i}^{k}|\\\\right)-1}\\n  , where  \\n  \\n    \\n      \\n        \\n          a\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{k-1}}\\n  is the total amount of processed vertices after step \\n  \\n    \\n      \\n        k\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle k-1}\\n  . This procedure repeats until there are no vertices left to process, hence \\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            p\\n            \\u2212\\n            1\\n          \\n        \\n        \\n          |\\n        \\n        \\n          Q\\n          \\n            i\\n          \\n          \\n            D\\n            +\\n            1\\n          \\n        \\n        \\n          |\\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\textstyle \\\\sum _{i=0}^{p-1}|Q_{i}^{D+1}|=0}\\n  . Below is a high level, single program, multiple data pseudo code overview of this algorithm.\\nNote that the prefix sum for the local offsets \\n  \\n    \\n      \\n        \\n          a\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        +\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            j\\n            \\u2212\\n            1\\n          \\n        \\n        \\n          |\\n        \\n        \\n          Q\\n          \\n            i\\n          \\n          \\n            k\\n          \\n        \\n        \\n          |\\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          a\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        +\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                0\\n              \\n              \\n                j\\n              \\n            \\n            \\n              |\\n            \\n            \\n              Q\\n              \\n                i\\n              \\n              \\n                k\\n              \\n            \\n            \\n              |\\n            \\n          \\n          )\\n        \\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\textstyle a_{k-1}+\\\\sum _{i=0}^{j-1}|Q_{i}^{k}|,\\\\dots ,a_{k-1}+\\\\left(\\\\sum _{i=0}^{j}|Q_{i}^{k}|\\\\right)-1}\\n   can be efficiently calculated in parallel.\\n\\np processing elements with IDs from 0 to p-1\\nInput: G = (V, E) DAG, distributed to PEs, PE index j = 0, ..., p - 1\\nOutput: topological sorting of G\\n\\nfunction traverseDAGDistributed\\n    \\u03b4 incoming degree of local vertices V\\n    Q = {v \\u2208 V | \\u03b4[v] = 0}                     // All vertices with indegree 0\\n    nrOfVerticesProcessed = 0\\n\\n    do                 \\n        global build prefix sum over size of Q     // get offsets and total amount of vertices in this step\\n        offset = nrOfVerticesProcessed + sum(Qi, i = 0 to j - 1)          // j is the processor index\\n        foreach u in Q                                       \\n            localOrder[u] = index++;\\n            foreach (u,v) in E do post message (u, v) to PE owning vertex v\\n        nrOfVerticesProcessed += sum(|Qi|, i = 0 to p - 1)\\n        deliver all messages to neighbors of vertices in Q  \\n        receive messages for local vertices V\\n        remove all vertices in Q\\n        foreach message (u, v) received:\\n            if --\\u03b4[v] = 0\\n                add v to Q\\n    while global size of Q > 0\\n\\n    return localOrder\\n\\nThe communication cost depends heavily on the given graph partition. As for runtime, on a CRCW-PRAM model that allows fetch-and-decrement in constant time, this algorithm runs in \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                \\n                  m\\n                  +\\n                  n\\n                \\n                p\\n              \\n            \\n            +\\n            D\\n            (\\n            \\u0394\\n            +\\n            log\\n            \\u2061\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\textstyle {\\\\mathcal {O}}\\\\left({\\\\frac {m+n}{p}}+D(\\\\Delta +\\\\log n)\\\\right)}\\n  , where D is again the longest path in G and \\u0394 the maximum degree.\\n\\nApplication to shortest path finding\\nThe topological ordering can also be used to quickly compute shortest paths through a weighted directed acyclic graph. Let V be the list of vertices in such a graph, in topological order. Then the following algorithm computes the shortest path from some source vertex s to all other vertices:\\n\\nEquivalently:\\n\\nOn a graph of n vertices and m edges, this algorithm takes \\u0398(n + m), i.e., linear, time.\\n\\nUniqueness\\nIf a topological sort has the property that all pairs of consecutive vertices in the sorted order are connected by edges, then these edges form a directed Hamiltonian path in the DAG. If a Hamiltonian path exists, the topological sort order is unique; no other order respects the edges of the path. Conversely, if a topological sort does not form a Hamiltonian path, the DAG will have two or more valid topological orderings, for in this case it is always possible to form a second valid ordering by swapping two consecutive vertices that are not connected by an edge to each other. Therefore, it is possible to test in linear time whether a unique ordering exists, and whether a Hamiltonian path exists, despite the NP-hardness of the Hamiltonian path problem for more general directed graphs (i.e. cyclic directed graphs).\\n\\nRelation to partial orders\\nTopological orderings are also closely related to the concept of a linear extension of a partial order in mathematics. A partially ordered set is just a set of objects together with a definition of the \\\"\\u2264\\\" inequality relation, satisfying the axioms of reflexivity (x \\u2264 x), antisymmetry (if x \\u2264 y and y \\u2264 x then x = y) and transitivity (if x \\u2264 y and y \\u2264 z, then x \\u2264 z). A total order is a partial order in which, for every two objects x and y in the set, either x \\u2264 y or y \\u2264 x. Total orders are familiar in computer science as the comparison operators needed to perform comparison sorting algorithms. For finite sets, total orders may be identified with linear sequences of objects, where the \\\"\\u2264\\\" relation is true whenever the first object precedes the second object in the order; a comparison sorting algorithm may be used to convert a total order into a sequence in this way. A linear extension of a partial order is a total order that is compatible with it, in the sense that, if x \\u2264 y in the partial order, then  x \\u2264 y in the total order as well.\\nOne can define a partial ordering from any DAG by letting the set of objects be the vertices of the DAG, and defining x \\u2264 y to be true, for any two vertices x and y, whenever there exists a directed path from x to y; that is,  whenever y is reachable from x. With these definitions, a topological ordering of the DAG is the same thing as a linear extension of this partial order. Conversely, any partial ordering may be defined as the reachability relation in a DAG. One way of doing this is to define a DAG that has a vertex for every object in the partially ordered set, and an edge xy for every pair of objects for which x \\u2264 y. An alternative way of doing this is to use the transitive reduction of the partial ordering; in general, this produces DAGs with fewer edges, but the reachability relation in these DAGs is still the same partial order. By using these constructions, one can use topological ordering algorithms to find linear extensions of partial orders.\\n\\nRelation to scheduling optimisation\\nBy definition, the solution of a scheduling problem that includes a precedence graph is a valid solution to topological sort (irrespective of the number of machines), however, topological sort in itself is not enough to optimally solve a scheduling optimisation problem. Hu's algorithm is a popular method used to solve scheduling problems that require a precedence graph and involve processing times (where the goal is to minimise the largest completion time amongst all the jobs). Like topological sort, Hu's algorithm is not unique and can be solved using DFS (by finding the largest path length and then assigning the jobs).\\n\\nSee also\\ntsort, a Unix program for topological sorting\\nFeedback arc set, a set of edges whose removal allows the remaining subgraph to be topologically sorted\\nTarjan's strongly connected components algorithm, an algorithm that gives the topologically sorted list of strongly connected components in a graph\\nPre-topological order\\n\\nReferences\\nFurther reading\\nD. E. Knuth, The Art of Computer Programming, Volume 1, section 2.2.3, which gives an algorithm for topological sorting of a partial ordering, and a brief history.\\n\\nExternal links\\nNIST Dictionary of Algorithms and Data Structures: topological sort\\nWeisstein, Eric W., \\\"Topological Sort\\\", MathWorld\"}, {\"Tournament sort\": \"Tournament sort is a sorting algorithm.  It improves upon the naive selection sort by using a priority queue to find the next element in the sort.  In the naive selection sort, it takes O(n) operations to select the next element of n elements; in a tournament sort, it takes O(log n) operations (after building the initial tournament in O(n)).  Tournament sort is a variation of heapsort.\\n\\nCommon application\\nTournament replacement selection sorts are used to gather the initial runs for external sorting algorithms. Conceptually, an external file is read and its elements are pushed into the priority queue until the queue is full. Then the minimum element is pulled from the queue and written as part of the first run. The next input element is read and pushed into the queue, and the min is selected again and added to the run. There's a small trick that if the new element being pushed into the queue is less than the last element added to the run, then the element's sort value is increased so it will be part of the next run. On average, a run will be 100% longer than the capacity of the priority queue.Tournament sorts may also be used in N-way merges.\\n\\nEtymology\\nThe name comes from its similarity to a single-elimination tournament where there are many players (or teams) that play in two-sided matches. Each match compares the players, and the winning player is promoted to play a match at the next level up. The hierarchy continues until the final match determines the ultimate winner. The tournament determines the best player, but the player who was beaten in the final match may not be the second best \\u2013 he may be inferior to other players the winner bested.\\n\\nImplementation\\nThe following is an implementation of tournament sort in Haskell, based on Scheme code by Stepanov and Kershenbaum.\\n\\nReferences\\n\\nKershenbaum et al 1988, \\\"Higher Order Imperative Programming\\\"\"}, {\"Tree sort\": \"A tree sort is a sort algorithm that builds a binary search tree from the elements to be sorted, and then traverses the tree (in-order) so that the elements come out in sorted order. Its typical use is sorting elements online: after each insertion, the set of elements seen so far is available in sorted order.\\nTree sort can be used as a one-time sort, but it is equivalent to quicksort as both recursively partition the elements based on a pivot, and since quicksort is in-place and has lower overhead, tree sort has few advantages over quicksort. It has better worst case complexity when a self-balancing tree is used, but even more overhead.\\n\\nEfficiency\\nAdding one item to a binary search tree is on average an O(log n) process (in big O notation). Adding n items is an O(n log n) process, making tree sorting a 'fast sort' process. Adding an item to an unbalanced binary tree requires O(n) time in the worst-case: When the tree resembles a linked list (degenerate tree). This results in a worst case of O(n\\u00b2) time for this sorting algorithm.\\nThis worst case occurs when the algorithm operates on an already sorted set, or one that is nearly sorted, reversed or nearly reversed. Expected O(n log n) time can however be achieved by shuffling the array, but this does not help for equal items.\\nThe worst-case behaviour can be improved by using a self-balancing binary search tree. Using such a tree, the algorithm has an O(n log n) worst-case performance, thus being degree-optimal for a comparison sort. However, tree sort algorithms require separate memory to be allocated for the tree, as opposed to in-place algorithms such as quicksort or heapsort. On most common platforms, this means that heap memory has to be used, which is a significant performance hit when compared to quicksort and heapsort. When using a splay tree as the binary search tree, the resulting algorithm (called splaysort) has the additional property that it is an adaptive sort, meaning that its running time is faster than O(n log n) for inputs that are nearly sorted.\\n\\nExample\\nThe following tree sort algorithm in pseudocode accepts a collection of comparable items and outputs the items in ascending order:\\n\\nIn a simple functional programming form, the algorithm (in Haskell) would look something like this:\\n\\nIn the above implementation, both the insertion algorithm and the retrieval algorithm have O(n\\u00b2) worst-case scenarios.\\n\\nExternal links\\n\\nBinary Tree Java Applet and Explanation at the Wayback Machine (archived 29 November 2016)\\nTree Sort of a Linked List\\nTree Sort in C++\\n\\n\\n== References ==\"}, {\"Weak heap\": \"In computer science, a weak heap is a data structure for priority queues, combining features of the binary heap and binomial heap.  It can be stored in an array as an implicit binary tree like a binary heap, and has the efficiency guarantees of binomial heaps.\\nA sorting algorithm using weak heaps, weak-heapsort, uses a number of comparisons that is close to the theoretical lower bound on the number of comparisons required to sort a list, so is particularly useful when comparison is expensive, such as when comparing strings using the full Unicode collation algorithm.\\n\\nDescription\\nA weak heap is most easily understood as a heap-ordered multi-way tree stored as a binary tree using the \\\"right-child left-sibling\\\" convention.  (This is equivalent to, but reversed from, the usual left-child right-sibling binary tree.)\\nIn the multi-way tree, and assuming a max-heap, each parent's key is greater than or equal to (\\u2265) all the child keys (and thus, by induction, all members of the subtree).\\nExpressed as a binary tree, this translates to the following invariants:\\nThe root node has no left child\\nFor every node, the value associated with that node is greater than or equal to the values associated with all nodes in its right subtree.\\nThe leaves of the tree have heights that are all within one of each other.The last condition is a consequence of the fact that an implicit binary tree is a complete binary tree.\\nThe structure of this tree maps very neatly onto the traditional 1-based (Ahnentafel) implicit binary tree arrangement, where node k has a next sibling (left child) numbered 2k and a first child (right child) numbered 2k + 1, by adding an additional root numbered 0.  This root has no siblings, only a first child, which is node 1 (2\\u00d70 + 1).\\nThis structure is very similar to that of a binomial heap, with a tree of height h being composed of a root plus trees of heights h \\u2212 1, h \\u2212 2, ..., 1.  A perfect (no missing leaves) weak heap with 2n elements is exactly isomorphic to a binomial heap of the same size, but the two algorithms handle sizes which are not a power of 2 differently: a binomial heap uses multiple perfect trees, while a weak heap uses a single imperfect tree.\\nWeak heaps require the ability to exchange the left and right children (and associated subtrees) of a node.  In an explicit (pointer-based) representation of the tree, this is straightforward.  In an implicit (array) representation, this requires one \\\"reverse bit\\\" per internal node to indicate which child is considered the left child.  A weak heap is thus not a strictly implicit data structure since it requires O(n) additional space (1/2 bit per node).  However, it is often possible to find space for this extra bit within the node structure, such as by tagging a pointer which is already present.\\nIn the implicit binary tree, node k with reverse bit rk has parent \\u230ak/2\\u230b, left child 2k + rk, and right child 2k + 1 \\u2212 rk.\\nViewed as a multi-way tree, each node in a weak heap is linked to two others: a \\\"next sibling\\\" and a \\\"first child\\\".  In the implicit tree, the links are fixed, so which of the two links is the sibling and which the first child is indicated by the reverse bit.\\n\\nOperations on weak heaps\\nNote that every node in a weak heap can be considered the root of a smaller weak heap by ignoring its next sibling.  Nodes with no first child are automatically valid weak heaps.\\nA node of height h has h \\u2212 1 children: a first child of height h \\u2212 1, a second child of height h \\u2212 2, and so on to the last child of height 1.  These may be found by following the first child link and then successive next sibling links.\\nIt also has next siblings of height h \\u2212 1, h \\u2212 2, etc.\\nA node's parent in the multi-way tree is called its \\\"distinguished ancestor\\\".  To find this in the binary tree, find the node's binary parent.  If the node is the right child (first child), the parent is the distinguished ancestor.  If the node is the left child (next sibling), its distinguished ancestor is the same as its binary parent's.  In the implicit tree, finding the binary parent is easy, but its reverse bit must be consulted to determine which type of child the node is.  (Early papers used the term \\\"grandparent\\\" for the distinguished ancestor, a meaning confusingly different from the usual \\\"parent of parent\\\".)\\nAlthough the distinguished ancestor may be log2n levels high in the tree, the average distance is 2.  (It's at least 1, and half of the time we recurse, so D = 1 + D/2, meaning that D = 2.)  Thus, even a simple iterative algorithm for finding the distinguished ancestor is sufficient.\\nLike binomial heaps, the fundamental operation on weak heaps is merging two heaps of equal height h, to make a weak heap of height h+1.  This requires exactly one comparison, between the roots.  Whichever root is greater (assuming a max-heap) is the final root.  Its first child is the losing root, which retains its children (right subtree).  The winning root's children are installed as siblings of the losing root.\\nThis operation can be performed on the implicit tree structure because the heaps being merged are never arbitrary.  Rather, the two heaps are formed as part of sifting a node up the multi-way tree:\\n\\nThe first is a normal weak heap (whose next sibling link exists, but is ignored).\\nThe second is the imaginary heap formed by linking the first root's distinguished ancestor (multi-way parent) to the first root's following siblings.At the beginning, the heap invariants apply everywhere except possibly between the first root and its distinguished ancestor.  All other nodes are less than or equal to their distinguished ancestors.\\nAfter comparing the two roots, the merge proceeds in one of two ways:\\n\\n(The distinguished ancestor is greater or equal.)  Nothing needs to be moved, and the result of the merge is the distinguished ancestor.\\n(The first root is greater.)  The first root's binary children (first child and next sibling) are exchanged (using the reverse bit), and then the first root and its distinguished ancestor are exchanged (by copying).The second case works because, in the multi-way tree, each node keeps its children with it.  The first root is promoted up the tree because it is greater than its distinguished ancestor.  Thus, it is safely greater than all of the ancestor's previous children.\\nThe previous ancestor, however, is not a safe parent for the first root's old children, because it is less than the first root and so it's not guaranteed to be greater than or equal to all of its children.\\nBy swapping the binary children, the appropriate subset of the demoted ancestor's old children (which are safely less than or equal to it) are demoted with it.  The demoted ancestor's new siblings are the first root's old children, promoted, which are safely less than or equal to the promoted first root.\\nAfter this operation, it is uncertain whether the invariant is maintained between the new distinguished ancestor and its distinguished ancestor, so the operation is repeated until the root is reached.\\n\\nWeak-heap sort\\nWeak heaps may be used to sort an array, in essentially the same way as a conventional heapsort.  First, a weak heap is built out of all of the elements of the array, and then the root is repeatedly exchanged with the last element, which is sifted down to its proper place.\\nA weak heap of n elements can be formed in n \\u2212 1 merges.  It can be done on various orders, but a simple bottom-up implementation works from the end of the array to the beginning, merging each node with its distinguished ancestor.  Note that finding the distinguished ancestor is simplified because the reverse bits in all parents of the heaps being merged are unmodified from their initial state (\\\"not reversed\\\"), and so do not need to be consulted.\\nAs with heapsort, if the array to be sorted is larger than the CPU cache, performance is improved if subtrees are merged as soon as two of the same size become available, rather than merging all subtrees on one level before proceeding to the next.Sifting down in a weak heap can be done in h = \\u2308log2n\\u2309 comparisons, as opposed to 2\\u2009log2n for a binary heap, or 1.5\\u2009log2n for the \\\"bottom-up heapsort\\\" variant.  This is done by \\\"merging up\\\": after swapping the root with the last element of the heap, find the last (height 1) child of the root.  Merge this with the root (its distinguished ancestor), resulting in a valid height-2 heap at the global root.  Then go to the previous sibling (binary parent) of the last merged node, and merge again.  Repeat until the root is reached, when it will be correct for the complete tree.\\n\\nPriority queue operations\\nIn a weak max-heap, the maximum value can be found (in constant time) as the value associated with the root node; similarly, in a weak min-heap, the minimum value can be found at the root.\\nAs with binary heaps, weak heaps can support the typical operations of a priority queue data structure: insert, delete-min, delete, or decrease-key, in logarithmic time per operation.\\nSifting up is done using the same process as in binary heaps.  The new node is added at the leaf level, then compared with its distinguished ancestor and swapped if necessary (the merge operation).  This is repeated until no more swaps are necessary or the root is reached.\\nVariants of the weak heap structure allow constant amortized time insertions and decrease-keys, matching the time for Fibonacci heaps.\\n\\nHistory and applications\\nWeak heaps were introduced by Dutton (1993), as part of a variant heap sort algorithm that (unlike the standard heap sort using binary heaps) could be used to sort n items using only n log2n + O(n) comparisons. They were later investigated as a more generally applicable priority queue data structure.\\n\\nReferences\\nFurther reading\\nEdelkamp, Stefan; Elmasry, Amr; Katajainen, Jyrki (November 2013). \\\"Weak heaps engineered\\\" (PDF). Journal of Discrete Algorithms. 23: 83\\u201397. doi:10.1016/j.jda.2013.07.002. We provide a catalogue of algorithms that optimize the standard algorithms in various ways. As the optimization criteria, we consider the worst-case running time, the number of instructions, branch mispredictions, cache misses, element comparisons, and element moves.\\nEdelkamp, Stefan; Elmasry, Amr; Katajainen, Jyrki; Wei\\u00df, Armin (July 2013). Weak Heaps and Friends: Recent Developments (PDF). Combinatorial Algorithms - 24th International Workshop. Rouen, France. doi:10.1007/978-3-642-45278-9_1.\"}, {\"X + Y sorting\": \"In computer science, \\n  \\n    \\n      \\n        \\n          X\\n        \\n        +\\n        \\n          Y\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\boldsymbol {X}}+{\\\\boldsymbol {Y}}}\\n   sorting is the problem of sorting pairs of numbers by their sums. Applications of the problem include transit fare minimisation, VLSI design, and sparse polynomial multiplication. As with comparison sorting and integer sorting more generally, algorithms for this problem can be based only on comparisons of these sums, or on other operations that work only when the inputs are small integers.\\nIt is unknown whether this problem has a comparison-based solution whose running time is asymptotically faster than sorting an unstructured list of equally many items. Therefore, research on the problem has focused on two approaches to settle the question of whether such an improvement is possible: the development of algorithms that improve on unstructured sorting in their number of comparisons rather than in their total running time, and lower bounds for the number of comparisons based on counting cells in subdivisions of high-dimensional spaces. Both approaches are historically tied together, in that the first algorithms that used few comparisons were based on the weakness of the cell-counting lower bounds.\\n\\nProblem statement and history\\nThe input to the \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting problem consists of two finite collections of numbers \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   and \\n  \\n    \\n      \\n        Y\\n      \\n    \\n    {\\\\displaystyle Y}\\n  , of the same length. The problem's output is the collection of all pairs of a number from \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   and a number from \\n  \\n    \\n      \\n        Y\\n      \\n    \\n    {\\\\displaystyle Y}\\n  , arranged into sorted order by the sum of each pair. As a small example, for the inputs \\n  \\n    \\n      \\n        X\\n        =\\n        {\\n        1\\n        ,\\n        2\\n        ,\\n        9\\n        }\\n      \\n    \\n    {\\\\displaystyle X=\\\\{1,2,9\\\\}}\\n   and \\n  \\n    \\n      \\n        Y\\n        =\\n        {\\n        0\\n        ,\\n        4\\n        ,\\n        9\\n        }\\n      \\n    \\n    {\\\\displaystyle Y=\\\\{0,4,9\\\\}}\\n  , the output should be the list of pairsof one element from \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   and one element from \\n  \\n    \\n      \\n        Y\\n      \\n    \\n    {\\\\displaystyle Y}\\n  , listed in sorted order by their sums of pairsOne way to solve the problem would be to construct the pairs to be sorted (the Cartesian product of the two collections) and use these pairs as input to a standard comparison sorting algorithm such as merge sort or heapsort. When the inputs have length \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  , they form \\n  \\n    \\n      \\n        \\n          n\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n^{2}}\\n   pairs, and the time to sort the pairs in this way is \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2}\\\\log n)}\\n  . In terms of its big O notation, this method is the fastest known algorithm for \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting.  Whether a faster algorithm exists is an open problem, posed by Elwyn Berlekamp prior to 1975.\\nA variant of the problem sorts the sumset, the set of sums of pairs, with duplicate sums condensed to a single value. For this variant, the size of the sumset may be significantly smaller than \\n  \\n    \\n      \\n        \\n          n\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n^{2}}\\n  , and output-sensitive algorithms for constructing it have been investigated.\\n\\nApplications\\nSteven Skiena recounts a practical application in transit fare minimisation, an instance of the shortest path problem: find the cheapest two-hop airplane ticket between two given cities, from an input that describes both the cost of each hop and which pairs of hops may be combined into a single ticket. Skiena's solution consists of sorting pairs of hops by their total cost as an instance of the \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting problem, and then testing the resulting pairs in this sorted order until finding one that is allowed. To generate the sorted pairs in this order, Skiena uses a priority queue of pairs, initially containing only a single pair, the one consisting of the two cheapest hops. Then, when a pair \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n   is removed from the queue and found to be disallowed, two more pairs are added, with one of these two pairs combining \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   with the next hop after \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   in a sorted list of the hops to the destination, and the other pair combining \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   with the next hop after \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   in a sorted list of hops from the start. In this way, each successive pair can be found in logarithmic time, and only the pairs up to the first allowable one need to be sorted.\\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting is the most expensive subroutine in an algorithm for a problem in VLSI design, in which one must place two subunits of a VLSI circuit side-by-side along a communications channel in order to minimize the width of the channel needed to route pairs of wires from one subunit to the other. As one subunit is continuously shifted relative to the other, the channel width only changes at discrete positions where the ends of two wires line up with each other, and finding the sorted ordering of these positions in order to compute the sequence of changes to the width can be performed by \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting. If this sorting problem could be sped up, it would also speed up this VLSI design task.Another application involves polynomial multiplication for polynomials of a single variable that may have many fewer terms than their degrees. The product of two polynomials can be expressed as a sum of products of pairs of terms, one from each polynomial, and placing these term-by-term products into degree order amounts to sorting them by the sum of degrees. For example, the instance of \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting with \\n  \\n    \\n      \\n        n\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle n=3}\\n   given as an example above corresponds to the  multiplication of two three-term polynomials to produce a nine-term polynomial:\\n\\nThe degrees are always integers, so integer-based algorithms for \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting may be applied. However, for polynomials whose number of terms is comparable to their degree, FFT-based polynomial multiplication algorithms may be significantly more efficient than term-by-term multiplication.\\n\\nNumber of orderings\\nA well-known lower bound for unstructured sorting, in the decision tree model, is based on the factorial number of sorted orders that an unstructured list may have. Because each comparison can at best reduce the number of possible orderings by a factor of two, sorting requires a number of comparisons at least equal to the binary logarithm of the factorial, which is \\n  \\n    \\n      \\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle n\\\\log _{2}n-O(n)}\\n  . Early work on \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting followed a similar approach by asking how many different sorted orderings are possible for this problem, and proving that this number is at most \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            8\\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{8n})}\\n  . However, because its binary logarithm is at most \\n  \\n    \\n      \\n        8\\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        +\\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle 8n\\\\log _{2}n+O(1)}\\n  , much smaller than the known time bounds for \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting, this method can only lead to weak lower bounds on the number of comparisons.The proof of this bound relates \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting to the complexity of an arrangement of hyperplanes in high-dimensional geometry. The two input collections for the \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting problem comprise \\n  \\n    \\n      \\n        2\\n        n\\n      \\n    \\n    {\\\\displaystyle 2n}\\n   numbers, which can alternatively be interpreted as the Cartesian coordinates of a point in the \\n  \\n    \\n      \\n        2\\n        n\\n      \\n    \\n    {\\\\displaystyle 2n}\\n  -dimensional space \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            2\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{2n}}\\n  . This space can be subdivided into cells, so that within a single cell all points correspond to inputs that produce the same sorted order. For this subdivision, each boundary between two cells lies within a hyperplane defined by an equality of pairs \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            j\\n          \\n        \\n        =\\n        \\n          x\\n          \\n            k\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            \\u2113\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}+y_{j}=x_{k}+y_{\\\\ell }}\\n  , where \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            j\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{i},y_{j})}\\n   and \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            k\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            \\u2113\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{k},y_{\\\\ell })}\\n   are two pairs whose ordering changes from one adjacent cell to the other. These hyperplanes are either generated by two disjoint pairs, or they have the simplified forms \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          x\\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}=x_{k}}\\n   or \\n  \\n    \\n      \\n        \\n          y\\n          \\n            j\\n          \\n        \\n        =\\n        \\n          y\\n          \\n            \\u2113\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{j}=y_{\\\\ell }}\\n  , so the number of distinct hyperplanes that can be determined in this way is\\n\\nThe number of cells that this number of hyperplanes can divide a space of dimension \\n  \\n    \\n      \\n        2\\n        n\\n      \\n    \\n    {\\\\displaystyle 2n}\\n   into is\\n Therefore, the set \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   has \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            8\\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{8n})}\\n   different possible sorted orderings.A similar style of analysis has been more successful in ruling out fast solutions to certain generalizations of \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting, by showing that they have too many orderings to sort quickly. In particular, Harper et al. (1975) suggest separately sorting \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   and \\n  \\n    \\n      \\n        Y\\n      \\n    \\n    {\\\\displaystyle Y}\\n  , and then constructing a two-dimensional matrix of the values of \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   that is sorted both by rows and by columns before using this partially-sorted data to complete the sort of \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n  . This idea of using a row-sorted and column-sorted matrix forms the basis for the method used by Skiena in the transportation application, and it can reduce the number of comparisons by a constant factor relative to naive comparison sorting. However, for matrices whose rows and columns are sorted in this way, the number of possible sorted orderings of the whole matrix is much larger than \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            8\\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{8n})}\\n  , so large that any comparison sorting algorithm that can work for arbitrary \\n  \\n    \\n      \\n        n\\n        \\u00d7\\n        n\\n      \\n    \\n    {\\\\displaystyle n\\\\times n}\\n   matrices that are sorted by rows and columns still requires \\n  \\n    \\n      \\n        \\u03a9\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Omega (n^{2}\\\\log n)}\\n   comparisons. Therefore, if the \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting problem is to be solved quickly, the solution must use\\nadditional information about the set \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   beyond this matrix ordering.\\n\\nNumber of comparisons\\nFor the classical comparison sorting problem, the time to sort and the number of comparisons needed to sort are within constant factors of each other.\\nBut for \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting, the number of comparisons is smaller than the best time bound known: Michael Fredman showed in 1976 that \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting can be done using only \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   comparisons. More generally, he showed that any set of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   elements, whose sorted ordering has already been restricted to a family \\n  \\n    \\n      \\n        \\u0393\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n   of orderings, can be sorted using \\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        \\n          |\\n        \\n        \\u0393\\n        \\n          |\\n        \\n        +\\n        O\\n        (\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}|\\\\Gamma |+O(N)}\\n   comparisons, by a form of binary insertion sort. For the \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting problem, \\n  \\n    \\n      \\n        N\\n        =\\n        \\n          n\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N=n^{2}}\\n  , and \\n  \\n    \\n      \\n        \\n          |\\n        \\n        \\u0393\\n        \\n          |\\n        \\n        =\\n        O\\n        (\\n        \\n          n\\n          \\n            8\\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle |\\\\Gamma |=O(n^{8n})}\\n  , so \\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        \\n          |\\n        \\n        \\u0393\\n        \\n          |\\n        \\n        =\\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}|\\\\Gamma |=O(n\\\\log n)}\\n   and Fredman's bound implies that only \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   comparisons are needed. However, in Fredman's method, the time needed to decide which comparisons to perform may be significantly higher than the bound on the number of comparisons.The first explicit algorithm that achieves both \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   comparisons and \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2}\\\\log n)}\\n   total complexity was published sixteen years after Fredman by Lambert (1992). The algorithm performs the following steps:\\n\\nRecursively sort the two sets \\n  \\n    \\n      \\n        X\\n        +\\n        X\\n      \\n    \\n    {\\\\displaystyle X+X}\\n   and \\n  \\n    \\n      \\n        Y\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle Y+Y}\\n  .\\nUse the equivalence \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        \\u2264\\n        \\n          x\\n          \\n            k\\n          \\n        \\n        \\u2212\\n        \\n          x\\n          \\n            \\u2113\\n          \\n        \\n        \\u21d4\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        +\\n        \\n          x\\n          \\n            \\u2113\\n          \\n        \\n        \\u2264\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        +\\n        \\n          x\\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}-x_{j}\\\\leq x_{k}-x_{\\\\ell }\\\\Leftrightarrow x_{i}+x_{\\\\ell }\\\\leq x_{j}+x_{k}}\\n   to infer the sorted orderings of \\n  \\n    \\n      \\n        X\\n        \\u2212\\n        X\\n      \\n    \\n    {\\\\displaystyle X-X}\\n   and \\n  \\n    \\n      \\n        Y\\n        \\u2212\\n        Y\\n      \\n    \\n    {\\\\displaystyle Y-Y}\\n   without additional comparisons.\\nMerge the two sets \\n  \\n    \\n      \\n        X\\n        \\u2212\\n        X\\n      \\n    \\n    {\\\\displaystyle X-X}\\n   and \\n  \\n    \\n      \\n        Y\\n        \\u2212\\n        Y\\n      \\n    \\n    {\\\\displaystyle Y-Y}\\n   into a single sorted order, using a number of comparisons linear in their total size.\\nUse the merged order and the equivalence \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            j\\n          \\n        \\n        \\u2264\\n        \\n          x\\n          \\n            k\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            \\u2113\\n          \\n        \\n        \\u21d4\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        \\u2212\\n        \\n          x\\n          \\n            k\\n          \\n        \\n        \\u2264\\n        \\n          y\\n          \\n            \\u2113\\n          \\n        \\n        \\u2212\\n        \\n          y\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}+y_{j}\\\\leq x_{k}+y_{\\\\ell }\\\\Leftrightarrow x_{i}-x_{k}\\\\leq y_{\\\\ell }-y_{j}}\\n   to infer the sorted order of \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   without additional comparisons.The part of the algorithm that recursively sorts \\n  \\n    \\n      \\n        X\\n        +\\n        X\\n      \\n    \\n    {\\\\displaystyle X+X}\\n   (or equivalently \\n  \\n    \\n      \\n        Y\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle Y+Y}\\n  ) does so by the following steps:\\n\\nSplit \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   into two equal sublists \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   and \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n  .\\nRecursively sort \\n  \\n    \\n      \\n        A\\n        +\\n        A\\n      \\n    \\n    {\\\\displaystyle A+A}\\n   and \\n  \\n    \\n      \\n        B\\n        +\\n        B\\n      \\n    \\n    {\\\\displaystyle B+B}\\n  \\nInfer the ordering on \\n  \\n    \\n      \\n        A\\n        +\\n        B\\n      \\n    \\n    {\\\\displaystyle A+B}\\n   using only the comparisons from a single merge step as above.\\nMerge the sorted results \\n  \\n    \\n      \\n        A\\n        +\\n        A\\n      \\n    \\n    {\\\\displaystyle A+A}\\n  , \\n  \\n    \\n      \\n        B\\n        +\\n        B\\n      \\n    \\n    {\\\\displaystyle B+B}\\n  , and \\n  \\n    \\n      \\n        A\\n        +\\n        B\\n      \\n    \\n    {\\\\displaystyle A+B}\\n   together.The number of comparisons \\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle C(n)}\\n   needed to perform this recursive algorithm on an input of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   items can be analyzed using the recurrence relation\\n\\nwhere the \\n  \\n    \\n      \\n        2\\n        C\\n        (\\n        n\\n        \\n          /\\n        \\n        2\\n        )\\n      \\n    \\n    {\\\\displaystyle 2C(n/2)}\\n   term of the recurrence counts the number of comparisons in the recursive calls to the algorithm to sort \\n  \\n    \\n      \\n        A\\n        +\\n        A\\n      \\n    \\n    {\\\\displaystyle A+A}\\n   and \\n  \\n    \\n      \\n        B\\n        +\\n        B\\n      \\n    \\n    {\\\\displaystyle B+B}\\n  , and the \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   term counts the number of comparisons used to merge the results. The master theorem for recurrence relations of this form shows that \\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n        =\\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle C(n)=O(n^{2}).}\\n   The total time complexity is slower, \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2}\\\\log n)}\\n  , because of the steps of the algorithm that use already-made comparisons to infer orderings of other sets. These steps can be performed in time \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2}\\\\log n)}\\n   by using a standard comparison-sorting algorithm with its comparison steps replaced by the stated inferences.If only comparisons between elements of \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   are allowed, then there is also a matching lower bound of \\n  \\n    \\n      \\n        \\u03a9\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Omega (n^{2})}\\n   on the number of comparisons, but with more general comparisons involving linear combinations of constant numbers of elements, only \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log ^{2}n)}\\n   comparisons are needed.\\n\\nNon-comparison-based algorithms\\nJust as integer sorting can be faster than comparison sorting for small-enough integers, the same is true for \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting. In particular,\\nwith integer inputs in the range from \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n   to some upper limit \\n  \\n    \\n      \\n        M\\n      \\n    \\n    {\\\\displaystyle M}\\n  , the problem can be solved in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        +\\n        M\\n        log\\n        \\u2061\\n        M\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n+M\\\\log M)}\\n   operations by means of the fast Fourier transform.\\n\\nRelated problems\\nSeveral other problems in computational geometry have equivalent or harder complexity to \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting, including constructing Minkowski sums of staircase polygons, finding the crossing points of an arrangement of lines in sorted order by their \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  -coordinates, listing pairs of points in sorted order by their distances, and testing whether one rectilinear polygon can be translated to fit within another.The problem of testing whether two of the pairs in the \\n  \\n    \\n      \\n        X\\n        +\\n        Y\\n      \\n    \\n    {\\\\displaystyle X+Y}\\n   sorting problem have equal sums can be solved by sorting the pairs and then testing consecutive pairs for equality. In turn, it could be used to solve the 3SUM problem, implying that it is unlikely to have a strongly subquadratic algorithm.\\n\\n\\n== References ==\"}, {\"Category:Comparison sorts\": \"A comparison sort is a particular type of sorting algorithm which can only read the list elements through a single abstract comparison operation (often a \\\"less than\\\" operator) that determines which of two elements should occur first in the final sorted list.\"}, {\"Adaptive heap sort\": \"In computer science, adaptive heap sort is a comparison-based sorting algorithm of the adaptive sort family. It is a variant of heap sort that performs better when the data contains existing order. Published by Christos Levcopoulos and Ola Petersson in 1992, the algorithm utilizes a new measure of presortedness, Osc,  as the number of oscillations. Instead of putting all the data into the heap as the traditional heap sort did, adaptive heap sort only take part of the data into the heap so that the run time will reduce significantly when the presortedness of the data is high.\\n\\nHeapsort\\nHeap sort is a sorting algorithm that utilizes binary heap data structure. The method treats an array as a complete binary tree and builds up a Max-Heap/Min-Heap to achieve sorting. It usually involves the following four steps.\\n\\nBuild a Max-Heap(Min-Heap): put all the data into the heap so that all nodes are either greater than or equal  (less than or equal to for Min-Heap) to each of its child nodes.\\nSwap the first element of the heap with the last element of the heap.\\nRemove the last element from the heap and put it at the end of the list. Adjust the heap so that the first element ends up at the right place in the heap.\\nRepeat Step 2 and 3 until the heap has only one element. Put this last element at the end of the list and output the list. The data in the list will be sorted.Below is a C/C++ implementation that builds up a Max-Heap and sorts the array after the heap is built.\\n\\nMeasures of presortedness\\nMeasures of presortedness measures the existing order in a given sequence. These measures of presortedness decides the number of data that will be put in to the heap during the sorting process  as well as the lower bound of running time.\\n\\nOscillations (Osc)\\nFor sequence \\n  \\n    \\n      \\n        X\\n        =\\n        \\u27e8\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle X=\\\\langle x_{1},x_{2},x_{3},\\\\dots ,x_{n}\\\\rangle }\\n  , Cross(xi) is defined as the number edges of the line plot of X that are intersected by a horizontal line through the point (i, xi). Mathematically, it is defined as \\n  \\n    \\n      \\n        \\n          \\n            C\\n            r\\n            o\\n            s\\n            s\\n          \\n        \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        {\\n        j\\n        \\u2223\\n        min\\n        {\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            j\\n            +\\n            1\\n          \\n        \\n        }\\n        <\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        <\\n        max\\n        {\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            j\\n            +\\n            1\\n          \\n        \\n        }\\n        \\n           for \\n        \\n        1\\n        \\u2264\\n        j\\n        <\\n        n\\n        }\\n        \\n          , for \\n        \\n        1\\n        \\u2264\\n        i\\n        \\u2264\\n        n\\n      \\n    \\n    {\\\\displaystyle {\\\\mathit {Cross}}(x_{i})=\\\\{j\\\\mid \\\\min\\\\{x_{j},x_{j+1}\\\\}<x_{i}<\\\\max\\\\{x_{j},x_{j+1}\\\\}{\\\\text{ for }}1\\\\leq j<n\\\\}{\\\\text{, for }}1\\\\leq i\\\\leq n}\\n  . The oscillation(Osc) of X is just the total number of intersections, defined as \\n  \\n    \\n      \\n        \\n          \\n            O\\n            s\\n            c\\n          \\n        \\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            \\u2211\\n            \\n              i\\n              =\\n              1\\n            \\n            \\n              n\\n            \\n          \\n          \\n            \\u2016\\n            \\n              \\n                C\\n                r\\n                o\\n                s\\n                s\\n              \\n            \\n            (\\n            \\n              x\\n              \\n                i\\n              \\n            \\n            )\\n            \\u2016\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathit {Osc}}(x)=\\\\textstyle \\\\sum _{i=1}^{n}\\\\displaystyle \\\\lVert {\\\\mathit {Cross}}(x_{i})\\\\rVert }\\n  .\\n\\nOther measures\\nBesides the original Osc measurement, other known measures include the number of inversions Inv, the number of runs Runs, the number of blocks Block, and the measures Max, Exc and Rem. Most of these different measurements are related for adaptive heap sort. Some measures dominate the others: every Osc-optimal algorithm is Inv optimal and Runs optimal; every Inv-optimal algorithm is Max optimal; and every Block-optimal algorithm is Exc optimal and Rem optimal.\\n\\nAlgorithm\\nAdaptive heap sort is a variant of heap sort that seeks optimality (asymptotically optimal) with respect to the lower bound derived with the measure of presortedness by taking advantage of the existing order in the data. In heap sort, for a data \\n  \\n    \\n      \\n        X\\n        =\\n        \\u27e8\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        \\u27e9\\n      \\n    \\n    {\\\\displaystyle X=\\\\langle x_{1},x_{2},x_{3},\\\\dots ,x_{n}\\\\rangle }\\n   , we put all n elements into the heap and then keep extracting the maximum (or minimum) for n times. Since the time of each max-extraction action is the logarithmic in the size of the heap, the total running time of standard heap sort is \\n  \\n    \\n      \\n        \\n          O\\n          (\\n          n\\n          log\\n          \\u2061\\n          n\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\color {Blue}O(n\\\\log n)}\\n  . For adaptive heap sort, instead of putting all the elements into the heap, only the possible maximums of the data (max-candidates) will be put into the heap so that fewer runs are required when each time we try to locate the maximum (or minimum).\\nFirst, a Cartesian tree is built from the input in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time by putting the data into a binary tree and making each node in the tree is greater(or smaller) than all its children nodes, and the root of the Cartesian tree is inserted into an empty binary heap. Then repeatedly extract the maximum from the binary heap, retrieve the maximum in the Cartesian tree, and add its left and right children (if any) which are themselves Cartesian trees, to the binary heap. If the input is already nearly sorted, the Cartesian trees will be very unbalanced, with few nodes having left and right children, resulting in the binary heap remaining small, and allowing the algorithm to sort more quickly than \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   for inputs that are already nearly sorted.Below is an implementation in pseudo-code:\\nInput: an array of n elements that need to be sorted\\n\\nConstruct the Cartesian tree l(x)\\nInsert the root of l(x) into a heap\\n\\nfor i = from 1 to n\\n{\\n    Perform ExtractMax on the heap \\n    if the max element extracted has any children in l(x)\\n    {\\n        retrieve the children in l(x)\\n        insert the children element into the heap\\n    }\\n}\\n\\nDrawbacks\\nDespite decades of research, there's still a gap between the theory of adaptive heap sort and its practical use. Because the algorithm makes use of Cartesian trees and pointer manipulation, it has low cache-efficiency and high memory requirements, both of which deteriorate the performance of implementations.\\n\\nSee also\\nAdaptive sort\\nHeapsort\\nCartesian tree\\n\\n\\n== References ==\"}, {\"Block sort\": \"Block sort, or block merge sort, is a sorting algorithm combining at least two merge operations with an insertion sort to arrive at O(n log n) in-place stable sorting. It gets its name from the observation that merging two sorted lists, A and B, is equivalent to breaking A into evenly sized blocks, inserting each A block into B under special rules, and merging AB pairs.\\nOne practical algorithm for O(log n) in place merging was proposed by Pok-Son Kim and Arne Kutzner in 2008.\\n\\nOverview\\nThe outer loop of block sort is identical to a bottom-up merge sort, where each level of the sort merges pairs of subarrays, A and B, in sizes of 1, then 2, then 4, 8, 16, and so on, until both subarrays combined are the array itself.\\nRather than merging A and B directly as with traditional methods, a block-based merge algorithm divides A into discrete blocks of size \\u221aA (resulting in \\u221aA number of blocks as well), inserts each A block into B such that the first value of each A block is less than or equal (\\u2264) to the B value immediately after it, then locally merges each A block with any B values between it and the next A block.\\nAs merges still require a separate buffer large enough to hold the A block to be merged, two areas within the array are reserved for this purpose (known as internal buffers). The first two A blocks are thus modified to contain the first instance of each value within A, with the original contents of those blocks shifted over if necessary. The remaining A blocks are then inserted into B and merged using one of the two buffers as swap space. This process causes the values in that buffer to be rearranged.\\nOnce every A and B block of every A and B subarray have been merged for that level of the merge sort, the values in that buffer must be sorted to restore their original order, so an insertion sort must be applied. The values in the buffers are then redistributed to their first sorted position within the array. This process repeats for each level of the outer bottom-up merge sort, at which point the array will have been stably sorted.\\n\\nAlgorithm\\nThe following operators are used in the code examples:\\n\\nAdditionally, block sort relies on the following operations as part of its overall algorithm:\\n\\nSwap: exchange the positions of two values in an array.\\nBlock swap: exchange a range of values within an array with values in a different range of the array.\\nBinary search: assuming the array is sorted, check the middle value of the current search range, then if the value is lesser check the lower range, and if the value is greater check the upper range. Block sort uses two variants: one which finds the first position to insert a value in the sorted array, and one which finds the last position.\\nLinear search: find a particular value in an array by checking every single element in order, until it is found.\\nInsertion sort: for each item in the array, loop backward and find where it needs to be inserted, then insert it at that position.\\nArray rotation: move the items in an array to the left or right by some number of spaces, with values on the edges wrapping around to the other side. Rotations can be implemented as three reversals.Rotate(array, amount, range)\\n    Reverse(array, range)\\n    Reverse(array, [range.start, range.start + amount))\\n    Reverse(array, [range.start + amount, range.end))\\n\\nFloor power of two: floor a value to the next power of two. Thus 63 becomes 32, 64 stays 64, and so forth.FloorPowerOfTwo(x)\\n    x = x | (x >> 1)\\n    x = x | (x >> 2)\\n    x = x | (x >> 4)\\n    x = x | (x >> 8)\\n    x = x | (x >> 16)\\n    if (this is a 64-bit system)\\n        x = x | (x >> 32)\\n    return x - (x >> 1)\\n\\nOuter loop\\nAs previously stated, the outer loop of a block sort is identical to a bottom-up merge sort. However, it benefits from the variant that ensures each A and B subarray are the same size to within one item:\\n\\n   BlockSort(array)\\n       power_of_two = FloorPowerOfTwo(array.size)\\n       scale = array.size/power_of_two // 1.0 \\u2264 scale < 2.0\\n      \\n       // insertion sort 16\\u201331 items at a time\\n       for (merge = 0; merge < power_of_two; merge += 16)\\n           start = merge * scale\\n           end = start + 16 * scale\\n           InsertionSort(array, [start, end))\\n      \\n       for (length = 16; length < power_of_two; length += length)\\n           for (merge = 0; merge < power_of_two; merge += length * 2)\\n               start = merge * scale\\n               mid = (merge + length) * scale\\n               end = (merge + length * 2) * scale\\n              \\n               if (array[end \\u2212 1] < array[start])\\n                   // the two ranges are in reverse order, so a rotation is enough to merge them\\n                   Rotate(array, mid \\u2212 start, [start, end))\\n               else if (array[mid \\u2212 1] > array[mid])\\n                   Merge(array, A = [start, mid), B = [mid, end))\\n               // else the ranges are already correctly ordered\\n\\nFixed-point math may also be used, by representing the scale factor as a fraction integer_part + numerator/denominator:\\n\\n   power_of_two = FloorPowerOfTwo(array.size)\\n   denominator = power_of_two/16\\n   numerator_step = array.size % denominator\\n   integer_step = floor(array.size/denominator)\\n  \\n   // insertion sort 16\\u201331 items at a time\\n  \\n   while (integer_step < array.size)\\n       integer_part = numerator = 0\\n       while (integer_part < array.size)\\n           // get the ranges for A and B\\n           start = integer_part\\n          \\n           integer_part += integer_step\\n           numerator += numerator_step\\n           if (numerator \\u2265 denominator)\\n               numerator \\u2212= denominator\\n               integer_part++\\n          \\n           mid = integer_part\\n          \\n           integer_part += integer_step\\n           numerator += numerator_step\\n           if (numerator \\u2265 denominator)\\n               numerator \\u2212= denominator\\n               integer_part++\\n          \\n           end = integer_part\\n          \\n           if (array[end \\u2212 1] < array[start])\\n               Rotate(array, mid \\u2212 start, [start, end))\\n           else if (array[mid \\u2212 1] > array[mid])\\n               Merge(array, A = [start, mid), B = [mid, end))\\n      \\n       integer_step += integer_step\\n       numerator_step += numerator_step\\n       if (numerator_step \\u2265 denominator)\\n           numerator_step \\u2212= denominator\\n           integer_step++\\n\\nExtract buffers\\nThe two internal buffers needed for each level of the merge step are created by moving the first 2\\u221aA first instances of their values within an A subarray to the start of A. First it iterates over the elements in A and counts off the unique values it needs, then it applies array rotations to move those unique values to the start. If A did not contain enough unique values to fill the two buffers (of size \\u221aA each), B can be used just as well. In this case it moves the last instance of each value to the end of B, with that part of B not being included during the merges.\\n\\nwhile (integer_step < array.size)\\n    block_size = \\u221ainteger_step\\n    buffer_size = integer_step/block_size + 1\\n    [extract two buffers of size 'buffer_size' each]\\n\\nIf B does not contain enough unique values either, it pulls out the largest number of unique values it could find, then adjusts the size of the A and B blocks such that the number of resulting A blocks is less than or equal to the number of unique items pulled out for the buffer. Only one buffer will be used in this case \\u2013 the second buffer won't exist.\\n\\nbuffer_size = [number of unique values found]\\nblock_size = integer_step/buffer_size + 1\\n  \\ninteger_part = numerator = 0\\nwhile (integer_part < array.size)\\n    [get the ranges for A and B]\\n    [adjust A and B to not include the ranges used by the buffers]\\n\\nTag A blocks\\nOnce the one or two internal buffers have been created, it begins merging each A and B subarray for this level of the merge sort. To do so, it divides each A and B subarray into evenly sized blocks of the size calculated in the previous step, where the first A block and last B block are unevenly sized if needed. It then loops over each of the evenly sized A blocks and swaps the second value with a corresponding value from the first of the two internal buffers. This is known as tagging the blocks.\\n\\n// blockA is the range of the remaining A blocks,\\n// and firstA is the unevenly sized first A block\\nblockA = [A.start, A.end)\\nfirstA = [A.start, A.start + |blockA| % block_size)\\n  \\n// swap the second value of each A block with the value in buffer1\\nfor (index = 0, indexA = firstA.end + 1; indexA < blockA.end; indexA += block_size)\\n    Swap(array[buffer1.start + index], array[indexA])\\n    index++\\n  \\nlastA = firstA\\nblockB = [B.start, B.start + minimum(block_size, |B|))\\nblockA.start += |firstA|\\n\\nRoll and drop\\nAfter defining and tagging the A blocks in this manner, the A blocks are rolled through the B blocks by block swapping the first evenly sized A block with the next B block. This process repeats until the first value of the A block with the smallest tag value is less than or equal to the last value of the B block that was just swapped with an A block.\\nAt that point, the minimum A block (the A block with the smallest tag value) is swapped to the start of the rolling A blocks and the tagged value is restored with its original value from the first buffer. This is known as dropping a block behind, as it will no longer be rolled along with the remaining A blocks. That A block is then inserted into the previous B block, first by using a binary search on B to find the index where the first value of A is less than or equal to the value at that index of B, and then by rotating A into B at that index.\\n\\n   minA = blockA.start\\n   indexA = 0\\n  \\n   while (true)\\n       // if there's a previous B block and the first value of the minimum A block is \\u2264\\n       // the last value of the previous B block, then drop that minimum A block behind.\\n       // or if there are no B blocks left then keep dropping the remaining A blocks.\\n       if ((|lastB| > 0 and array[lastB.end - 1] \\u2265 array[minA]) or |blockB| = 0)\\n           // figure out where to split the previous B block, and rotate it at the split\\n           B_split = BinaryFirst(array, array[minA], lastB)\\n           B_remaining = lastB.end - B_split\\n          \\n           // swap the minimum A block to the beginning of the rolling A blocks\\n           BlockSwap(array, blockA.start, minA, block_size)\\n          \\n           // restore the second value for the A block\\n           Swap(array[blockA.start + 1], array[buffer1.start + indexA])\\n           indexA++\\n          \\n           // rotate the A block into the previous B block\\n           Rotate(array, blockA.start - B_split, [B_split, blockA.start + block_size))\\n          \\n           // locally merge the previous A block with the B values that follow it,\\n           // using the second internal buffer as swap space (if it exists)\\n           if (|buffer2| > 0)\\n               MergeInternal(array, lastA, [lastA.end, B_split), buffer2)\\n           else\\n               MergeInPlace(array, lastA, [lastA.end, B_split))\\n          \\n           // update the range for the remaining A blocks,\\n           // and the range remaining from the B block after it was split\\n           lastA = [blockA.start - B_remaining, blockA.start - B_remaining + block_size)\\n           lastB = [lastA.end, lastA.end + B_remaining)\\n          \\n           // if there are no more A blocks remaining, this step is finished\\n           blockA.start = blockA.start + block_size\\n           if (|blockA| = 0)\\n               break\\n          \\n           minA = [new minimum A block] (see below)\\n       else if (|blockB| < block_size)\\n           // move the last B block, which is unevenly sized,\\n           // to before the remaining A blocks, by using a rotation\\n           Rotate(array, blockB.start - blockA.start, [blockA.start, blockB.end))\\n          \\n           lastB = [blockA.start, blockA.start + |blockB|)\\n           blockA.start += |blockB|\\n           blockA.end += |blockB|\\n           minA += |blockB|\\n           blockB.end = blockB.start\\n       else\\n           // roll the leftmost A block to the end by swapping it with the next B block\\n           BlockSwap(array, blockA.start, blockB.start, block_size)\\n           lastB = [blockA.start, blockA.start + block_size)\\n           if (minA = blockA.start)\\n               minA = blockA.end\\n          \\n           blockA.start += block_size\\n           blockA.end += block_size\\n           blockB.start += block_size\\n          \\n           // this is equivalent to minimum(blockB.end + block_size, B.end),\\n           // but that has the potential to overflow\\n           if (blockB.end > B.end - block_size)\\n               blockB.end = B.end\\n           else\\n               blockB.end += block_size\\n  \\n   // merge the last A block with the remaining B values\\n   if (|buffer2| > 0)\\n       MergeInternal(array, lastA, [lastA.end, B.end), buffer2)\\n   else\\n       MergeInPlace(array, lastA, [lastA.end, B.end))\\n\\nOne optimization that can be applied during this step is the floating-hole technique. When the minimum A block is dropped behind and needs to be rotated into the previous B block, after which its contents are swapped into the second internal buffer for the local merges, it would be faster to swap the A block to the buffer beforehand, and to take advantage of the fact that the contents of that buffer do not need to retain any order. So rather than rotating the second buffer (which used to be the A block before the block swap) into the previous B block at position index, the values in the B block after index can simply be block swapped with the last items of the buffer.\\nThe floating hole in this case refers to the contents of the second internal buffer floating around the array, and acting as a hole in the sense that the items do not need to retain their order.\\n\\nLocal merges\\nOnce the A block has been rotated into the B block, the previous A block is then merged with the B values that follow it, using the second buffer as swap space. When the first A block is dropped behind this refers to the unevenly sized A block at the start, when the second A block is dropped behind it means the first A block, and so forth.\\n\\nMergeInternal(array, A, B, buffer)\\n    // block swap the values in A with those in 'buffer'\\n    BlockSwap(array, A.start, buffer.start, |A|)\\n\\n    A_count = 0, B_count = 0, insert = 0\\n    while (A_count < |A| and B_count < |B|)\\n        if (array[buffer.start + A_count] \\u2264 array[B.start + B_count])\\n            Swap(array[A.start + insert], array[buffer.start + A_count])\\n            A_count++\\n        else\\n            Swap(array[A.start + insert], array[B.start + B_count])\\n            B_count++\\n        insert++\\n\\n    // block swap the remaining part of the buffer with the remaining part of the array\\n    BlockSwap(array, buffer.start + A_count, A.start + insert, |A| - A_count)\\n\\nIf the second buffer does not exist, a strictly in-place merge operation must be performed, such as a rotation-based version of the Hwang and Lin algorithm, the Dudzinski and Dydek algorithm, or a repeated binary search and rotate.\\n\\nMergeInPlace(array, A, B)\\n    while (|A| > 0 and |B| > 0)\\n        // find the first place in B where the first item in A needs to be inserted\\n        mid = BinaryFirst(array, array[A.start], B)\\n\\n        // rotate A into place\\n        amount = mid - A.end\\n        Rotate(array, amount, [A.start, mid))\\n\\n        // calculate the new A and B ranges\\n        B = [mid, B.end)\\n        A = [A.start + amount, mid)\\n        A.start = BinaryLast(array, array[A.start], A)\\n\\nAfter dropping the minimum A block and merging the previous A block with the B values that follow it, the new minimum A block must be found within the blocks that are still being rolled through the array. This is handled by running a linear search through those A blocks and comparing the tag values to find the smallest one.\\n\\nminA = blockA.start\\nfor (findA = minA + block_size; findA < blockA.end - 1; findA += block_size)\\n    if (array[findA + 1] < array[minA + 1])\\n        minA = findA\\n\\nThese remaining A blocks then continue rolling through the array and being dropped and inserted where they belong. This process repeats until all of the A blocks have been dropped and rotated into the previous B block.\\nOnce the last remaining A block has been dropped behind and inserted into B where it belongs, it should be merged with the remaining B values that follow it. This completes the merge process for that particular pair of A and B subarrays. However, it must then repeat the process for the remaining A and B subarrays for the current level of the merge sort.\\nNote that the internal buffers can be reused for every set of A and B subarrays for this level of the merge sort, and do not need to be re-extracted or modified in any way.\\n\\nRedistribute\\nAfter all of the A and B subarrays have been merged, the one or two internal buffers are still left over. The first internal buffer was used for tagging the A blocks, and its contents are still in the same order as before, but the second internal buffer may have had its contents rearranged when it was used as swap space for the merges. This means the contents of the second buffer will need to be sorted using a different algorithm, such as insertion sort. The two buffers must then be redistributed back into the array using the opposite process that was used to create them.\\nAfter repeating these steps for every level of the bottom-up merge sort, the block sort is completed.\\n\\nVariants\\nBlock sort works by extracting two internal buffers, breaking A and B subarrays into evenly sized blocks, rolling and dropping the A blocks into B (using the first buffer to track the order of the A blocks), locally merging using the second buffer as swap space, sorting the second buffer, and redistributing both buffers. While the steps do not change, these subsystems can vary in their actual implementation.\\nOne variant of block sort allows it to use any amount of additional memory provided to it, by using this external buffer for merging an A subarray or A block with B whenever A fits into it. In this situation it would be identical to a merge sort.\\nGood choices for the buffer size include:\\n\\nRather than tagging the A blocks using the contents of one of the internal buffers, an indirect movement-imitation buffer can be used instead. This is an internal buffer defined as s1 t s2, where s1 and s2 are each as large as the number of A and B blocks, and t contains any values immediately following s1 that are equal to the last value of s1 (thus ensuring that no value in s2 appears in s1). A second internal buffer containing \\u221aA unique values is still used. The first \\u221aA values of s1 and s2 are then swapped with each other to encode information into the buffer about which blocks are A blocks and which are B blocks. When an A block at index i is swapped with a B block at index j (where the first evenly sized A block is initially at index 0), s1[i] and s1[j] are swapped with s2[i] and s2[j], respectively. This imitates the movements of the A blocks through B. The unique values in the second buffer are used to determine the original order of the A blocks as they are rolled through the B blocks. Once all of the A blocks have been dropped, the movement-imitation buffer is used to decode whether a given block in the array is an A block or a B block, each A block is rotated into B, and the second internal buffer is used as swap space for the local merges.\\nThe second value of each A block doesn't necessarily need to be tagged \\u2013 the first, last, or any other element could be used instead. However, if the first value is tagged, the values will need to be read from the first internal buffer (where they were swapped) when deciding where to drop the minimum A block.\\nMany sorting algorithms can be used to sort the contents of the second internal buffer, including unstable sorts like quicksort, since the contents of the buffer are guaranteed to unique. Insertion sort is still recommended, though, for its situational performance and lack of recursion.\\n\\nAnalysis\\nBlock sort is a well-defined and testable class of algorithms, with working implementations available as a merge and as a sort. This allows its characteristics to be measured and considered.\\n\\nComplexity\\nBlock sort begins by performing insertion sort on groups of 16\\u201331 items in the array. Insertion sort is an \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   operation, so this leads to anywhere from \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          16\\n          \\n            2\\n          \\n        \\n        \\u00d7\\n        n\\n        \\n          /\\n        \\n        16\\n        )\\n      \\n    \\n    {\\\\displaystyle O(16^{2}\\\\times n/16)}\\n   to \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          31\\n          \\n            2\\n          \\n        \\n        \\u00d7\\n        n\\n        \\n          /\\n        \\n        31\\n        )\\n      \\n    \\n    {\\\\displaystyle O(31^{2}\\\\times n/31)}\\n  , which is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   once the constant factors are omitted. It must also apply an insertion sort on the second internal buffer after each level of merging is completed. However, as this buffer was limited to \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   in size, the \\n  \\n    \\n      \\n        O\\n        \\n          \\n            \\n              n\\n            \\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle O{\\\\sqrt {n}}^{2}}\\n   operation also ends up being \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\nNext it must extract two internal buffers for each level of the merge sort. It does so by iterating over the items in the A and B subarrays and incrementing a counter whenever the value changes, and upon finding enough values it rotates them to the start of A or the end of B. In the worst case this will end up searching the entire array before finding \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   non-contiguous unique values, which requires \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   comparisons and \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   rotations for \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   values. This resolves to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        +\\n        \\n          \\n            n\\n          \\n        \\n        \\u00d7\\n        \\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n+{\\\\sqrt {n}}\\\\times {\\\\sqrt {n}})}\\n  , or \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\nWhen none of the A or B subarrays contained \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   unique values to create the internal buffers, a normally suboptimal in-place merge operation is performed where it repeatedly binary searches and rotates A into B. However, the known lack of unique values within any of the subarrays places a hard limit on the number of binary searches and rotations that will be performed during this step, which is again \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   items rotated up to \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times, or \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  . The size of each block is also adjusted to be smaller in the case where it found \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   unique values but not 2\\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n  , which further limits the number of unique values contained within any A or B block.\\nTagging the A blocks is performed \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times for each A subarray, then the A blocks are rolled through and inserted into the B blocks up to \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times. The local merges retain the same \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   complexity of a standard merge, albeit with more assignments since the values must be swapped rather than copied. The linear search for finding the new minimum A block iterates over \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   blocks \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times. And the buffer redistribution process is identical to the buffer extraction but in reverse, and therefore has the same \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   complexity.\\nAfter omitting all but the highest complexity and considering that there are \\n  \\n    \\n      \\n        log\\n        \\u2061\\n        \\n          n\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\log {n}}\\n   levels in the outer merge loop, this leads to a final asymptotic complexity of \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        \\n          n\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log {n})}\\n   for the worst and average cases. For the best case, where the data is already in order, the merge step performs  \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        16\\n      \\n    \\n    {\\\\displaystyle n/16}\\n   comparisons for the first level, then \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        32\\n      \\n    \\n    {\\\\displaystyle n/32}\\n  , \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        64\\n      \\n    \\n    {\\\\displaystyle n/64}\\n  , \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        128\\n      \\n    \\n    {\\\\displaystyle n/128}\\n  , etc. This is a well-known mathematical series which resolves to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\n\\nMemory\\nAs block sort is non-recursive and does not require the use of dynamic allocations, this leads to constant stack and heap space. It uses O(1) auxiliary memory in a transdichotomous model, which accepts that the O(log n) bits needed to keep track of the ranges for A and B cannot be any greater than 32 or 64 on 32-bit or 64-bit computing systems, respectively, and therefore simplifies to O(1) space for any array that can feasibly be allocated.\\n\\nStability\\nAlthough items in the array are moved out of order during a block sort, each operation is fully reversible and will have restored the original order of equivalent items by its completion.\\nStability requires the first instance of each value in an array before sorting to still be the first instance of that value after sorting. Block sort moves these first instances to the start of the array to create the two internal buffers, but when all of the merges are completed for the current level of the block sort, those values are distributed back to the first sorted position within the array. This maintains stability.\\nBefore rolling the A blocks through the B blocks, each A block has its second value swapped with a value from the first buffer. At that point the A blocks are moved out of order to roll through the B blocks. However, once it finds where it should insert the smallest A block into the previous B block, that smallest A block is moved back to the start of the A blocks and its second value is restored. By the time all of the A blocks have been inserted, the A blocks will be in order again and the first buffer will contain its original values in the original order.\\nUsing the second buffer as swap space when merging an A block with some B values causes the contents of that buffer to be rearranged. However, as the algorithm already ensured the buffer only contains unique values, sorting the contents of the buffer is sufficient to restore their original stable order.\\n\\nAdaptivity\\nBlock sort is an adaptive sort on two levels: first, it skips merging A and B subarrays that are already in order. Next, when A and B need to be merged and are broken into evenly sized blocks, the A blocks are only rolled through B as far as is necessary, and each block is only merged with the B values immediately following it. The more ordered the data originally was, the fewer B values there will be that need to be merged into A.\\n\\nAdvantages\\nBlock sort is a stable sort that does not require additional memory, which is useful in cases where there is not enough free memory to allocate the O(n) buffer. When using the external buffer variant of block sort, it can scale from using O(n) memory to progressively smaller buffers as needed, and will still work efficiently within those constraints.\\n\\nDisadvantages\\nBlock sort does not exploit sorted ranges of data on as fine a level as some other algorithms, such as Timsort. It only checks for these sorted ranges at the two predefined levels: the A and B subarrays, and the A and B blocks. It is also harder to implement and parallelize compared to a merge sort.\\n\\n\\n== References ==\"}, {\"Bogosort\": \"In computer science, bogosort (also known as permutation sort, stupid sort, slowsort or bozosort) is a sorting algorithm based on the generate and test paradigm. The function successively generates permutations of its input until it finds one that is sorted. It is not considered useful for sorting, but may be used for educational purposes, to contrast it with more efficient algorithms.\\nTwo versions of this algorithm exist: a deterministic version that enumerates all permutations until it hits a sorted one, and a randomized version that randomly permutes its input. An analogy for the working of the latter version is to sort a deck of cards by throwing the deck into the air, picking the cards up at random, and repeating the process until the deck is sorted. Its name is a portmanteau of the words bogus and sort.\\n\\nDescription of the algorithm\\nThe following is a description of the randomized algorithm in pseudocode:\\n\\nwhile not sorted(deck):\\n    shuffle(deck)\\n\\nHere is the above pseudocode rewritten in Python 3:\\n\\nThis code assumes that data is a simple, mutable, array-like data structure\\u2014like Python's built-in list\\u2014whose elements can be compared without issue.\\n\\nRunning time and termination\\nIf all elements to be sorted are distinct, the expected number of comparisons performed in the average case by randomized bogosort is asymptotically equivalent to (e \\u2212 1)n!, and the expected number of swaps in the average case equals (n \\u2212 1)n!. The expected number of swaps grows faster than the expected number of comparisons, because if the elements are not in order, this will usually be discovered after only a few comparisons, no matter how many elements there are; but the work of shuffling the collection is proportional to its size. In the worst case, the number of comparisons and swaps are both unbounded, for the same reason that a tossed coin might turn up heads any number of times in a row.\\nThe best case occurs if the list as given is already sorted; in this case the expected number of comparisons is n \\u2212 1, and no swaps at all are carried out.For any collection of fixed size, the expected running time of the algorithm is finite for much the same reason that the infinite monkey theorem holds: there is some probability of getting the right permutation, so given an unbounded number of tries it will almost surely eventually be chosen.\\n\\nRelated algorithms\\nGorosort\\nis a sorting algorithm introduced in the 2011 Google Code Jam. As long as the list is not in order, a subset of all elements is randomly permuted. If this subset is optimally chosen each time this is performed, the expected value of the total number of times this operation needs to be done is equal to the number of misplaced elements.\\nBogobogosort\\nis an algorithm that was designed not to succeed before the heat death of the universe on any sizable list. It works by recursively calling itself with smaller and smaller copies of the beginning of the list to see if they are sorted.  The base case is a single element, which is always sorted.  For other cases, it compares the last element to the maximum element from the previous elements in the list.  If the last element is greater or equal, it checks if the order of the copy matches the previous version, and if so returns.  Otherwise, it reshuffles the current copy of the list and restarts its recursive check.\\nBozosort\\nis another sorting algorithm based on random numbers. If the list is not in order, it picks two items at random and swaps them, then checks to see if the list is sorted. The running time analysis of a bozosort is more difficult, but some estimates are found in H. Gruber's analysis of \\\"perversely awful\\\" randomized sorting algorithms. O(n!) is found to be the expected average case.\\nWorstsort\\nis a pessimal sorting algorithm that is guaranteed to complete in finite time; however, its efficiency can be arbitrarily bad, depending on its configuration. The worstsort algorithm is based on a bad sorting algorithm, badsort. The badsort algorithm accepts two parameters: L, which is the list to be sorted, and k, which is a recursion depth. At recursion level k = 0, badsort merely uses a common sorting algorithm, such as bubblesort, to sort its inputs and return the sorted list. That is to say, badsort(L, 0) = bubblesort(L). Therefore, badsort's time complexity is O(n2) if k = 0. However, for any k > 0, badsort(L, k) first generates P, the list of all permutations of L. Then, badsort calculates badsort(P, k \\u2212 1), and returns the first element of the sorted P. To make worstsort truly pessimal, k may be assigned to the value of a computable increasing function such as \\n  \\n    \\n      \\n        f\\n        :\\n        \\n          N\\n        \\n        \\u2192\\n        \\n          N\\n        \\n      \\n    \\n    {\\\\displaystyle f\\\\colon \\\\mathbb {N} \\\\to \\\\mathbb {N} }\\n   (e.g. f(n) = A(n, n), where A is Ackermann's function).  Ergo, to sort a list arbitrarily badly, you would execute worstsort(L, f) = badsort(L, f(length(L))), where length(L) is the number of elements in L. The resulting algorithm has complexity \\n  \\n    \\n      \\n        \\u03a9\\n        \\n          (\\n          \\n            \\n              (\\n              \\n                n\\n                \\n                  !\\n                  \\n                    (\\n                    f\\n                    (\\n                    n\\n                    )\\n                    )\\n                  \\n                \\n              \\n              )\\n            \\n            \\n              2\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\textstyle \\\\Omega \\\\left(\\\\left(n!^{(f(n))}\\\\right)^{2}\\\\right)}\\n  , where \\n  \\n    \\n      \\n        n\\n        \\n          !\\n          \\n            (\\n            m\\n            )\\n          \\n        \\n        =\\n        (\\n        \\u2026\\n        (\\n        (\\n        n\\n        !\\n        )\\n        !\\n        )\\n        !\\n        \\u2026\\n        )\\n        !\\n      \\n    \\n    {\\\\displaystyle n!^{(m)}=(\\\\dotso ((n!)!)!\\\\dotso )!}\\n   = factorial of n iterated m times. This algorithm can be made as inefficient as one wishes by picking a fast enough growing function f.\\nSlowsort\\nis a different humorous sorting algorithm that employs a misguided divide-and-conquer strategy to achieve massive complexity.\\nQuantum Bogosort\\nis a hypothetical sorting algorithm based on bogosort, created as an in-joke among computer scientists. The algorithm generates a random permutation of its input using a quantum source of entropy, checks if the list is sorted, and, if it is not, destroys the universe. Assuming that the many-worlds interpretation holds, the use of this algorithm will result in at least one surviving universe where the input was successfully sorted in O(n) time.\\nMiracle sort\\nis a sorting algorithm that checks if the array is sorted until a miracle occurs. It continually checks the array until it is sorted, never changing the order of the array. Because the order is never altered, the algorithm has a hypothetical time complexity of O(\\u221e), but it can still sort through events such as miracles or single-event upsets. Particular care must be taken in the implementation of this algorithm as optimizing compilers may simply transform it into a while(true) loop.\\n\\nSee also\\nLas Vegas algorithm\\nStooge sort\\n\\nNotes\\nReferences\\nExternal links\\n\\nBogoSort on WikiWikiWeb\\nInefficient sort algorithms\\nBogosort: an implementation that runs on Unix-like systems, similar to the standard sort program.\\nBogosort and jmmcg::bogosort: Simple, yet perverse, C++ implementations of the bogosort algorithm.\\nBogosort NPM package: bogosort implementation for Node.js ecosystem.\\nMax Sherman Bogo-sort is Sort of Slow, June 2013\"}, {\"Bubble sort\": \"Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the input list element by element, comparing the current element with the one after it, swapping their values if needed. These passes through the list are repeated until no swaps had to be performed during a pass, meaning that the list has become fully sorted. The algorithm, which is a comparison sort, is named for the way the larger elements \\\"bubble\\\" up to the top of the list. \\nThis simple algorithm performs poorly in real world use and is used primarily as an educational tool. More efficient algorithms such as quicksort, timsort, or merge sort are used by the sorting libraries built into popular programming languages such as Python and Java.\\n\\nAnalysis\\nPerformance\\nBubble sort has a worst-case and average complexity of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  , where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of items being sorted. Most practical sorting algorithms have substantially better worst-case or average complexity, often \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n  . Even other \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   sorting algorithms, such as insertion sort, generally run faster than bubble sort, and are no more complex. For this reason, bubble sort is rarely used in practice.\\nLike insertion sort, bubble sort is adaptive, giving it an advantage over algorithms like quicksort. This means that it may outperform those algorithms in cases where the list is already mostly sorted (having a small number of inversions), despite the fact that it has worse average-case time complexity. For example, bubble sort is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   on a list that is already sorted, while quicksort would still perform its entire \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   sorting process.\\nWhile any sorting algorithm can be made \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   on a presorted list simply by checking the list before the algorithm runs, improved performance on almost-sorted lists is harder to replicate.\\n\\nRabbits and turtles\\nThe distance and direction that elements must move during the sort determine bubble sort's performance because elements move in different directions at different speeds. An element that must move toward the end of the list can move quickly because it can take part in successive swaps. For example, the largest element in the list will win every swap, so it moves to its sorted position on the first pass even if it starts near the beginning. On the other hand, an element that must move toward the beginning of the list cannot move faster than one step per pass, so elements move toward the beginning very slowly. If the smallest element is at the end of the list, it will take \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   passes to move it to the beginning. This has led to these types of elements being named rabbits and turtles, respectively, after the characters in Aesop's fable of The Tortoise and the Hare.\\nVarious efforts have been made to eliminate turtles to improve upon the speed of bubble sort. Cocktail sort is a bi-directional bubble sort that goes from beginning to end, and then reverses itself, going end to beginning. It can move turtles fairly well, but it retains \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   worst-case complexity. Comb sort compares elements separated by large gaps, and can move turtles extremely quickly before proceeding to smaller and smaller gaps to smooth out the list. Its average speed is comparable to faster algorithms like quicksort.\\n\\nStep-by-step example\\nTake an array of numbers \\\"5 1 4 2 8\\\", and sort the array from lowest number to greatest number using bubble sort. In each step, elements written in bold are being compared. Three passes will be required;\\n\\nFirst Pass\\n( 5 1 4 2 8 ) \\u2192 ( 1 5 4 2 8 ), Here, algorithm compares the first two elements, and swaps since 5 > 1.\\n( 1 5 4 2 8 ) \\u2192 ( 1 4 5 2 8 ), Swap since 5 > 4\\n( 1 4 5 2 8 ) \\u2192 ( 1 4 2 5 8 ), Swap since 5 > 2\\n( 1 4 2 5 8 ) \\u2192 ( 1 4 2 5 8 ), Now, since these elements are already in order (8 > 5), algorithm does not swap them.\\nSecond Pass\\n( 1 4 2 5 8 ) \\u2192 ( 1 4 2 5 8 )\\n( 1 4 2 5 8 ) \\u2192 ( 1 2 4 5 8 ), Swap since 4 > 2\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )Now, the array is already sorted, but the algorithm does not know if it is completed. The algorithm needs one additional whole pass without any swap to know it is sorted.\\n\\nThird Pass\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n\\nImplementation\\nPseudocode implementation\\nIn pseudocode the algorithm can be expressed as (0-based array):\\n\\nOptimizing bubble sort\\nThe bubble sort algorithm can be optimized by observing that the n-th pass finds the n-th largest element and puts it into its final place. So, the inner loop can avoid looking at the last n \\u2212 1 items when running for the n-th time:\\n\\nMore generally, it can happen that more than one element is placed in their final position on a single pass. In particular, after every pass, all elements after the last swap are sorted, and do not need to be checked again. This allows to skip over many elements, resulting in about a worst case 50% improvement in comparison count (though no improvement in swap counts), and adds very little complexity because the new code subsumes the \\\"swapped\\\" variable:\\nTo accomplish this in pseudocode, the following can be written:\\n\\nAlternate modifications, such as the cocktail shaker sort attempt to improve on the bubble sort performance while keeping the same idea of repeatedly comparing and swapping adjacent items.\\n\\nUse\\nAlthough bubble sort is one of the simplest sorting algorithms to understand and implement, its O(n2) complexity means that its efficiency decreases dramatically on lists of more than a small number of elements. Even among simple O(n2) sorting algorithms, algorithms like insertion sort are usually considerably more efficient.\\nDue to its simplicity, bubble sort is often used to introduce the concept of an algorithm, or a sorting algorithm, to introductory computer science students. However, some researchers such as Owen Astrachan have gone to great lengths to disparage bubble sort and its continued popularity in computer science education, recommending that it no longer even be taught.The Jargon File, which famously calls bogosort \\\"the archetypical [sic] perversely awful algorithm\\\", also calls bubble sort \\\"the generic bad algorithm\\\". Donald Knuth, in The Art of Computer Programming, concluded that \\\"the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems\\\", some of which he then discusses.Bubble sort is asymptotically equivalent in running time to insertion sort in the worst case, but the two algorithms differ greatly in the number of swaps necessary. Experimental results such as those of Astrachan have also shown that insertion sort performs considerably better even on random lists. For these reasons many modern algorithm textbooks avoid using the bubble sort algorithm in favor of insertion sort.\\nBubble sort also interacts poorly with modern CPU hardware. It produces at least twice as many writes as insertion sort, twice as many cache misses, and asymptotically more branch mispredictions. Experiments by Astrachan sorting strings in Java show bubble sort to be roughly one-fifth as fast as an insertion sort and 70% as fast as a selection sort.In computer graphics bubble sort is popular for its capability to detect a very small error (like swap of just two elements) in almost-sorted arrays and fix it with just linear complexity (2n). For example, it is used in a polygon filling algorithm, where bounding lines are sorted by their x coordinate at a specific scan line (a line parallel to the x axis) and with incrementing y their order changes (two elements are swapped) only at intersections of two lines. Bubble sort is a stable sort algorithm, like insertion sort.\\n\\nVariations\\nOdd\\u2013even sort is a parallel version of bubble sort, for message passing systems.\\nPasses can be from right to left, rather than left to right. This is more efficient for lists with unsorted items added to the end.\\nCocktail shaker sort alternates leftwards and rightwards passes.\\nI can't believe it can sort is a sorting algorithm that appears to be an incorrect version of bubble sort, but can be formally proven to work in a way more akin to insertion sort.\\n\\nDebate over name\\nBubble sort has been occasionally referred to as a \\\"sinking sort\\\".For example, Donald Knuth describes the insertion of values at or towards their desired location as letting \\\"[the value] settle to its proper level\\\", and that \\\"this method of sorting has sometimes been called the sifting or sinking technique.This debate is perpetuated by the ease with which one may consider this algorithm from two different but equally valid perspectives:\\n\\nThe larger values might be regarded as heavier and therefore be seen to progressively sink to the bottom of the list\\nThe smaller values might be regarded as lighter and therefore be seen to progressively bubble up to the top of the list.\\n\\nIn popular culture\\nIn 2007, former Google CEO Eric Schmidt asked then-presidential candidate Barack Obama during an interview about the best way to sort one million integers; Obama paused for a moment and replied: \\\"I think the bubble sort would be the wrong way to go.\\\"\\n\\nNotes\\nReferences\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Problem 2-2, pg.40.\\nSorting in the Presence of Branch Prediction and Caches\\nFundamentals of Data Structures by Ellis Horowitz, Sartaj Sahni and Susan Anderson-Freed ISBN 81-7371-605-6\\nOwen Astrachan. Bubble Sort: An Archaeological Algorithmic Analysis\\nComputer Integrated Manufacturing by Spasic PhD, Srdic MSc, Open Source, 1987.[1]\\n\\nExternal links\\n\\nMartin, David R. (2007). \\\"Animated Sorting Algorithms: Bubble Sort\\\". Archived from the original on 2015-03-03. \\u2013 graphical demonstration\\n\\\"Lafore's Bubble Sort\\\". (Java applet animation)\\nOEIS sequence A008302 (Table (statistics) of the number of permutations of [n] that need k pair-swaps during the sorting)\"}, {\"Cache-oblivious distribution sort\": \"The cache-oblivious distribution sort is a comparison-based sorting algorithm. It is similar to quicksort, but it is a cache-oblivious algorithm, designed for a setting where the number of elements to sort is too large to fit in a cache where operations are done. In the external memory model, the number of memory transfers it needs to perform a sort of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   items on a machine with cache of size \\n  \\n    \\n      \\n        Z\\n      \\n    \\n    {\\\\displaystyle Z}\\n   and cache lines of length \\n  \\n    \\n      \\n        L\\n      \\n    \\n    {\\\\displaystyle L}\\n   is \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          \\n            N\\n            L\\n          \\n        \\n        \\n          log\\n          \\n            Z\\n          \\n        \\n        \\u2061\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle O({\\\\frac {N}{L}}\\\\log _{Z}N)}\\n  , under the tall cache assumption that \\n  \\n    \\n      \\n        Z\\n        =\\n        \\u03a9\\n        (\\n        \\n          L\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle Z=\\\\Omega (L^{2})}\\n  . This number of memory transfers has been shown to be asymptotically optimal for comparison sorts. This distribution sort also achieves the asymptotically optimal runtime complexity of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        N\\n        log\\n        \\u2061\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (N\\\\log N)}\\n  .\\n\\nAlgorithm\\nOverview\\nDistribution sort operates on a contiguous array of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   elements. To sort the elements, it performs the following:\\n\\nPartition the array into \\n  \\n    \\n      \\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {N}}}\\n   contiguous subarrays of size \\n  \\n    \\n      \\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {N}}}\\n  , and recursively sort each subarray.\\nDistribute the elements of the sorted subarrays into \\n  \\n    \\n      \\n        q\\n        \\u2264\\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle q\\\\leq {\\\\sqrt {N}}}\\n   buckets \\n  \\n    \\n      \\n        \\n          B\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          B\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          B\\n          \\n            q\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B_{1},B_{2},\\\\ldots ,B_{q}}\\n   each of size at most \\n  \\n    \\n      \\n        2\\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2{\\\\sqrt {N}}}\\n   such that for every i from 1 to q-1,  every element of bucket \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B_{i}}\\n   is not larger than any element in \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n            +\\n            1\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle B_{i+1}.}\\n   This distribution step is the main step of this algorithm, and is covered in more detail below.\\nRecursively sort each bucket.\\nOutput the concatenation of the buckets.\\n\\nDistribution step\\nAs mentioned in step 2 above, the goal of the distribution step is to distribute the sorted subarrays into q buckets \\n  \\n    \\n      \\n        \\n          B\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          B\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          B\\n          \\n            q\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle B_{1},B_{2},\\\\ldots ,B_{q}.}\\n   The distribution step algorithm maintains two invariants. The first is that each bucket has size at most \\n  \\n    \\n      \\n        2\\n        \\n          \\n            N\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2{\\\\sqrt {N}}}\\n   at any time, and any element in bucket \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B_{i}}\\n   is no larger than any element in bucket \\n  \\n    \\n      \\n        \\n          B\\n          \\n            i\\n            +\\n            1\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle B_{i+1}.}\\n   The second is that every bucket has an associated pivot, a value which is greater than all elements in the bucket.\\nInitially, the algorithm starts with one empty bucket with pivot \\n  \\n    \\n      \\n        \\u221e\\n      \\n    \\n    {\\\\displaystyle \\\\infty }\\n  . As it fills buckets, it creates new buckets by splitting a bucket into two when it would be made overfull (by having at least \\n  \\n    \\n      \\n        (\\n        2\\n        \\n          \\n            N\\n          \\n        \\n        +\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle (2{\\\\sqrt {N}}+1)}\\n   elements placed into it). The split is done by performing the linear time median finding algorithm, and partitioning based on this median. The pivot of the lower bucket will be set to the median found, and the pivot of the higher bucket will be set to the same as the bucket before the split. At the end of the distribution step, all elements are in the buckets, and the two invariants will still hold.\\nTo accomplish this, each subarray and bucket will have a state associated with it. The state of a subarray consists of an index next of the next element to be read from the subarray, and a bucket number bnum indicating which bucket index the element should be copied to. By convention, \\n  \\n    \\n      \\n        b\\n        n\\n        u\\n        m\\n        =\\n        \\u221e\\n      \\n    \\n    {\\\\displaystyle bnum=\\\\infty }\\n   if all elements in the subarray have been distributed. (Note that when we split a bucket, we have to increment all bnum values of all subarrays whose bnum value is greater than the index of the bucket that is split.) The state of a bucket consists of the value of the bucket's pivot, and the number of elements currently in the bucket.\\nConsider the follow basic strategy: iterate through each subarray, attempting to copy over its element at position next. If the element is smaller than the pivot of bucket bnum, then place it in that bucket, possibly incurring a bucket split. Otherwise, increment bnum until a bucket whose pivot is large enough is found. Though this correctly distributes all elements, it does not exhibit a good cache performance.\\nInstead, the distribution step is performed in a recursive divide-and-conquer. The step will be performed as a call to the function distribute, which takes three parameters i, j, and m. distribute(i, j, m) will distribute elements from the i-th through (i+m-1)-th subarrays into buckets, starting from \\n  \\n    \\n      \\n        \\n          B\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle B_{j}}\\n  . It requires as a precondition that each subarray r in the range \\n  \\n    \\n      \\n        i\\n        ,\\n        \\u2026\\n        ,\\n        i\\n        +\\n        m\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle i,\\\\ldots ,i+m-1}\\n   has its \\n  \\n    \\n      \\n        b\\n        n\\n        u\\n        m\\n        [\\n        r\\n        ]\\n        \\u2265\\n        j\\n      \\n    \\n    {\\\\displaystyle bnum[r]\\\\geq j}\\n  . The execution of distribute(i, j, m) will guarantee that each \\n  \\n    \\n      \\n        b\\n        n\\n        u\\n        m\\n        [\\n        r\\n        ]\\n        \\u2265\\n        j\\n        +\\n        m\\n      \\n    \\n    {\\\\displaystyle bnum[r]\\\\geq j+m}\\n  . The whole distribution step is distribute\\n  \\n    \\n      \\n        (\\n        1\\n        ,\\n        1\\n        ,\\n        \\n          \\n            N\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (1,1,{\\\\sqrt {N}})}\\n  . Pseudocode for the implementation of distribute is shown below:\\n\\nThe base case, where m=1, has a call to the subroutine copy_elems. In this base case, all elements from subarray i that belong to bucket j are added at once. If this leads to bucket j having too many elements, it splits the bucket with the procedure described beforehand.\\n\\nSee also\\nCache-oblivious algorithm\\nFunnelsort\\nExternal sorting\\n\\nReferences\\nHarald Prokop. Cache-Oblivious Algorithms. Masters thesis, MIT. 1999.\"}, {\"Cascade merge sort\": \"Cascade merge sort is similar to the polyphase merge sort but uses a simpler distribution.  The merge is slower than a polyphase merge when there are fewer than six files, but faster when there are more than six.\\n\\nReferences\\nBradley, James (1982), File and Data Base Techniques, Holt, Rinehart and Winston, ISBN 0-03-058673-9\\n\\nExternal links\\nhttp://www.minkhollow.ca/Courses/461/Notes/Cosequential/Cascade.html\"}, {\"Cocktail shaker sort\": \"Cocktail shaker sort, also known as bidirectional bubble sort, cocktail sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is an extension of bubble sort.  The algorithm extends bubble sort by operating in two directions.  While it improves on bubble sort by more quickly moving items to the beginning of the list, it provides only marginal performance improvements. \\nLike most variants of bubble sort, cocktail shaker sort is used primarily as an educational tool. More performant algorithms such as quicksort, merge sort, or timsort are used by the sorting libraries built into popular programming languages such as Python and Java.\\n\\nPseudocode\\nThe simplest form goes through the whole list each time:\\n\\nprocedure cocktailShakerSort(A : list of sortable items) is\\n    do\\n        swapped := false\\n        for each i in 0 to length(A) \\u2212 1 do:\\n            if A[i] > A[i + 1] then // test whether the two elements are in the wrong order\\n                swap(A[i], A[i + 1]) // let the two elements change places\\n                swapped := true\\n            end if\\n        end for\\n        if not swapped then\\n            // we can exit the outer loop here if no swaps occurred.\\n            break do-while loop\\n        end if\\n        swapped := false\\n        for each i in length(A) \\u2212 1 to 0 do:\\n            if A[i] > A[i + 1] then\\n                swap(A[i], A[i + 1])\\n                swapped := true\\n            end if\\n        end for\\n    while swapped // if no elements have been swapped, then the list is sorted\\nend procedure\\n\\nThe first rightward pass will shift the largest element to its correct place at the end, and the following leftward pass will shift the smallest element to its correct place at the beginning. The second complete pass will shift the second largest and second smallest elements to their correct places, and so on. After i passes, the first i and the last i elements in the list are in their correct positions, and do not need to be checked. By shortening the part of the list that is sorted each time, the number of operations can be halved (see bubble sort).\\nThis is an example of the algorithm in MATLAB/OCTAVE with the optimization of remembering the last swap index and updating the bounds.\\n\\nDifferences from bubble sort\\nCocktail shaker sort is a slight variation of bubble sort. It differs in that instead of repeatedly passing through the list from bottom to top, it passes alternately from bottom to top and then from top to bottom. It can achieve slightly better performance than a standard bubble sort. The reason for this is that bubble sort only passes through the list in one direction and therefore can only move items backward one step each iteration.\\nAn example of a list that proves this point is the list (2,3,4,5,1), which would only need to go through one pass of cocktail sort to become sorted, but if using an ascending bubble sort would take four passes. However one cocktail sort pass should be counted as two bubble sort passes. Typically cocktail sort is less than two times faster than bubble sort.\\nAnother optimization can be that the algorithm remembers where the last actual swap has been done. In the next iteration, there will be no swaps beyond this limit and the algorithm has shorter passes. As the cocktail shaker sort goes bidirectionally, the range of possible swaps, which is the range to be tested, will reduce per pass, thus reducing the overall running time slightly.\\n\\nComplexity\\nThe complexity of the cocktail shaker sort in big O notation is \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   for both the worst case and the average case, but it becomes closer to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   if the list is mostly ordered before applying the sorting algorithm. For example, if every element is at a position that differs by at most k (k \\u2265 1) from the position it is going to end up in, the complexity of cocktail shaker sort becomes \\n  \\n    \\n      \\n        O\\n        (\\n        k\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle O(kn).}\\n  \\nThe cocktail shaker sort is also briefly discussed in the book The Art of Computer Programming, along with similar refinements of bubble sort. In conclusion, Knuth states about bubble sort and its improvements:\\n\\nBut none of these refinements leads to an algorithm better than straight insertion [that is, insertion sort]; and we already know that straight insertion isn't suitable for large N. [...] In short, the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems.\\n\\nReferences\\nSources\\nHartenstein, R. (July 2010). \\\"A new World Model of Computing\\\" (PDF). The Grand Challenge to Reinvent Computing. Belo Horizonte, Brazil: CSBC. Archived from the original (PDF) on 2013-08-07. Retrieved 2011-01-14.\\n\\nExternal links\\n\\nInteractive demo of cocktail sort\\nJava source code and an animated demo of cocktail sort (called bi-directional bubble sort) and several other algorithms\\n\\\".NET Implementation of cocktail sort and several other algorithms\\\". Archived from the original on 2012-02-12.\"}, {\"Comb sort\": \"Comb sort is a relatively simple sorting algorithm originally designed by W\\u0142odzimierz Dobosiewicz and Artur Borowy in 1980, later rediscovered (and given the name \\\"Combsort\\\") by Stephen Lacey and Richard Box in 1991. Comb sort improves on bubble sort in the same way that Shellsort improves on insertion sort.nist.gov's \\\"diminishing increment sort\\\" definition mentions the term 'comb sort' as visualizing iterative passes of the data, \\\"where the teeth of a comb touch;\\\" the former term is linked to Don Knuth.\\n\\nAlgorithm\\nThe basic idea is to eliminate turtles, or small values near the end of the list, since in a bubble sort these slow the sorting down tremendously. Rabbits, large values around the beginning of the list, do not pose a problem in bubble sort.\\nIn bubble sort, when any two elements are compared, they always have a gap (distance from each other) of 1. The basic idea of comb sort is that the gap can be much more than 1. The inner loop of bubble sort, which does the actual swap, is modified such that the gap between swapped elements goes down (for each iteration of outer loop) in steps of a \\\"shrink factor\\\" k: [n/k, n/k2, n/k3, ..., 1].\\nThe gap starts out as the length of the list n being sorted divided by the shrink factor k (generally 1.3; see below) and one pass of the aforementioned modified bubble sort is applied with that gap. Then the gap is divided by the shrink factor again, the list is sorted with this new gap, and the process repeats until the gap is 1. At this point, comb sort continues using a gap of 1 until the list is fully sorted. The final stage of the sort is thus equivalent to a bubble sort, but by this time most turtles have been dealt with, so a bubble sort will be efficient.\\nThe shrink factor has a great effect on the efficiency of comb sort. k = 1.3 has been suggested as an ideal shrink factor by the authors of the original article after empirical testing on over 200,000 random lists. A value too small slows the algorithm down by making unnecessarily many comparisons, whereas a value too large fails to effectively deal with turtles, making it require many passes with 1 gap size.\\nThe pattern of repeated sorting passes with decreasing gaps is similar to Shellsort, but in Shellsort the array is sorted completely each pass before going on to the next-smallest gap.  Comb sort's passes do not completely sort the elements.  This is the reason that Shellsort gap sequences have a larger optimal shrink factor of about 2.2.\\n\\nPseudocode\\nfunction combsort(array input) is\\n\\n    gap := input.size // Initialize gap size\\n    shrink := 1.3 // Set the gap shrink factor\\n    sorted := false\\n\\n    loop while sorted = false\\n        // Update the gap value for a next comb\\n        gap := floor(gap / shrink)\\n        if gap \\u2264 1 then\\n            gap := 1\\n            sorted := true // If there are no swaps this pass, we are done\\n        end if\\n\\n        // A single \\\"comb\\\" over the input list\\n        i := 0\\n        loop while i + gap < input.size // See Shell sort for a similar idea\\n            if input[i] > input[i+gap] then\\n                swap(input[i], input[i+gap])\\n                sorted := false\\n                // If this assignment never happens within the loop,\\n                // then there have been no swaps and the list is sorted.\\n             end if\\n    \\n             i := i + 1\\n         end loop\\n     end loop\\nend function\\n\\nPython code\\nPlus, two quick Python implementations: one works on the list (or array, or other mutable type where the operations used on it make sense to the language) in-place, the other makes a list with the same values as the given data and returns that after sorting it (similar to the builtin sorted function).\\n\\nSee also\\n\\nBubble sort, a generally slower algorithm, is the basis of comb sort.\\nCocktail sort, or bidirectional bubble sort, is a variation of bubble sort that also addresses the problem of turtles, albeit less effectively.\\n\\n\\n== References ==\"}, {\"Comparison sort\": \"A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a \\\"less than or equal to\\\" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list. The only requirement is that the operator forms a total preorder over the data, with:\\n\\nif a \\u2264 b and b \\u2264 c then a \\u2264 c (transitivity)\\nfor all a and b, a \\u2264 b or b \\u2264 a (connexity).It is possible that both a \\u2264 b and b \\u2264 a; in this case either may come first in the sorted list. In a stable sort, the input order determines the sorted order in this case.\\nA metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a balance scale. Their goal is to line up the weights in order by their weight without any information except that obtained by placing two weights on the scale and seeing which one is heavier (or if they weigh the same).\\n\\nExamples\\nSome of the most well-known comparison sorts include:\\n\\nQuicksort\\nHeapsort\\nShellsort\\nMerge sort\\nIntrosort\\nInsertion sort\\nSelection sort\\nBubble sort\\nOdd\\u2013even sort\\nCocktail shaker sort\\nCycle sort\\nMerge-insertion sort\\nSmoothsort\\nTimsort\\nBlock sort\\n\\nPerformance limits and advantages of different sorting techniques\\nThere are fundamental limits on the performance of comparison sorts. A comparison sort must have an average-case lower bound of \\u03a9(n log n) comparison operations, which is known as linearithmic time. This is a consequence of the limited information available through comparisons alone \\u2014 or, to put it differently, of the vague algebraic structure of totally ordered sets. In this sense, mergesort, heapsort, and introsort are asymptotically optimal in terms of the number of comparisons they must perform, although this metric neglects other operations. Non-comparison sorts (such as the examples discussed below) can achieve O(n) performance by using operations other than comparisons, allowing them to sidestep this lower bound (assuming elements are constant-sized).\\nComparison sorts may run faster on some lists; many adaptive sorts such as insertion sort run in O(n) time on an already-sorted or nearly-sorted list. The \\u03a9(n log n) lower bound applies only to the case in which the input list can be in any possible order.\\nReal-world measures of sorting speed may need to take into account the ability of some algorithms to optimally use relatively fast cached computer memory, or the application may benefit from sorting methods where sorted data begins to appear to the user quickly (and then user's speed of reading will be the limiting factor) as opposed to sorting methods where no output is available until the whole list is sorted.\\nDespite these limitations, comparison sorts offer the notable practical advantage that control over the comparison function allows sorting of many different datatypes and fine control over how the list is sorted. For example, reversing the result of the comparison function allows the list to be sorted in reverse; and one can sort a list of tuples in lexicographic order by just creating a comparison function that compares each part in sequence:\\n\\nfunction tupleCompare((lefta, leftb, leftc), (righta, rightb, rightc))\\n    if lefta \\u2260 righta\\n        return compare(lefta, righta)\\n    else if leftb \\u2260 rightb\\n        return compare(leftb, rightb)\\n    else\\n        return compare(leftc, rightc)\\n\\nComparison sorts generally adapt more easily to complex orders such as the order of floating-point numbers. Additionally, once a comparison function is written, any comparison sort can be used without modification; non-comparison sorts typically require specialized versions for each datatype.\\nThis flexibility, together with the efficiency of the above comparison sorting algorithms on modern computers, has led to widespread preference for comparison sorts in most practical work.\\n\\nAlternatives\\nSome sorting problems admit a strictly faster solution than the \\u03a9(n log n) bound for comparison sorting by using non-comparison sorts; an example is integer sorting, where all keys are integers. When the keys form a small (compared to n) range, counting sort is an example algorithm that runs in linear time. Other integer sorting algorithms, such as radix sort, are not asymptotically faster than comparison sorting, but can be faster in practice.\\nThe problem of sorting pairs of numbers by their sum is not subject to the \\u03a9(n\\u00b2 log n) bound either (the square resulting from the pairing up); the best known algorithm still takes O(n\\u00b2 log n) time, but only O(n\\u00b2) comparisons.\\n\\nNumber of comparisons required to sort a list\\nThe number of comparisons that a comparison sort algorithm requires increases in proportion to \\n  \\n    \\n      \\n        n\\n        log\\n        \\u2061\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle n\\\\log(n)}\\n  , where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of elements to sort.  This bound is asymptotically tight.\\nGiven a list of distinct numbers (we can assume this because this is a worst-case analysis), there are \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   factorial permutations exactly one of which is the list in sorted order. The sort algorithm must gain enough information from the comparisons to identify the correct permutation. If the algorithm always completes after at most \\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle f(n)}\\n   steps, it cannot distinguish more than \\n  \\n    \\n      \\n        \\n          2\\n          \\n            f\\n            (\\n            n\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2^{f(n)}}\\n   cases because the keys are distinct and each comparison has only two possible outcomes. Therefore,\\n\\n  \\n    \\n      \\n        \\n          2\\n          \\n            f\\n            (\\n            n\\n            )\\n          \\n        \\n        \\u2265\\n        n\\n        !\\n      \\n    \\n    {\\\\displaystyle 2^{f(n)}\\\\geq n!}\\n  , or equivalently \\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        )\\n        \\u2265\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        !\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle f(n)\\\\geq \\\\log _{2}(n!).}\\n  By looking at the first \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle n/2}\\n   factors of \\n  \\n    \\n      \\n        n\\n        !\\n        =\\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\u22ef\\n        1\\n      \\n    \\n    {\\\\displaystyle n!=n(n-1)\\\\cdots 1}\\n  , we obtain\\n\\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        !\\n        )\\n        \\u2265\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        \\n          (\\n          \\n            \\n              (\\n              \\n                \\n                  n\\n                  2\\n                \\n              \\n              )\\n            \\n            \\n              \\n                n\\n                2\\n              \\n            \\n          \\n          )\\n        \\n        =\\n        \\n          \\n            n\\n            2\\n          \\n        \\n        \\n          \\n            \\n              log\\n              \\u2061\\n              n\\n            \\n            \\n              log\\n              \\u2061\\n              2\\n            \\n          \\n        \\n        \\u2212\\n        \\n          \\n            n\\n            2\\n          \\n        \\n        =\\n        \\u0398\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}(n!)\\\\geq \\\\log _{2}\\\\left(\\\\left({\\\\frac {n}{2}}\\\\right)^{\\\\frac {n}{2}}\\\\right)={\\\\frac {n}{2}}{\\\\frac {\\\\log n}{\\\\log 2}}-{\\\\frac {n}{2}}=\\\\Theta (n\\\\log n).}\\n  \\n\\n  \\n    \\n      \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        n\\n        !\\n        )\\n        =\\n        \\u03a9\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\log _{2}(n!)=\\\\Omega (n\\\\log n).}\\n  This provides the lower-bound part of the claim. A more precise bound can be given via Stirling's approximation. An upper bound of the same form, with the same leading term as the bound obtained from Stirling's approximation, follows from the existence of the algorithms that attain this bound in the worst case, like merge sort.\\nThe above argument provides an absolute, rather than only asymptotic lower bound on the number of comparisons, namely \\n  \\n    \\n      \\n        \\n          \\u2308\\n          \\n            \\n              log\\n              \\n                2\\n              \\n            \\n            \\u2061\\n            (\\n            n\\n            !\\n            )\\n          \\n          \\u2309\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left\\\\lceil \\\\log _{2}(n!)\\\\right\\\\rceil }\\n   comparisons. This lower bound is fairly good (it can be approached within a linear tolerance by a simple merge sort), but it is known to be inexact. For example, \\n  \\n    \\n      \\n        \\n          \\u2308\\n          \\n            \\n              log\\n              \\n                2\\n              \\n            \\n            \\u2061\\n            (\\n            13\\n            !\\n            )\\n          \\n          \\u2309\\n        \\n        =\\n        33\\n      \\n    \\n    {\\\\displaystyle \\\\left\\\\lceil \\\\log _{2}(13!)\\\\right\\\\rceil =33}\\n  , but the minimal number of comparisons to sort 13 elements has been proved to be 34.\\nDetermining the exact number of comparisons needed to sort a given number of entries is a computationally hard problem even for small n, and no simple formula for the solution is known. For some of the few concrete values that have been computed, see OEIS: A036604.\\n\\nLower bound for the average number of comparisons\\nA similar bound applies to the average number of comparisons. Assuming that\\n\\nall keys are distinct, i.e. every comparison will give either a>b or a<b, and\\nthe input is a random permutation, chosen uniformly from the set of all possible permutations of n elements,it is impossible to determine which order the input is in with fewer than log2(n!) comparisons on average.\\nThis can be most easily seen using concepts from information theory. The Shannon entropy of such a random permutation is log2(n!) bits. Since a comparison can give only two results, the maximum amount of information it provides is 1 bit. Therefore, after k comparisons the remaining entropy of the permutation, given the results of those comparisons, is at least log2(n!) \\u2212 k bits on average. To perform the sort, complete information is needed, so the remaining entropy must be 0. It follows that k must be at least log2(n!) on average. \\nThe lower bound derived via information theory is phrased as 'information-theoretic lower bound'. Information-theoretic lower bound is correct but is not necessarily the strongest lower bound. And in some cases, the information-theoretic lower bound of a problem may even be far from the true lower bound. For example, the information-theoretic lower bound of selection is \\n  \\n    \\n      \\n        \\n          \\u2308\\n          \\n            \\n              log\\n              \\n                2\\n              \\n            \\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          \\u2309\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left\\\\lceil \\\\log _{2}(n)\\\\right\\\\rceil }\\n   whereas \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   comparisons are needed by an adversarial argument. The interplay between information-theoretic lower bound and the true lower bound are much like a real-valued function lower-bounding an integer function. However, this is not exactly correct when the average case is considered.\\nTo unearth what happens while analyzing the average case, the key is that what does 'average' refer to? Averaging over what? With some knowledge of information theory, the information-theoretic lower bound averages over the set of all permutations as a whole. But any computer algorithms (under what are believed currently) must treat each permutation as an individual instance of the problem. Hence, the average lower bound we're searching for is averaged over all individual cases.\\nTo search for the lower bound relating to the non-achievability of computers, we adopt the Decision tree model. Let's rephrase a bit of what our objective is. In the Decision tree model, the lower bound to be shown is the lower bound of the average length of root-to-leaf paths of an \\n  \\n    \\n      \\n        n\\n        !\\n      \\n    \\n    {\\\\displaystyle n!}\\n  -leaf binary tree (in which each leaf corresponds to a permutation). It would be convinced to say that a balanced full binary tree achieves the minimum of the average length. With some careful calculations, for a balanced full binary tree with \\n  \\n    \\n      \\n        n\\n        !\\n      \\n    \\n    {\\\\displaystyle n!}\\n   leaves, the average length of root-to-leaf paths is given by \\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              (\\n              2\\n              n\\n              !\\n              \\u2212\\n              \\n                2\\n                \\n                  \\u230a\\n                  \\n                    log\\n                    \\n                      2\\n                    \\n                  \\n                  \\u2061\\n                  n\\n                  !\\n                  \\u230b\\n                  +\\n                  1\\n                \\n              \\n              )\\n              \\u22c5\\n              \\u2308\\n              \\n                log\\n                \\n                  2\\n                \\n              \\n              \\u2061\\n              n\\n              !\\n              \\u2309\\n              +\\n              (\\n              \\n                2\\n                \\n                  \\u230a\\n                  \\n                    log\\n                    \\n                      2\\n                    \\n                  \\n                  \\u2061\\n                  n\\n                  !\\n                  \\u230b\\n                  +\\n                  1\\n                \\n              \\n              \\u2212\\n              n\\n              !\\n              )\\n              \\u22c5\\n              \\u230a\\n              \\n                log\\n                \\n                  2\\n                \\n              \\n              \\u2061\\n              n\\n              !\\n              \\u230b\\n            \\n            \\n              n\\n              !\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {(2n!-2^{\\\\lfloor \\\\log _{2}n!\\\\rfloor +1})\\\\cdot \\\\lceil \\\\log _{2}n!\\\\rceil +(2^{\\\\lfloor \\\\log _{2}n!\\\\rfloor +1}-n!)\\\\cdot \\\\lfloor \\\\log _{2}n!\\\\rfloor }{n!}}}\\n  For example, for n = 3, the information-theoretic lower bound for the average case is approximately 2.58, while the average lower bound derived via Decision tree model is 8/3, approximately 2.67.\\nIn the case that multiple items may have the same key, there is no obvious statistical interpretation for the term \\\"average case\\\", so an argument like the above cannot be applied without making specific assumptions about the distribution of keys.\\n\\nn log n maximum number of comparisons for array-size in format 2^k\\nCan easy compute for real algorithm sorted-list-merging (array are sorted n-blocks with size 1, merge to 1-1 to 2, merge 2-2 to 4...).\\n\\n(1) = = = = = = = =\\n\\n(2) =   =   =   =     // max 1 compares (size1+size2-1), 4x repeats to concat 8 arrays with size 1 and 1\\n   === === === ===\\n\\n(3)   =       =       // max 7 compares, 2x repeats to concat 4 arrays with size 2 and 2\\n     ===     ===  \\n    =====   ===== \\n   ======= =======\\n\\n(4)                   // max 15 compares, 1x repeats to concat 2 arrays with size 4 and 4\\n\\nFormula extraction:\\nn = 256 = 2^8 (array size in format 2^k, for simplify)\\nOn = (n-1) + 2(n/2-1) + 4(n/4-1) + 8(n/8-1) + 16(n/16-1) + 32(n/32-1) + 64(n/64-1) + 128(n/128-1)\\nOn = (n-1) + (n-2) + (n-4) + (n-8) + (n-16) + (n-32) + (n-64) + (n-128)\\nOn = n+n+n+n+n+n+n+n - (1+2+4+8+16+32+64+128)   | 1+2+4... = formula for geometric sequence Sn = a1 * (q^i - 1) / (n - 1), n is number of items, a1 is first item\\nOn = 8*n - 1 * (2^8 - 1) / (2 - 1)\\nOn = 8*n - (2^8 - 1)   | 2^8 = n\\nOn = 8*n - (n - 1)\\nOn = (8-1)*n + 1   | 8 = ln(n)/ln(2) = ln(256)/ln(2)\\nOn = (ln(n)/ln(2) - 1) * n + 1\\n\\nExample:\\nn = 2^4 = 16, On ~= 3*n\\nn = 2^8 = 256, On ~= 7*n\\nn = 2^10 = 1.024, On ~= 9*n\\nn = 2^20 = 1.048.576, On ~= 19*n\\n\\nSorting a pre-sorted list\\nIf a list is already close to sorted, according to some measure of sortedness, the number of comparisons required to sort it can be smaller. An adaptive sort takes advantage of this \\\"presortedness\\\" and runs more quickly on nearly-sorted inputs, often while still maintaining an \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   worst case time bound. An example is adaptive heap sort, a sorting algorithm based on Cartesian trees. It takes time \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log k)}\\n  , where \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   is the average, over all values \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   in the sequence, of the number of times the sequence jumps from below \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   to above \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   or vice versa.\\n\\nNotes\\nReferences\\nDonald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Second Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Section 5.3.1: Minimum-Comparison Sorting, pp. 180\\u2013197.\"}, {\"Cubesort\": \"Cubesort is a parallel sorting algorithm that builds a self-balancing multi-dimensional array from the keys to be sorted. As the axes are of similar length the structure resembles a cube. After each key is inserted the cube can be rapidly converted to an array.A cubesort implementation written in C was published in 2014.\\n\\nOperation\\nCubesort's algorithm uses a specialized binary search on each axis to find the location to insert an element. When an axis grows too large it is split. Locality of reference is optimal as only four binary searches are performed on small arrays for each insertion. By using many small dynamic arrays the high cost for insertion on single large arrays is avoided.\\n\\nReferences\\nExternal links\\nCubesort description and implementation in C\\nNiedermeier, Rolf (1996). \\\"Recursively divisible problems\\\". Algorithms and Computation. Lecture Notes in Computer Science. Vol. 1178. Springer Berlin Heidelberg. pp. 187\\u2013188. doi:10.1007/BFb0009494. eISSN 1611-3349. ISBN 978-3-540-62048-8. ISSN 0302-9743. (passing mention)\"}, {\"Cycle sort\": \"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result.\\nUnlike nearly every other sort, items are never written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort.\\nMinimizing the number of writes is useful when making writes to some huge data set is very expensive, such as with EEPROMs like Flash memory where each write reduces the lifespan of the memory.\\n\\nAlgorithm\\nTo illustrate the idea of cycle sort, consider a list with distinct elements. Given an element \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  , we can find the index at which it will occur in the sorted list by simply counting the number of elements in the entire list that are smaller than \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  . Now\\n\\nIf the element is already at the correct position, do nothing.\\nIf it is not, we will write it to its intended position. That position is inhabited by a different element \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  , which we then have to move to its correct position. This process of displacing elements to their correct positions continues until an element is moved to the original position of \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  . This completes a cycle.\\nRepeating this process for every element sorts the list, with a single writing operation if and only if an element is not already at its correct position. While computing the correct positions takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time for every single element, thus resulting in a quadratic time algorithm, the number of writing operations is minimized.\\n\\nImplementation\\nTo create a working implementation from the above outline, two issues need to be addressed:\\n\\nWhen computing the correct positions, we have to make sure not to double-count the first element of the cycle.\\nIf there are duplicate elements present, when we try to move an element \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   to its correct position, that position might already be inhabited by an \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  . Simply swapping these would cause the algorithm to cycle indefinitely. Instead, we have to insert the element after any of its duplicates.The following Python implementation performs cycle sort on an array, counting the number of writes to that array that were needed to sort it.\\nPython\\n\\nThe next implementation written in C++ simply performs cyclic array sorting.\\n\\nSituation-specific optimizations\\nWhen the array contains only duplicates of a relatively small number of items, a constant-time perfect hash function can greatly speed up finding where to put an item1, turning the sort from \\u0398(n2) time to \\u0398(n + k) time, where k is the total number of hashes. The array ends up sorted in the order of the hashes, so choosing a hash function that gives you the right ordering is important.\\nBefore the sort, create a histogram, sorted by hash, counting the number of occurrences of each hash in the array. Then create a table with the cumulative sum of each entry in the histogram. The cumulative sum table will then contain the position in the array of each element. The proper place of elements can then be found by a constant-time hashing and cumulative sum table lookup rather than a linear search.\\n\\nReferences\\nExternal links\\n^  \\\"Cycle-Sort: A Linear Sorting Method\\\", The Computer Journal (1990) 33 (4): 365-367.\\n\\nOriginal source of unrestricted variant\\nCyclesort - a curious little sorting algorithm\"}, {\"Funnelsort\": \"Funnelsort is a comparison-based sorting algorithm. It is similar to mergesort, but it is a cache-oblivious algorithm, designed for a setting where the number of elements to sort is too large to fit in a cache where operations are done. It was introduced by Matteo Frigo, Charles Leiserson, Harald Prokop, and Sridhar Ramachandran in 1999 in the context of the cache oblivious model.\\n\\nMathematical properties\\nIn the external memory model, the number of memory transfers it needs to perform a sort of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   items on a machine with cache of size \\n  \\n    \\n      \\n        Z\\n      \\n    \\n    {\\\\displaystyle Z}\\n   and cache lines of length \\n  \\n    \\n      \\n        L\\n      \\n    \\n    {\\\\displaystyle L}\\n   is \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            \\n              \\n                \\n                  N\\n                  L\\n                \\n              \\n            \\n            \\n              log\\n              \\n                Z\\n              \\n            \\n            \\u2061\\n            N\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left({\\\\tfrac {N}{L}}\\\\log _{Z}N\\\\right)}\\n  , under the tall cache assumption that \\n  \\n    \\n      \\n        Z\\n        =\\n        \\u03a9\\n        (\\n        \\n          L\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle Z=\\\\Omega (L^{2})}\\n  . This number of memory transfers has been shown to be asymptotically optimal for comparison sorts. Funnelsort also achieves the asymptotically optimal runtime complexity of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        N\\n        log\\n        \\u2061\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (N\\\\log N)}\\n  .\\n\\nAlgorithm\\nBasic overview\\nFunnelsort operates on a contiguous array of \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n   elements. To sort the elements, it performs the following:\\n\\nSplit the input into \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n   arrays of size \\n  \\n    \\n      \\n        \\n          N\\n          \\n            2\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{2/3}}\\n  , and sort the arrays recursively.\\nMerge the \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n   sorted sequences using a \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n  -merger. (This process will be described in more detail.)Funnelsort is similar to merge sort in that some number of subarrays are recursively sorted, after which a merging step combines the subarrays into one sorted array. Merging is performed by a device called a k-merger, which is described in the section below.\\n\\nk-mergers\\nA k-merger takes \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   sorted sequences. Upon one invocation of a k-merger, it outputs the first \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3}}\\n   elements of the sorted sequence obtained by merging the k input sequences.\\nAt the top level, funnelsort uses a \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n  -merger on \\n  \\n    \\n      \\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{1/3}}\\n   sequences of length \\n  \\n    \\n      \\n        \\n          N\\n          \\n            2\\n            \\n              /\\n            \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle N^{2/3}}\\n  , and invokes this merger once.\\nThe k-merger is built recursively out of \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n  -mergers. It consists of \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   input \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n  -mergers \\n  \\n    \\n      \\n        \\n          I\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          I\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          I\\n          \\n            \\n              k\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle I_{1},I_{2},\\\\ldots ,I_{\\\\sqrt {k}}}\\n  , and a single output \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n  -merger \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n  .\\nThe k inputs are separated into \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   sets of \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   inputs each. Each of these sets is an input to one of the input mergers. The output of each input merger is connected to a buffer, a FIFO queue that can hold \\n  \\n    \\n      \\n        2\\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2k^{3/2}}\\n   elements. The buffers are implemented as circular queues.\\nThe outputs of the \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   buffers are connected to the inputs of the output merger \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n  . Finally, the output of \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n   is the output of the entire k-merger.\\nIn this construction, any input merger only outputs \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   items at once, but the buffer it outputs to has double the space. This is done so that an input merger can be called only when its buffer does not have enough items, but that when it is called, it outputs a lot of items at once (namely, \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   of them).\\nA k-merger works recursively in the following way. To output \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3}}\\n   elements, it recursively invokes its output merger \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   times. However, before it makes a call to \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n  , it checks all of its buffers, filling each of them that are less than half full. To fill the i-th buffer, it recursively invokes the corresponding input merger \\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle I_{i}}\\n   once. If this cannot be done (due to the merger running out of inputs), this step is skipped. Since this call outputs \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   elements, the buffer contains at least \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   elements. At the end of all these operations, the k-merger has output the first \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3}}\\n   of its input elements, in sorted order.\\n\\nAnalysis\\nMost of the analysis of this algorithm revolves around analyzing the space and cache miss complexity of the k-merger.\\nThe first important bound is that a k-merger can be fit in \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          k\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(k^{2})}\\n   space. To see this, we let \\n  \\n    \\n      \\n        S\\n        (\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle S(k)}\\n   denote the space needed for a k-merger. To fit the \\n  \\n    \\n      \\n        \\n          k\\n          \\n            1\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{1/2}}\\n   buffers of size \\n  \\n    \\n      \\n        2\\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2k^{3/2}}\\n   takes \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          k\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(k^{2})}\\n   space. To fit the \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}+1}\\n   smaller buffers takes \\n  \\n    \\n      \\n        (\\n        \\n          \\n            k\\n          \\n        \\n        +\\n        1\\n        )\\n        S\\n        (\\n        \\n          \\n            k\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle ({\\\\sqrt {k}}+1)S({\\\\sqrt {k}})}\\n   space. Thus, the space satisfies the recurrence \\n  \\n    \\n      \\n        S\\n        (\\n        k\\n        )\\n        =\\n        (\\n        \\n          \\n            k\\n          \\n        \\n        +\\n        1\\n        )\\n        S\\n        (\\n        \\n          \\n            k\\n          \\n        \\n        )\\n        +\\n        O\\n        (\\n        \\n          k\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle S(k)=({\\\\sqrt {k}}+1)S({\\\\sqrt {k}})+O(k^{2})}\\n  . This recurrence has solution \\n  \\n    \\n      \\n        S\\n        (\\n        k\\n        )\\n        =\\n        O\\n        (\\n        \\n          k\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle S(k)=O(k^{2})}\\n  .\\nIt follows that there is a positive constant \\n  \\n    \\n      \\n        \\u03b1\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   such that a problem of size at most \\n  \\n    \\n      \\n        \\u03b1\\n        \\n          \\n            Z\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\alpha {\\\\sqrt {Z}}}\\n   fits entirely in cache, meaning that it incurs no additional cache misses.\\nLetting \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle Q_{M}(k)}\\n   denote the number of cache misses incurred by a call to a k-merger, one can show that \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        k\\n        )\\n        =\\n        O\\n        (\\n        (\\n        \\n          k\\n          \\n            3\\n          \\n        \\n        \\n          log\\n          \\n            Z\\n          \\n        \\n        \\u2061\\n        k\\n        )\\n        \\n          /\\n        \\n        L\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle Q_{M}(k)=O((k^{3}\\\\log _{Z}k)/L).}\\n   This is done by an induction argument. It has \\n  \\n    \\n      \\n        k\\n        \\u2264\\n        \\u03b1\\n        \\n          \\n            Z\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k\\\\leq \\\\alpha {\\\\sqrt {Z}}}\\n   as a base case. For larger k, we can bound the number of times a \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n  -merger is called. The output merger is called exactly \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   times. The total number of calls on input mergers is at most \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        2\\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}+2{\\\\sqrt {k}}}\\n  . This gives a total bound of \\n  \\n    \\n      \\n        2\\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        2\\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 2k^{3/2}+2{\\\\sqrt {k}}}\\n   recursive calls. In addition, the algorithm checks every buffer to see if needs to be filled. This is done on \\n  \\n    \\n      \\n        \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {k}}}\\n   buffers every step for \\n  \\n    \\n      \\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{3/2}}\\n   steps, leading to a max of \\n  \\n    \\n      \\n        \\n          k\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{2}}\\n   cache misses for all the checks.\\nThis leads to the recurrence \\n  \\n    \\n      \\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        k\\n        )\\n        \\u2264\\n        (\\n        2\\n        \\n          k\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        2\\n        \\n          \\n            k\\n          \\n        \\n        )\\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        \\n          \\n            k\\n          \\n        \\n        )\\n        +\\n        \\n          k\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle Q_{M}(k)\\\\leq (2k^{3/2}+2{\\\\sqrt {k}})Q_{M}({\\\\sqrt {k}})+k^{2}}\\n  , which can be shown to have the solution given above.\\nFinally, the total cache misses \\n  \\n    \\n      \\n        Q\\n        (\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle Q(N)}\\n   for the entire sort can be analyzed. It satisfies the recurrence \\n  \\n    \\n      \\n        Q\\n        (\\n        N\\n        )\\n        =\\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        Q\\n        (\\n        \\n          N\\n          \\n            2\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        )\\n        +\\n        \\n          Q\\n          \\n            M\\n          \\n        \\n        (\\n        \\n          N\\n          \\n            1\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle Q(N)=N^{1/3}Q(N^{2/3})+Q_{M}(N^{1/3}).}\\n   This can be shown to have solution \\n  \\n    \\n      \\n        Q\\n        (\\n        N\\n        )\\n        =\\n        O\\n        (\\n        (\\n        N\\n        \\n          /\\n        \\n        L\\n        )\\n        \\n          log\\n          \\n            Z\\n          \\n        \\n        \\u2061\\n        N\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle Q(N)=O((N/L)\\\\log _{Z}N).}\\n\\nLazy funnelsort\\nLazy funnelsort is a modification of the funnelsort, introduced by Gerth St\\u00f8lting Brodal and Rolf Fagerberg in 2002.\\nThe modification is that when a merger is invoked, it does not have to fill each of its buffers. Instead, it lazily fills a buffer only when it is empty. This modification has the same asymptotic runtime and memory transfers as the original funnelsort, but has applications in cache-oblivious algorithms for problems in computational geometry in a method known as distribution sweeping.\\n\\nSee also\\nCache-oblivious algorithm\\nCache-oblivious distribution sort\\nExternal sorting\\n\\n\\n== References ==\"}, {\"Gnome sort\": \"Gnome sort (nicknamed stupid sort) is a variation of the insertion sort sorting algorithm that does not use nested loops. Gnome sort was originally proposed by Iranian computer scientist Hamid Sarbazi-Azad (professor of Computer Science and Engineering at Sharif University of Technology) in 2000. The sort was first called stupid sort (not to be confused with bogosort), and then later described by Dick Grune and named gnome sort.Gnome sort performs at least as many comparisons as insertion sort and has the same asymptotic run time characteristics. Gnome sort works by building a sorted list one element at a time, getting each item to the proper place in a series of swaps. The average running time is O(n2) but tends towards O(n) if the list is initially almost sorted.Dick Grune described the sorting method with the following story:\\n\\nGnome Sort is based on the technique used by the standard Dutch Garden Gnome (Du.: tuinkabouter). \\nHere is how a garden gnome sorts a line of flower pots. \\nBasically, he looks at the flower pot next to him and the previous one; if they are in the right order he steps one pot forward, otherwise, he swaps them and steps one pot backward. \\nBoundary conditions: if there is no previous pot, he steps forwards; if there is no pot next to him, he is done.\\n\\nPseudocode\\nHere is pseudocode for the gnome sort using a zero-based array:\\n\\nExample\\nGiven an unsorted array, a = [5, 3, 2, 4], the gnome sort takes the following steps during the while loop. The current position is highlighted in bold and indicated as a value of the variable pos.\\n\\nNotes\\nReferences\\nExternal links\\n\\nGnome sort\"}, {\"Heapsort\": \"In computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like selection sort, heapsort divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step.Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime (and as such is used by Introsort as a fallback should it detect that quicksort is becoming degenerate). Heapsort is an in-place algorithm, but it is not a stable sort.\\nHeapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, Robert W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.\\n\\nOverview\\nThe heapsort algorithm can be divided into two parts.\\nIn the first step, a heap is built out of the data (see Binary heap \\u00a7 Building a heap). The heap is often placed in an array with the layout of a complete binary tree. The complete binary tree maps the binary tree structure into the array indices; each array index represents a node; the index of the node's parent, left child branch, or right child branch are simple expressions. For a zero-based array, the root node is stored at index 0; if i is the index of the current node, then\\n\\n  iParent(i)     = floor((i-1) / 2) where floor functions map a real number to the largest leading integer.\\n  iLeftChild(i)  = 2*i + 1\\n  iRightChild(i) = 2*i + 2\\n\\nIn the second step, a sorted array is created by repeatedly removing the largest element from the heap (the root of the heap), and inserting it into the array. The heap is updated after each removal to maintain the heap property. Once all objects have been removed from the heap, the result is a sorted array.\\nHeapsort can be performed in place. The array can be split into two parts, the sorted array and the heap. The storage of heaps as arrays is diagrammed here. The heap's invariant is preserved after each extraction, so the only cost is that of extraction.\\n\\nAlgorithm\\nThe heapsort algorithm involves preparing the list by first turning it into a max heap. The algorithm then repeatedly swaps the first value of the list with the last value, decreasing the range of values considered in the heap operation by one, and sifting the new first value into its position in the heap. This repeats until the range of considered values is one value in length.\\nThe steps are:\\n\\nCall the buildMaxHeap() function on the list. Also referred to as heapify(), this builds a heap from a list in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   operations.\\nSwap the first element of the list with the final element. Decrease the considered range of the list by one.\\nCall the siftDown() function on the list to sift the new first element to its appropriate index in the heap.\\nGo to step (2) unless the considered range of the list is one element.The buildMaxHeap() operation is run once, and is O(n) in performance. The siftDown() function is O(log n), and is called n times. Therefore, the performance of this algorithm is O(n + n log n) = O(n log n).\\n\\nPseudocode\\nThe following is a simple way to implement the algorithm in pseudocode. Arrays are zero-based and swap is used to exchange two elements of the array. Movement 'down' means from the root towards the leaves, or from lower indices to higher. Note that during the sort, the largest element is at the root of the heap at a[0], while at the end of the sort, the largest element is in a[end].\\n\\nprocedure heapsort(a, count) is\\n    input: an unordered array a of length count\\n \\n    (Build the heap in array a so that largest value is at the root)\\n    heapify(a, count)\\n\\n    (The following loop maintains the invariants that a[0:end] is a heap and every element\\n     beyond end is greater than everything before it (so a[end:count] is in sorted order))\\n    end \\u2190 count - 1\\n    while end > 0 do\\n        (a[0] is the root and largest value. The swap moves it in front of the sorted elements.)\\n        swap(a[end], a[0])\\n        (the heap size is reduced by one)\\n        end \\u2190 end - 1\\n        (the swap ruined the heap property, so restore it)\\n        siftDown(a, 0, end)\\n\\nThe sorting routine uses two subroutines, heapify and siftDown. The former is the common in-place heap construction routine, while the latter is a common subroutine for implementing heapify.\\n\\n(Put elements of 'a' in heap order, in-place)\\nprocedure heapify(a, count) is\\n    (start is assigned the index in 'a' of the last parent node)\\n    (the last element in a 0-based array is at index count-1; find the parent of that element)\\n    start \\u2190 iParent(count-1)\\n    \\n    while start \\u2265 0 do\\n        (sift down the node at index 'start' to the proper place such that all nodes below\\n         the start index are in heap order)\\n        siftDown(a, start, count - 1)\\n        (go to the next parent node)\\n        start \\u2190 start - 1\\n    (after sifting down the root all nodes/elements are in heap order)\\n\\n(Repair the heap whose root element is at index 'start', assuming the heaps rooted at its children are valid)\\nprocedure siftDown(a, start, end) is\\n    root \\u2190 start\\n\\n    while iLeftChild(root) \\u2264 end do    (While the root has at least one child)\\n        child \\u2190 iLeftChild(root)   (Left child of root)\\n        swap \\u2190 root                (Keeps track of child to swap with)\\n\\n        if a[swap] < a[child] then\\n            swap \\u2190 child\\n        (If there is a right child and that child is greater)\\n        if child+1 \\u2264 end and a[swap] < a[child+1] then\\n            swap \\u2190 child + 1\\n        if swap = root then\\n            (The root holds the largest element. Since we assume the heaps rooted at the\\n             children are valid, this means that we are done.)\\n            return\\n        else\\n            Swap(a[root], a[swap])\\n            root \\u2190 swap          (repeat to continue sifting down the child now)\\n\\nThe heapify procedure can be thought of as building a heap from the bottom up by successively sifting downward to establish the heap property. An alternative version (shown below) that builds the heap top-down and sifts upward may be simpler to understand. This siftUp version can be visualized as starting with an empty heap and successively inserting elements, whereas the siftDown version given above treats the entire input array as a full but \\\"broken\\\" heap and \\\"repairs\\\" it starting from the last non-trivial sub-heap (that is, the last parent node).\\n\\nAlso, the siftDown version of heapify has O(n) time complexity, while the siftUp version given below has O(n log n) time complexity due to its equivalence with inserting each element, one at a time, into an empty heap.\\nThis may seem counter-intuitive since, at a glance, it is apparent that the former only makes half as many calls to its logarithmic-time sifting function as the latter; i.e., they seem to differ only by a constant factor, which never affects asymptotic analysis.\\nTo grasp the intuition behind this difference in complexity, note that the number of swaps that may occur during any one siftUp call increases with the depth of the node on which the call is made. The crux is that there are many (exponentially many) more \\\"deep\\\" nodes than there are \\\"shallow\\\" nodes in a heap, so that siftUp may have its full logarithmic running-time on the approximately linear number of calls made on the nodes at or near the \\\"bottom\\\" of the heap. On the other hand, the number of swaps that may occur during any one siftDown call decreases as the depth of the node on which the call is made increases. Thus, when the siftDown heapify begins and is calling siftDown on the bottom and most numerous node-layers, each sifting call will incur, at most, a number of swaps equal to the \\\"height\\\" (from the bottom of the heap) of the node on which the sifting call is made. In other words, about half the calls to siftDown will have at most only one swap, then about a quarter of the calls will have at most two swaps, etc.\\nThe heapsort algorithm itself has O(n log n) time complexity using either version of heapify.\\n\\n procedure heapify(a,count) is\\n     (end is assigned the index of the first (left) child of the root)\\n     end := 1\\n     \\n     while end < count\\n         (sift up the node at index end to the proper place such that all nodes above\\n          the end index are in heap order)\\n         siftUp(a, 0, end)\\n         end := end + 1\\n     (after sifting up the last node all nodes are in heap order)\\n \\n procedure siftUp(a, start, end) is\\n     input:  start represents the limit of how far up the heap to sift.\\n                   end is the node to sift up.\\n     child := end \\n     while child > start\\n         parent := iParent(child)\\n         if a[parent] < a[child] then (out of max-heap order)\\n             swap(a[parent], a[child])\\n             child := parent (repeat to continue sifting up the parent now)\\n         else\\n             return\\n\\nNote that unlike siftDown approach where, after each swap, you need to call only the siftDown subroutine to fix the broken heap; the siftUp subroutine alone cannot fix the broken heap. The heap needs to be built every time after a swap by calling the heapify procedure since \\\"siftUp\\\" assumes that the element getting swapped ends up in its final place, as opposed to \\\"siftDown\\\" allows for continuous adjustments of items lower in the heap until the invariant is satisfied. The adjusted pseudocode for using siftUp approach is given below.\\n\\n procedure heapsort(a, count) is\\n    input: an unordered array a of length count\\n \\n    (Build the heap in array a so that largest value is at the root)\\n    heapify(a, count)\\n\\n    (The following loop maintains the invariants that a[0:end] is a heap and every element\\n     beyond end is greater than everything before it (so a[end:count] is in sorted order))\\n    end \\u2190 count - 1\\n    while end > 0 do\\n        (a[0] is the root and largest value. The swap moves it in front of the sorted elements.)\\n        swap(a[end], a[0])\\n        (rebuild the heap using siftUp after the swap ruins the heap property)\\n        heapify(a, end)\\n        (reduce the heap size by one)\\n        end \\u2190 end - 1\\n\\nVariations\\nFloyd's heap construction\\nThe most important variation to the basic algorithm, which is included in all practical implementations, is a heap-construction algorithm by Floyd which runs in O(n) time and uses siftdown rather than siftup, avoiding the need to implement siftup at all.\\nRather than starting with a trivial heap and repeatedly adding leaves, Floyd's algorithm starts with the leaves, observing that they are trivial but valid heaps by themselves, and then adds parents. Starting with element n/2 and working backwards, each internal node is made the root of a valid heap by sifting down. The last step is sifting down the first element, after which the entire array obeys the heap property.\\nThe worst-case number of comparisons during the Floyd's heap-construction phase of heapsort is known to be equal to 2n \\u2212 2s2(n) \\u2212 e2(n), where s2(n) is the number of 1 bits in the binary representation of n and e2(n) is number of trailing 0 bits.The standard implementation of Floyd's heap-construction algorithm causes a large number of cache misses once the size of the data exceeds that of the CPU cache.:\\u200a87\\u200a Much better performance on large data sets can be obtained by merging in depth-first order, combining subheaps as soon as possible, rather than combining all subheaps on one level before proceeding to the one above.\\n\\nBottom-up heapsort\\nBottom-up heapsort is a variant which reduces the number of comparisons required by a significant factor. While ordinary heapsort requires 2n log2 n + O(n) comparisons worst-case and on average, the bottom-up variant requires n log2n + O(1) comparisons on average, and 1.5n log2n + O(n) in the worst case.If comparisons are cheap (e.g. integer keys) then the difference is unimportant, as top-down heapsort compares values that have already been loaded from memory. If, however, comparisons require a function call or other complex logic, then bottom-up heapsort is advantageous.\\nThis is accomplished by improving the siftDown procedure. The change improves the linear-time heap-building phase somewhat, but is more significant in the second phase. Like ordinary heapsort, each iteration of the second phase extracts the top of the heap, a[0], and fills the gap it leaves with a[end], then sifts this latter element down the heap. But this element comes from the lowest level of the heap, meaning it is one of the smallest elements in the heap, so the sift-down will likely take many steps to move it back down. In ordinary heapsort, each step of the sift-down requires two comparisons, to find the minimum of three elements: the new node and its two children.\\nBottom-up heapsort instead finds the path of largest children to the leaf level of the tree (as if it were inserting \\u2212\\u221e) using only one comparison per level. Put another way, it finds a leaf which has the property that it and all of its ancestors are greater than or equal to their siblings. (In the absence of equal keys, this leaf is unique.) Then, from this leaf, it searches upward (using one comparison per level) for the correct position in that path to insert a[end]. This is the same location as ordinary heapsort finds, and requires the same number of exchanges to perform the insert, but fewer comparisons are required to find that location.Because it goes all the way to the bottom and then comes back up, it is called heapsort with bounce by some authors.\\nfunction leafSearch(a, i, end) is\\n    j \\u2190 i\\n    while iRightChild(j) \\u2264 end do\\n        (Determine which of j's two children is the greater)\\n        if a[iRightChild(j)] > a[iLeftChild(j)] then\\n            j \\u2190 iRightChild(j)\\n        else\\n            j \\u2190 iLeftChild(j)\\n    (At the last level, there might be only one child)\\n    if iLeftChild(j) \\u2264 end then\\n        j \\u2190 iLeftChild(j)\\n    return j\\n\\nThe return value of the leafSearch is used in the modified siftDown routine:\\nprocedure siftDown(a, i, end) is\\n    j \\u2190 leafSearch(a, i, end)\\n    while a[i] > a[j] do\\n        j \\u2190 iParent(j)\\n    x \\u2190 a[j]\\n    a[j] \\u2190 a[i]\\n    while j > i do\\n        swap x, a[iParent(j)]\\n        j \\u2190 iParent(j)\\n\\nBottom-up heapsort was announced as beating quicksort (with median-of-three pivot selection) on arrays of size \\u226516000.A 2008 re-evaluation of this algorithm showed it to be no faster than ordinary heapsort for integer keys, presumably because modern branch prediction nullifies the cost of the predictable comparisons which bottom-up heapsort manages to avoid.A further refinement does a binary search in the path to the selected leaf, and sorts in a worst case of (n+1)(log2(n+1) + log2 log2(n+1) + 1.82) + O(log2n) comparisons, approaching the information-theoretic lower bound of n log2n \\u2212 1.4427n comparisons.A variant which uses two extra bits per internal node (n\\u22121 bits total for an n-element heap) to cache information about which child is greater (two bits are required to store three cases: left, right, and unknown) uses less than n log2n + 1.1n compares.\\n\\nOther variations\\nTernary heapsort uses a ternary heap instead of a binary heap; that is, each element in the heap has three children. It is more complicated to program, but does a constant number of times fewer swap and comparison operations. This is because each sift-down step in a ternary heap requires three comparisons and one swap, whereas in a binary heap two comparisons and one swap are required. Two levels in a ternary heap cover 32 = 9 elements, doing more work with the same number of comparisons as three levels in the binary heap, which only cover 23 = 8. This is primarily of academic interest, or as a student exercise, as the additional complexity is not worth the minor savings, and bottom-up heapsort beats both.\\nMemory-optimized heapsort:\\u200a87\\u200a improves heapsort's locality of reference by increasing the number of children even more. This increases the number of comparisons, but because all children are stored consecutively in memory, reduces the number of cache lines accessed during heap traversal, a net performance improvement.\\nOut-of-place heapsort improves on bottom-up heapsort by eliminating the worst case, guaranteeing n log2n + O(n) comparisons. When the maximum is taken, rather than fill the vacated space with an unsorted data value, fill it with a \\u2212\\u221e sentinel value, which never \\\"bounces\\\" back up. It turns out that this can be used as a primitive in an in-place (and non-recursive) \\\"QuickHeapsort\\\" algorithm. First, you perform a quicksort-like partitioning pass, but reversing the order of the partitioned data in the array. Suppose (without loss of generality) that the smaller partition is the one greater than the pivot, which should go at the end of the array, but our reversed partitioning step places it at the beginning. Form a heap out of the smaller partition and do out-of-place heapsort on it, exchanging the extracted maxima with values from the end of the array. These are less than the pivot, meaning less than any value in the heap, so serve as \\u2212\\u221e sentinel values. Once the heapsort is complete (and the pivot moved to just before the now-sorted end of the array), the order of the partitions has been reversed, and the larger partition at the beginning of the array may be sorted in the same way. (Because there is no non-tail recursion, this also eliminates quicksort's O(log n) stack usage.)\\nThe smoothsort algorithm is a variation of heapsort developed by Edsger W. Dijkstra in 1981. Like heapsort, smoothsort's upper bound is O(n log n). The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n log n) regardless of the initial sorted state. Due to its complexity, smoothsort is rarely used.\\nLevcopoulos and Petersson describe a variation of heapsort based on a heap of Cartesian trees. First, a Cartesian tree is built from the input in O(n) time, and its root is placed in a 1-element binary heap. Then we repeatedly extract the minimum from the binary heap, output the tree's root element, and add its left and right children (if any) which are themselves Cartesian trees, to the binary heap. As they show, if the input is already nearly sorted, the Cartesian trees will be very unbalanced, with few nodes having left and right children, resulting in the binary heap remaining small, and allowing the algorithm to sort more quickly than O(n log n) for inputs that are already nearly sorted.\\nSeveral variants such as weak heapsort require n log2 n+O(1) comparisons in the worst case, close to the theoretical minimum, using one extra bit of state per node. While this extra bit makes the algorithms not truly in-place, if space for it can be found inside the element, these algorithms are simple and efficient,:\\u200a40\\u200a but still slower than binary heaps if key comparisons are cheap enough (e.g. integer keys) that a constant factor does not matter.\\nKatajainen's \\\"ultimate heapsort\\\" requires no extra storage, performs n log2 n+O(1) comparisons, and a similar number of element moves. It is, however, even more complex and not justified unless comparisons are very expensive.\\n\\nComparison with other sorts\\nHeapsort primarily competes with quicksort, another very efficient general purpose in-place comparison-based sort algorithm.\\nHeapsort's primary advantages are its simple, non-recursive code, minimal auxiliary storage requirement, and reliably good performance: its best and worst cases are within a small constant factor of each other, and of the theoretical lower bound on comparison sorts. While it cannot do better than O(n log n) for pre-sorted inputs, it does not suffer from quicksort's O(n2) worst case, either. (The latter can be avoided with careful implementation, but that makes quicksort far more complex, and one of the most popular solutions, introsort, uses heapsort for the purpose.)\\nIts primary disadvantages are its poor locality of reference and its inherently serial nature; the accesses to the implicit tree are widely scattered and mostly random, and there is no straightforward way to convert it to a parallel algorithm.\\nThis makes it popular in embedded systems, real-time computing, and systems concerned with maliciously chosen inputs, such as the Linux kernel. It is also a good choice for any application which does not expect to be bottlenecked on sorting.\\nA well-implemented quicksort is usually 2\\u20133 times faster than heapsort. Although quicksort requires fewer comparisons, this is a minor factor. (Results claiming twice as many comparisons are measuring the top-down version; see \\u00a7 Bottom-up heapsort.) The main advantage of quicksort is its much better locality of reference: partitioning is a linear scan with good spatial locality, and the recursive subdivision has good temporal locality. With additional effort, quicksort can also be implemented in mostly branch-free code, and multiple CPUs can be used to sort subpartitions in parallel. Thus, quicksort is preferred when the additional performance justifies the implementation effort.\\nThe other major O(n log n) sorting algorithm is merge sort, but that rarely competes directly with heapsort because it is not in-place. Merge sort's requirement for \\u03a9(n) extra space (roughly half the size of the input) is usually prohibitive except in the situations where merge sort has a clear advantage:\\n\\nWhen a stable sort is required\\nWhen taking advantage of (partially) pre-sorted input\\nSorting linked lists (in which case merge sort requires minimal extra space)\\nParallel sorting; merge sort parallelizes even better than quicksort and can easily achieve close to linear speedup\\nExternal sorting; merge sort has excellent locality of reference\\n\\nExample\\nLet { 6, 5, 3, 1, 8, 7, 2, 4 } be the list that we want to sort from the smallest to the largest. (NOTE, for 'Building the Heap' step: Larger nodes don't stay below smaller node parents. They are swapped with parents, and then recursively checked if another swap is needed, to keep larger numbers above smaller numbers on the heap binary tree.)\\n\\nBuild the heap\\nSorting\\nNotes\\nReferences\\nWilliams, J. W. J. (1964). \\\"Algorithm 232 \\u2013 Heapsort\\\". Communications of the ACM. 7 (6): 347\\u2013348. doi:10.1145/512274.512284.\\nFloyd, Robert W. (1964). \\\"Algorithm 245 \\u2013 Treesort 3\\\". Communications of the ACM. 7 (12): 701. doi:10.1145/355588.365103. S2CID 52864987.\\nCarlsson, Svante [in Swedish] (1987). \\\"Average-case results on heapsort\\\". BIT Numerical Mathematics. 27 (1): 2\\u201317. doi:10.1007/bf01937350. S2CID 31450060.\\nKnuth, Donald (1997). \\\"\\u00a75.2.3, Sorting by Selection\\\". The Art of Computer Programming. Vol. 3: Sorting and Searching (3rd ed.). Addison-Wesley. pp. 144\\u2013155. ISBN 978-0-201-89685-5.\\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. ISBN 0-262-03293-7. Chapters 6 and 7 Respectively: Heapsort and Priority Queues\\nA PDF of Dijkstra's original paper on Smoothsort\\nHeaps and Heapsort Tutorial by David Carlson, St. Vincent College\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Heap Sort at the Wayback Machine (archived 6 March 2015) \\u2013 graphical demonstration\\nCourseware on Heapsort from Univ. Oldenburg \\u2013 With text, animations and interactive exercises\\nNIST's Dictionary of Algorithms and Data Structures: Heapsort\\nHeapsort implemented in 12 languages\\nSorting revisited by Paul Hsieh\\nA PowerPoint presentation demonstrating how Heap sort works that is for educators.\\nOpen Data Structures \\u2013 Section 11.1.3 \\u2013 Heap-Sort, Pat Morin\"}, {\"Insertion sort\": \"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time by comparisons. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:\\n\\nSimple implementation: Jon Bentley shows a three-line C/C++ version that is five lines when optimized.\\nEfficient for (quite) small data sets, much like other quadratic (i.e., O(n2)) sorting algorithms\\nMore efficient in practice than most other simple quadratic algorithms such as selection sort or bubble sort\\nAdaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(kn) when each element in the input is no more than k places away from its sorted position\\nStable; i.e., does not change the relative order of elements with equal keys\\nIn-place; i.e., only requires a constant amount O(1) of additional memory space\\nOnline; i.e., can sort a list as it receives itWhen people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.\\n\\nAlgorithm\\nInsertion sort iterates, consuming one input element each repetition, and grows a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.\\nSorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.\\nThe resulting array after k iterations has the property where the first k + 1 entries are sorted (\\\"+1\\\" because the first entry is skipped). In each iteration the first remaining entry of the input is removed, and inserted into the result at the correct position, thus extending the result:\\n\\nbecomes\\n\\nwith each element greater than x copied to the right as it is compared against x.\\nThe most common variant of insertion sort, which operates on arrays, can be described as follows:\\n\\nSuppose there exists a function called Insert designed to insert a value into a sorted sequence at the beginning of an array. It operates by beginning at the end of the sequence and shifting each element one place to the right until a suitable position is found for the new element. The function has the side effect of overwriting the value stored immediately after the sorted sequence in the array.\\nTo perform an insertion sort, begin at the left-most element of the array and invoke Insert to insert each element encountered into its correct position. The ordered sequence into which the element is inserted is stored at the beginning of the array in the set of indices already examined. Each insertion overwrites a single value: the value being inserted.Pseudocode of the complete algorithm follows, where the arrays are zero-based:\\ni \\u2190 1\\nwhile i < length(A)\\n    j \\u2190 i\\n    while j > 0 and A[j-1] > A[j]\\n        swap A[j] and A[j-1]\\n        j \\u2190 j - 1\\n    end while\\n    i \\u2190 i + 1\\nend while\\n\\nThe outer loop runs over all the elements except the first one, because the single-element prefix A[0:1] is trivially sorted, so the invariant that the first i entries are sorted is true from the start. The inner loop moves element A[i] to its correct place so that after the loop, the first i+1 elements are sorted. Note that the and-operator in the test must use short-circuit evaluation, otherwise the test might result in an array bounds error, when j=0 and it tries to evaluate A[j-1] > A[j] (i.e. accessing A[-1] fails).\\nAfter expanding the swap operation in-place as x \\u2190 A[j]; A[j] \\u2190 A[j-1]; A[j-1] \\u2190 x (where x is a temporary variable), a slightly faster version can be produced that moves A[i] to its position in one go and only performs one assignment in the inner loop body:\\ni \\u2190 1\\nwhile i < length(A)\\n    x \\u2190 A[i]\\n    j \\u2190 i - 1\\n    while j >= 0 and A[j] > x\\n        A[j+1] \\u2190 A[j]\\n        j \\u2190 j - 1\\n    end while\\n    A[j+1] \\u2190 x\\n    i \\u2190 i + 1\\nend while\\n\\nThe new inner loop shifts elements to the right to clear a spot for x = A[i].\\nThe algorithm can also be implemented in a recursive way. The recursion just replaces the outer loop, calling itself and storing successively smaller values of n on the stack until n equals 0, where the function then returns up the call chain to execute the code after each recursive call starting with n equal to 1, with n increasing by 1 as each instance of the function returns to the prior instance. The initial call would be insertionSortR(A, length(A)-1).\\n\\nfunction insertionSortR(array A, int n)\\n    if n > 0\\n        insertionSortR(A, n-1)\\n        x \\u2190 A[n]\\n        j \\u2190 n-1\\n        while j >= 0 and A[j] > x\\n            A[j+1] \\u2190 A[j]\\n            j \\u2190 j-1\\n        end while\\n        A[j+1] \\u2190 x\\n    end if\\nend function\\n\\nIt does not make the code any shorter, it also doesn't reduce the execution time, but it increases the additional memory consumption from O(1) to O(N) (at the deepest level of recursion the stack contains N references to the A array, each with accompanying value of variable n from N down to 1).\\n\\nBest, worst, and average cases\\nThe best case input is an array that is already sorted. In this case insertion sort has a linear running time (i.e., O(n)). During each iteration, the first remaining element of the input is only compared with the right-most element of the sorted subsection of the array.\\nThe simplest worst case input is an array sorted in reverse order. The set of all worst case inputs consists of all arrays where each element is the smallest or second-smallest of the elements before it. In these cases every iteration of the inner loop will scan and shift the entire sorted subsection of the array before inserting the next element. This gives insertion sort a quadratic running time (i.e., O(n2)).\\nThe average case is also quadratic, which makes insertion sort impractical for sorting large arrays. However, insertion sort is one of the fastest algorithms for sorting very small arrays, even faster than quicksort; indeed, good quicksort implementations use insertion sort for arrays smaller than a certain threshold, also when arising as subproblems; the exact threshold must be determined experimentally and depends on the machine, but is commonly around ten.\\nExample: The following table shows the steps for sorting the sequence {3, 7, 4, 9, 5, 2, 6, 1}.  In each step, the key under consideration is underlined. The key that was moved (or left in place because it was the biggest yet considered) in the previous step is marked with an asterisk.\\n\\n3  7  4  9  5  2  6  1\\n3* 7  4  9  5  2  6  1\\n3  7* 4  9  5  2  6  1\\n3  4* 7  9  5  2  6  1\\n3  4  7  9* 5  2  6  1\\n3  4  5* 7  9  2  6  1\\n2* 3  4  5  7  9  6  1\\n2  3  4  5  6* 7  9  1\\n1* 2  3  4  5  6  7  9\\n\\nRelation to other sorting algorithms\\nInsertion sort is very similar to selection sort. As in selection sort, after k passes through the array, the first k elements are in sorted order. However, the fundamental difference between the two algorithms is that insertion sort scans backwards from the current key, while selection sort scans forwards.  This results in selection sort making the first k elements the k smallest elements of the unsorted input, while in insertion sort they are simply the first k elements of the input.\\nThe primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the (k + 1)-st element is greater than the k-th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort. On average (assuming the rank of the (k + 1)-st element rank is random), insertion sort will require comparing and shifting half of the previous k elements, meaning that insertion sort will perform about half as many comparisons as selection sort on average.\\nIn the worst case for insertion sort (when the input array is reverse-sorted), insertion sort performs just as many comparisons as selection sort. However, a disadvantage of insertion sort over selection sort is that it requires more writes due to the fact that, on each iteration, inserting the (k + 1)-st element into the sorted portion of the array requires many element swaps to shift all of the following elements, while only a single swap is required for each iteration of selection sort. In general, insertion sort will write to the array O(n2) times, whereas selection sort will write only O(n) times. For this reason selection sort may be preferable in cases where writing to memory is significantly more expensive than reading, such as with EEPROM or flash memory.\\nWhile some divide-and-conquer algorithms such as quicksort and mergesort outperform insertion sort for larger arrays, non-recursive sorting algorithms such as insertion sort or selection sort are generally faster for very small arrays (the exact size varies by environment and implementation, but is typically between 7 and 50 elements). Therefore, a useful optimization in the implementation of those algorithms is a hybrid approach, using the simpler algorithm when the array has been divided to a small size.\\n\\nVariants\\nD.L. Shell made substantial improvements to the algorithm; the modified version is called Shell sort.  The sorting algorithm compares elements separated by a distance that decreases on each pass. Shell sort has distinctly improved running times in practical work, with two simple variants requiring O(n3/2) and O(n4/3) running time.If the cost of comparisons exceeds the cost of swaps, as is the case for example with string keys stored by reference or with human interaction (such as choosing one of a pair displayed side-by-side), then using binary insertion sort may yield better performance. Binary insertion sort employs a binary search to determine the correct location to insert new elements, and therefore performs \\u2308log2 n\\u2309 comparisons in the worst case. When each element in the array is searched for and inserted this is O(n log n). The algorithm as a whole still has a running time of O(n2) on average because of the series of swaps required for each insertion.The number of swaps can be reduced by calculating the position of multiple elements before moving them. For example, if the target position of two elements is calculated before they are moved into the proper position, the number of swaps can be reduced by about 25% for random data. In the extreme case, this variant works similar to merge sort.\\nA variant named binary merge sort uses a binary insertion sort to sort groups of 32 elements, followed by a final sort using merge sort. It combines the speed of insertion sort on small data sets with the speed of merge sort on large data sets.To avoid having to make a series of swaps for each insertion, the input could be stored in a linked list, which allows elements to be spliced into or out of the list in constant time when the position in the list is known.  However, searching a linked list requires sequentially following the links to the desired position: a linked list does not have random access, so it cannot use a faster method such as binary search.  Therefore, the running time required for searching is O(n), and the time for sorting is O(n2). If a more sophisticated data structure (e.g., heap or binary tree) is used, the time required for searching and insertion can be reduced significantly; this is the essence of heap sort and binary tree sort.\\nIn 2006 Bender, Martin Farach-Colton, and Mosteiro published a new variant of insertion sort called library sort or gapped insertion sort that leaves a small number of unused spaces (i.e., \\\"gaps\\\") spread throughout the array. The benefit is that insertions need only shift elements over until a gap is reached. The authors show that this sorting algorithm runs with high probability in O(n log n) time.If a skip list is used, the insertion time is brought down to O(log n), and swaps are not needed because the skip list is implemented on a linked list structure.  The final running time for insertion would be O(n log n).\\n\\nList insertion sort code in C\\nIf the items are stored in a linked list, then the list can be sorted with O(1) additional space.  The algorithm starts with an initially empty (and therefore trivially sorted) list. The input items are taken off the list one at a time, and then inserted in the proper place in the sorted list. When the input list is empty, the sorted list has the desired result.\\n\\nThe algorithm below uses a trailing pointer for the insertion into the sorted list. A simpler recursive method rebuilds the list each time (rather than splicing) and can use O(n) stack space.\\n\\nReferences\\nFurther reading\\nKnuth, Donald (1998), \\\"5.2.1: Sorting by Insertion\\\", The Art of Computer Programming, vol. 3. Sorting and Searching (second ed.), Addison-Wesley, pp. 80\\u2013105, ISBN 0-201-89685-0.\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Insertion Sort at the Wayback Machine (archived 8 March 2015) \\u2013 graphical demonstration\\nAdamovsky, John Paul, Binary Insertion Sort \\u2013 Scoreboard \\u2013 Complete Investigation and C Implementation, Pathcom.\\nInsertion Sort \\u2013 a comparison with other O(n2) sorting algorithms, UK: Core war.\\nInsertion sort (C) (wiki), LiteratePrograms \\u2013 implementations of insertion sort in C and several other programming languages\"}, {\"Introsort\": \"Introsort or introspective sort is a hybrid sorting algorithm that provides both fast average performance and (asymptotically) optimal worst-case performance. It begins with quicksort, it switches to heapsort when the recursion depth exceeds a level based on (the logarithm of) the number of elements being sorted and it switches to insertion sort when the number of elements is below some threshold. This combines the good parts of the three algorithms, with practical performance comparable to quicksort on typical data sets and worst-case O(n log n) runtime due to the heap sort. Since the three algorithms it uses are comparison sorts, it is also a comparison sort.\\nIntrosort was invented by David Musser in Musser (1997), in which he also introduced introselect, a hybrid selection algorithm based on quickselect (a variant of quicksort), which falls back to median of medians and thus provides worst-case linear complexity, which is optimal. Both algorithms were introduced with the purpose of providing generic algorithms for the C++ Standard Library which had both fast average performance and optimal worst-case performance, thus allowing the performance requirements to be tightened.  Introsort is in place and not stable.\\n\\nPseudocode\\nIf a heapsort implementation and partitioning functions of the type discussed in the quicksort article are available, the introsort can be described succinctly as\\n\\nprocedure sort(A : array):\\n    maxdepth \\u2190 \\u230alog2(length(A))\\u230b \\u00d7 2\\n    introsort(A, maxdepth)\\n\\nprocedure introsort(A, maxdepth):\\n    n \\u2190 length(A)\\n    if n < 16:\\n        insertionsort(A)\\n    else if maxdepth = 0:\\n        heapsort(A)\\n    else:\\n        p \\u2190 partition(A)  // assume this function does pivot selection, p is the final position of the pivot\\n        introsort(A[1:p-1], maxdepth - 1)\\n        introsort(A[p+1:n], maxdepth - 1)\\n\\nThe factor 2 in the maximum depth is arbitrary; it can be tuned for practical performance. A[i:j] denotes the array slice of items i to j including both A[i] and A[j]. The indices are assumed to start with 1 (the first element of the A array is A[1]).\\n\\nAnalysis\\nIn quicksort, one of the critical operations is choosing the pivot: the element around which the list is partitioned. The simplest pivot selection algorithm is to take the first or the last element of the list as the pivot, causing poor behavior for the case of sorted or nearly sorted input. Niklaus Wirth's variant uses the middle element to prevent these occurrences, degenerating to O(n2) for contrived sequences. The median-of-3 pivot selection algorithm takes the median of the first, middle, and last elements of the list; however, even though this performs well on many real-world inputs, it is still possible to contrive a median-of-3 killer list that will cause dramatic slowdown of a quicksort based on this pivot selection technique.\\nMusser reported that on a median-of-3 killer sequence of 100,000 elements, introsort's running time was 1/200 that of median-of-3 quicksort. Musser also considered the effect on caches of Sedgewick's delayed small sorting, where small ranges are sorted at the end in a single pass of insertion sort. He reported that it could double the number of cache misses, but that its performance with double-ended queues was significantly better and should be retained for template libraries, in part because the gain in other cases from doing the sorts immediately was not great.\\n\\nImplementations\\nIntrosort or some variant is used in a number of standard library sort functions, including some C++ sort implementations.\\nThe June 2000 SGI C++ Standard Template Library stl_algo.h implementation of unstable sort uses the Musser introsort approach with the recursion depth to switch to heapsort passed as a parameter, median-of-3 pivot selection and the Knuth final insertion sort pass for partitions smaller than 16.\\nThe GNU Standard C++ library is similar: uses introsort with a maximum depth of 2\\u00d7log2 n, followed by an insertion sort on partitions smaller than 16.LLVM libc++ also uses introsort with a maximum depth of 2\\u00d7log2 n, however the size limit for insertion sort is different for different data types (30 if swaps are trivial, 6 otherwise). Also, arrays with sizes up to 5 are handled separately. Kutenin (2022) provides an overview for some changes made by LLVM, with a focus on the 2022 fix for quadraticness.The Microsoft .NET Framework Class Library, starting from version 4.5 (2012), uses introsort instead of simple quicksort.Go uses introsort with small modification: for slices of 12 or less elements it uses Shellsort instead of insertion sort, and more advanced median of three medians of three pivot selection for quicksort.\\nJava, starting from version 14 (2020), uses a hybrid sorting algorithm that uses merge sort for highly structured arrays (arrays that are composed of a small number of sorted subarrays) and introsort otherwise to sort arrays of ints, longs, floats and doubles.\\n\\nVariants\\npdqsort\\nPattern-defeating quicksort (pdqsort) is a variant of introsort incorporating the following improvements:\\nMedian-of-three pivoting,\\n\\\"BlockQuicksort\\\" partitioning technique to mitigate branch misprediction penalities,\\nLinear time performance for certain input patterns (adaptive sort),\\nUse element shuffling on bad cases before trying the slower heapsort.pdqsort is used by Rust, GAP, and the C++ library Boost.\\n\\nReferences\\n\\n\\n=== General ===\"}, {\"Library sort\": \"Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions. The name comes from an analogy:\\n\\nSuppose a librarian were to store their books alphabetically on a long shelf, starting with the As at the left end, and continuing to the right along the shelf with no spaces between the books until the end of the Zs. If the librarian acquired a new book that belongs to the B section, once they find the correct space in the B section, they will have to move every book over, from the middle of the Bs all the way down to the Zs in order to make room for the new book. This is an insertion sort. However, if they were to leave a space after every letter, as long as there was still space after B, they would only have to move a few books to make room for the new one. This is the basic principle of the Library Sort.\\nThe algorithm was proposed by Michael A. Bender, Mart\\u00edn Farach-Colton, and Miguel Mosteiro in 2004 and was published in 2006.Like the insertion sort it is based on, library sort is a comparison sort; however, it was shown to have a high probability of running in O(n log n) time (comparable to quicksort), rather than an insertion sort's O(n2). There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.\\nCompared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. The amount and distribution of that space would be implementation dependent. In the paper the size of the needed array is (1 + \\u03b5)n, but with no further recommendations on how to choose \\u03b5. Moreover, it is neither adaptive nor stable. In order to warrant the with-high-probability time bounds, it requires to randomly permute the input, what changes the relative order of equal elements and shuffles any presorted input. Also, the algorithm uses binary search to find the insertion point for each element, which does not take profit of presorted input.\\nAnother drawback is that it cannot be run as an online algorithm, because it is not possible to randomly shuffle the input. If used without this shuffling, it could easily degenerate into quadratic behaviour.\\nOne weakness of insertion sort is that it may require a high number of swap operations and be costly if memory write is expensive. Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, but is also adding an extra cost in the rebalancing step. In addition, locality of reference will be poor compared to mergesort as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets.\\n\\nImplementation\\nAlgorithm\\nLet us say we have an array of n elements. We choose the gap we intend to give. Then we would have a final array of size (1 + \\u03b5)n. The algorithm works in log n rounds. In each round we insert as many elements as there are in the final array already, before re-balancing the array. For finding the position of inserting, we apply Binary Search in the final array and then swap the following elements till we hit an empty space. Once the round is over, we re-balance the final array by inserting spaces between each element.\\nFollowing are three important steps of the algorithm:\\n\\nBinary Search: Finding the position of insertion by applying binary search within the already inserted elements. This can be done by linearly moving towards left or right side of the array if you hit an empty space in the middle element.\\nInsertion: Inserting the element in the position found and swapping the following elements by 1 position till an empty space is hit. This is done in logarithmic time, with high probability.\\nRe-Balancing: Inserting spaces between each pair of elements in the array. The cost of rebalancing is linear in the number of elements already inserted. As these lengths increase with the powers of 2 for each round, the total cost of rebalancing is also linear.\\n\\nPseudocode\\nprocedure rebalance(A, begin, end) is\\n    r \\u2190 end\\n    w \\u2190 end \\u00f7 2\\n\\n    while r \\u2265 begin do\\n        A[w+1] \\u2190 gap\\n        A[w] \\u2190 A[r]\\n        r \\u2190 r \\u2212 1\\n        w \\u2190 w \\u2212 2\\n\\nprocedure sort(A) is\\n    n \\u2190 length(A)\\n    S \\u2190 new array of n gaps\\n\\n    for i \\u2190 1 to floor(log2(n) + 1) do\\n        for j \\u2190 2^i to 2^(i + 1) do\\n            ins \\u2190 binarysearch(A[j], S, 2^(i \\u2212 1))\\n            insert A[j] at S[ins]\\n\\nHere, binarysearch(el, A, k) performs binary search in the first k elements of A, skipping over gaps, to find a place where to locate element el. Insertion should favor gaps over filled-in elements.\\n\\nReferences\\nExternal links\\nGapped Insertion Sort\"}, {\"Merge sort\": \"In computer science, merge sort (also commonly spelled as mergesort) is an efficient, general-purpose, and comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide-and-conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up merge sort appeared in a report by Goldstine and von Neumann as early as 1948.\\n\\nAlgorithm\\nConceptually, a merge sort works as follows:\\n\\nDivide the unsorted list into n sublists, each containing one element (a list of one element is considered sorted).\\nRepeatedly merge sublists to produce new sorted sublists until there is only one sublist remaining.  This will be the sorted list.\\n\\nTop-down implementation\\nExample C-like code using indices for top-down merge sort algorithm that recursively splits the list (called runs in this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step is avoided with alternating the direction of the merge with each level of recursion (except for an initial one-time copy, that can be avoided too). To help understand this, consider an array with two elements. The elements are copied to B[], then merged back to A[]. If there are four elements, when the bottom of the recursion level is reached, single element runs from A[] are merged to B[], and then at the next higher level of recursion, those two-element runs are merged to A[]. This pattern continues with each level of recursion.\\n\\nSorting the entire array is accomplished by TopDownMergeSort(A, B, length(A)).\\n\\nBottom-up implementation\\nExample C-like code using indices for bottom-up merge sort algorithm which treats the list as an array of n sublists (called runs in this example) of size 1, and iteratively merges sub-lists back and forth between two buffers:\\n\\nTop-down implementation using lists\\nPseudocode for top-down merge sort algorithm which recursively divides the input list into smaller sublists until the sublists are trivially sorted, and then merges the sublists while returning up the call chain.\\n\\nfunction merge_sort(list m) is\\n    // Base case. A list of zero or one elements is sorted, by definition.\\n    if length of m \\u2264 1 then\\n        return m\\n\\n    // Recursive case. First, divide the list into equal-sized sublists\\n    // consisting of the first half and second half of the list.\\n    // This assumes lists start at index 0.\\n    var left := empty list\\n    var right := empty list\\n    for each x with index i in m do\\n        if i < (length of m)/2 then\\n            add x to left\\n        else\\n            add x to right\\n\\n    // Recursively sort both sublists.\\n    left := merge_sort(left)\\n    right := merge_sort(right)\\n\\n    // Then merge the now-sorted sublists.\\n    return merge(left, right)\\n\\nIn this example, the merge function merges the left and right sublists.\\n\\nfunction merge(left, right) is\\n    var result := empty list\\n\\n    while left is not empty and right is not empty do\\n        if first(left) \\u2264 first(right) then\\n            append first(left) to result\\n            left := rest(left)\\n        else\\n            append first(right) to result\\n            right := rest(right)\\n\\n    // Either left or right may have elements left; consume them.\\n    // (Only one of the following loops will actually be entered.)\\n    while left is not empty do\\n        append first(left) to result\\n        left := rest(left)\\n    while right is not empty do\\n        append first(right) to result\\n        right := rest(right)\\n    return result\\n\\nBottom-up implementation using lists\\nPseudocode for bottom-up merge sort algorithm which uses a small fixed size array of references to nodes, where array[i] is either a reference to a list of size 2i or nil. node is a reference or pointer to a node. The merge() function would be similar to the one shown in the top-down merge lists example, it merges two already sorted lists, and handles empty lists. In this case, merge() would use node for its input parameters and return value.\\n\\nfunction merge_sort(node head) is\\n    // return if empty list\\n    if head = nil then\\n        return nil\\n    var node array[32]; initially all nil\\n    var node result\\n    var node next\\n    var int  i\\n    result := head\\n    // merge nodes into array\\n    while result \\u2260 nil do\\n        next := result.next;\\n        result.next := nil\\n        for (i = 0; (i < 32) && (array[i] \\u2260 nil); i += 1) do\\n            result := merge(array[i], result)\\n            array[i] := nil\\n        // do not go past end of array\\n        if i = 32 then\\n            i -= 1\\n        array[i] := result\\n        result := next\\n    // merge array into single list\\n    result := nil\\n    for (i = 0; i < 32; i += 1) do\\n        result := merge(array[i], result)\\n    return result\\n\\nAnalysis\\nIn sorting n objects, merge sort has an average and worst-case performance of O(n log n). If the running time of merge sort for a list of length n is T(n), then the recurrence relation T(n) = 2T(n/2) + n follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the n steps taken to merge the resulting two lists). The closed form follows from the master theorem for divide-and-conquer recurrences.\\nThe number of comparisons made by merge sort in the worst case is given by the sorting numbers. These numbers are equal to or slightly smaller than (n \\u2308lg n\\u2309 \\u2212 2\\u2308lg n\\u2309 + 1), which is between (n lg n \\u2212 n + 1) and (n lg n + n + O(lg n)). Merge sort's best case takes about half as many iterations as its worst case.For large n and a randomly ordered input list, merge sort's expected (average) number of comparisons approaches \\u03b1\\u00b7n fewer than the worst case, where \\n  \\n    \\n      \\n        \\u03b1\\n        =\\n        \\u2212\\n        1\\n        +\\n        \\n          \\u2211\\n          \\n            k\\n            =\\n            0\\n          \\n          \\n            \\u221e\\n          \\n        \\n        \\n          \\n            1\\n            \\n              \\n                2\\n                \\n                  k\\n                \\n              \\n              +\\n              1\\n            \\n          \\n        \\n        \\u2248\\n        0.2645.\\n      \\n    \\n    {\\\\displaystyle \\\\alpha =-1+\\\\sum _{k=0}^{\\\\infty }{\\\\frac {1}{2^{k}+1}}\\\\approx 0.2645.}\\n  \\nIn the worst case, merge sort uses approximately 39% fewer comparisons than quicksort does in its average case, and in terms of moves, merge sort's worst case complexity is O(n log n) - the same complexity as quicksort's best case.Merge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially, and is thus popular in languages such as Lisp, where sequentially accessed data structures are very common. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort.\\nMerge sort's most common implementation does not sort in place; therefore, the memory size of the input must be allocated for the sorted output to be stored in (see below for variations that need only n/2 extra spaces).\\n\\nNatural merge sort\\nA natural merge sort is similar to a bottom-up merge sort except that any naturally occurring runs (sorted sequences) in the input are exploited. Both monotonic and bitonic (alternating up/down) runs may be exploited, with lists (or equivalently tapes or files) being convenient data structures (used as FIFO queues or LIFO stacks). In the bottom-up merge sort, the starting point assumes each run is one item long. In practice, random input data will have many short runs that just happen to be sorted. In the typical case, the natural merge sort may not need as many passes because there are fewer runs to merge. In the best case, the input is already sorted (i.e., is one run), so the natural merge sort need only make one pass through the data. In many practical cases, long natural runs are present, and for that reason natural merge sort is exploited as the key component of Timsort. Example:\\n\\nStart       :  3  4  2  1  7  5  8  9  0  6\\nSelect runs : (3  4)(2)(1  7)(5  8  9)(0  6)\\nMerge       : (2  3  4)(1  5  7  8  9)(0  6)\\nMerge       : (1  2  3  4  5  7  8  9)(0  6)\\nMerge       : (0  1  2  3  4  5  6  7  8  9)\\n\\nFormally, the natural merge sort is said to be Runs-optimal, where \\n  \\n    \\n      \\n        \\n          \\n            R\\n            u\\n            n\\n            s\\n          \\n        \\n        (\\n        L\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {Runs}}(L)}\\n   is the number of runs in \\n  \\n    \\n      \\n        L\\n      \\n    \\n    {\\\\displaystyle L}\\n  , minus one.\\nTournament replacement selection sorts are used to gather the initial runs for external sorting algorithms.\\n\\nPing-pong merge sort\\nInstead of merging two blocks at a time, a ping-pong merge merges four blocks at a time. The four sorted blocks are merged simultaneously to auxiliary space into two sorted blocks, then the two sorted blocks are merged back to main memory. Doing so omits the copy operation and reduces the total number of moves by half. An early public domain implementation of a four-at-once merge was by WikiSort in 2014, the method was later that year described as an optimization for patience sorting and named a ping-pong merge. Quadsort implemented the method in 2020 and named it a quad merge.\\n\\nIn-place merge sort\\nOne drawback of merge sort, when implemented on arrays, is its O(n) working memory requirement. Several methods to reduce memory or make merge sort fully in-place have been suggested:\\n\\nKronrod (1969) suggested an alternative version of merge sort that uses constant additional space.\\nKatajainen et al. present an algorithm that requires a constant amount of working memory: enough storage space to hold one element of the input array, and additional space to hold O(1) pointers into the input array. They achieve an O(n log n) time bound with small constants, but their algorithm is not stable.\\nSeveral attempts have been made at producing an in-place merge algorithm that can be combined with a standard (top-down or bottom-up) merge sort to produce an in-place merge sort. In this case, the notion of \\\"in-place\\\" can be relaxed to mean \\\"taking logarithmic stack space\\\", because standard merge sort requires that amount of space for its own stack usage. It was shown by Geffert et al. that in-place, stable merging is possible in O(n log n) time using a constant amount of scratch space, but their algorithm is complicated and has high constant factors: merging arrays of length n and m can take 5n + 12m + o(m) moves. This high constant factor and complicated in-place algorithm was made simpler and easier to understand. Bing-Chao Huang and Michael A. Langston presented a straightforward linear time algorithm practical in-place merge to merge a sorted list using fixed amount of additional space. They both have used the work of Kronrod and others. It merges in linear time and constant extra space. The algorithm takes little more average time than standard merge sort algorithms, free to exploit O(n) temporary extra memory cells, by less than a factor of two. Though the algorithm is much faster in a practical way but it is unstable also for some lists. But using similar concepts, they have been able to solve this problem. Other in-place algorithms include SymMerge, which takes O((n + m) log (n + m)) time in total and is stable. Plugging such an algorithm into merge sort increases its complexity to the non-linearithmic, but still quasilinear, O(n (log n)2).\\nMany applications of external sorting use a form of merge sorting where the input get split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of pages fit into main memory.\\nA modern stable linear and in-place merge variant is block merge sort which creates a section of unique values to use as swap space.\\nThe space overhead can be reduced to sqrt(n) by using binary searches and rotations. This method is employed by the C++ STL library and quadsort.\\nAn alternative to reduce the copying into multiple lists is to associate a new field of information with each key (the elements in m are called keys). This field will be used to link the keys and any associated information together in a sorted list (a key and its related information is called a record). Then the merging of the sorted lists proceeds by changing the link values; no records need to be moved at all. A field which contains only a link will generally be smaller than an entire record so less space will also be used. This is a standard sorting technique, not restricted to merge sort.\\nA simple way to reduce the space overhead to n/2 is to maintain left and right as a combined structure, copy only the left part of m into temporary space, and to direct the merge routine to place the merged output into m. With this version it is better to allocate the temporary space outside the merge routine, so that only one allocation is needed. The excessive copying mentioned previously is also mitigated, since the last pair of lines before the return result statement (function  merge in the pseudo code above) become superfluous.\\n\\nUse with tape drives\\nAn external merge sort is practical to run using disk or tape drives when the data to be sorted is too large to fit into memory. External sorting explains how merge sort is implemented with disk drives. A typical tape drive sort uses four tape drives. All I/O is sequential (except for rewinds at the end of each pass). A minimal implementation can get by with just two record buffers and a few program variables.\\nNaming the four tape drives as A, B, C, D, with the original data on A, and using only two record buffers, the algorithm is similar to the bottom-up implementation, using pairs of tape drives instead of arrays in memory. The basic algorithm can be described as follows:\\n\\nMerge pairs of records from A; writing two-record sublists alternately to C and D.\\nMerge two-record sublists from C and D into four-record sublists; writing these alternately to A and B.\\nMerge four-record sublists from A and B into eight-record sublists; writing these alternately to C and D\\nRepeat until you have one list containing all the data, sorted\\u2014in log2(n) passes.Instead of starting with very short runs, usually a hybrid algorithm is used, where the initial pass will read many records into memory, do an internal sort to create a long run, and then distribute those long runs onto the output set. The step avoids many early passes. For example, an internal sort of 1024 records will save nine passes. The internal sort is often large because it has such a benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory. One of them, the Knuth's 'snowplow' (based on a binary min-heap), generates runs twice as long (on average) as a size of memory used.With some overhead, the above algorithm can be modified to use three tapes. O(n log n) running time can also be achieved using two queues, or a stack and a queue, or three stacks. In the other direction, using k > two tapes (and O(k) items in memory), we can reduce the number of tape operations in O(log k) times by using a k/2-way merge.\\nA more sophisticated merge sort that optimizes tape (and disk) drive usage is the polyphase merge sort.\\n\\nOptimizing merge sort\\nOn modern computers, locality of reference can be of paramount importance in software optimization, because multilevel memory hierarchies are used. Cache-aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the tiled merge sort algorithm stops partitioning subarrays when subarrays of size S are reached, where S is the number of data items fitting into a CPU's cache. Each of these subarrays is sorted with an in-place sorting algorithm such as insertion sort, to discourage memory swaps, and normal merge sort is then completed in the standard recursive fashion. This algorithm has demonstrated better performance on machines that benefit from cache optimization. (LaMarca & Ladner 1997)\\n\\nParallel merge Sort\\nMerge sort parallelizes well due to the use of the divide-and-conquer method. Several different parallel variants of the algorithm have been developed over the years. Some parallel merge sort algorithms are strongly related to the sequential top-down merge algorithm while others have a different general structure and use the K-way merge method.\\n\\nMerge sort with parallel recursion\\nThe sequential merge sort procedure can be described in two phases, the divide phase and the merge phase. The first consists of many recursive calls that repeatedly perform the same division process until the subsequences are trivially sorted (containing one or no element). An intuitive approach is the parallelization of those recursive calls. Following pseudocode describes the merge sort with parallel recursion using the fork and join keywords:\\n\\n// Sort elements lo through hi (exclusive) of array A.\\nalgorithm mergesort(A, lo, hi) is\\n    if lo+1 < hi then  // Two or more elements.\\n        mid := \\u230a(lo + hi) / 2\\u230b\\n        fork mergesort(A, lo, mid)\\n        mergesort(A, mid, hi)\\n        join\\n        merge(A, lo, mid, hi)\\n\\nThis algorithm is the trivial modification of the sequential version and does not parallelize well. Therefore, its speedup is not very impressive. It has a span of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n)}\\n  , which is only an improvement of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (\\\\log n)}\\n   compared to the sequential version (see Introduction to Algorithms). This is mainly due to the sequential merge method, as it is the bottleneck of the parallel executions.\\n\\nMerge sort with parallel merging\\nBetter parallelism can be achieved by using a parallel merge algorithm. Cormen et al. present a binary variant that merges two sorted sub-sequences into one sorted output sequence.In one of the sequences (the longer one if unequal length), the element of the middle index is selected. Its position in the other sequence is determined in such a way that this sequence would remain sorted if this element were inserted at this position. Thus, one knows how many other elements from both sequences are smaller and the position of the selected element in the output sequence can be calculated. For the partial sequences of the smaller and larger elements created in this way, the merge algorithm is again executed in parallel until the base case of the recursion is reached.\\nThe following pseudocode shows the modified parallel merge sort method using the parallel merge algorithm (adopted from Cormen et al.).\\n\\n/**\\n * A: Input array\\n * B: Output array\\n * lo: lower bound\\n * hi: upper bound\\n * off: offset\\n */\\nalgorithm parallelMergesort(A, lo, hi, B, off) is\\n    len := hi - lo + 1\\n    if len == 1 then\\n        B[off] := A[lo]\\n    else let T[1..len] be a new array\\n        mid := \\u230a(lo + hi) / 2\\u230b \\n        mid' := mid - lo + 1\\n        fork parallelMergesort(A, lo, mid, T, 1)\\n        parallelMergesort(A, mid + 1, hi, T, mid' + 1) \\n        join \\n        parallelMerge(T, 1, mid', mid' + 1, len, B, off)\\n\\nIn order to analyze a recurrence relation for the worst case span, the recursive calls of parallelMergesort have to be incorporated only once due to their parallel execution, obtaining\\n\\nFor detailed information about the complexity of the parallel merge procedure, see Merge algorithm.\\nThe solution of this recurrence is given by\\n\\nThis parallel merge algorithm reaches a parallelism of \\n  \\n    \\n      \\n        \\u0398\\n        \\n          (\\n          \\n            \\n              n\\n              \\n                (\\n                log\\n                \\u2061\\n                n\\n                \\n                  )\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\textstyle \\\\Theta \\\\left({\\\\frac {n}{(\\\\log n)^{2}}}\\\\right)}\\n  , which is much higher than the parallelism of the previous algorithm. Such a sort can perform well in practice when combined with a fast stable sequential sort, such as insertion sort, and a fast sequential merge as a base case for merging small arrays.\\n\\nParallel multiway merge sort\\nIt seems arbitrary to restrict the merge sort algorithms to a binary merge method, since there are usually p > 2 processors available. A better approach may be to use a K-way merge method, a generalization of binary merge, in which \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   sorted sequences are merged. This merge variant is well suited to describe a sorting algorithm on a PRAM.\\n\\nBasic Idea\\nGiven an unsorted sequence of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements, the goal is to sort the sequence with \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   available processors. These elements are distributed equally among all processors and sorted locally using a sequential Sorting algorithm. Hence, the sequence consists of sorted sequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{1},...,S_{p}}\\n   of length \\n  \\n    \\n      \\n        \\u2308\\n        \\n          \\n            n\\n            p\\n          \\n        \\n        \\u2309\\n      \\n    \\n    {\\\\textstyle \\\\lceil {\\\\frac {n}{p}}\\\\rceil }\\n  . For simplification let \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   be a multiple of \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  , so that \\n  \\n    \\n      \\n        \\n          |\\n          \\n            S\\n            \\n              i\\n            \\n          \\n          |\\n        \\n        =\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle \\\\left\\\\vert S_{i}\\\\right\\\\vert ={\\\\frac {n}{p}}}\\n   for \\n  \\n    \\n      \\n        i\\n        =\\n        1\\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        p\\n      \\n    \\n    {\\\\displaystyle i=1,...,p}\\n  .\\nThese sequences will be used to perform a multisequence selection/splitter selection. For \\n  \\n    \\n      \\n        j\\n        =\\n        1\\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        p\\n      \\n    \\n    {\\\\displaystyle j=1,...,p}\\n  , the algorithm determines splitter elements \\n  \\n    \\n      \\n        \\n          v\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{j}}\\n   with global rank \\n  \\n    \\n      \\n        k\\n        =\\n        j\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle k=j{\\\\frac {n}{p}}}\\n  . Then the corresponding positions of \\n  \\n    \\n      \\n        \\n          v\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          v\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{1},...,v_{p}}\\n   in each sequence \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   are determined with binary search and thus the \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   are further partitioned into \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   subsequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n            ,\\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            i\\n            ,\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i,1},...,S_{i,p}}\\n   with \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n            ,\\n            j\\n          \\n        \\n        :=\\n        {\\n        x\\n        \\u2208\\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        r\\n        a\\n        n\\n        k\\n        (\\n        \\n          v\\n          \\n            j\\n            \\u2212\\n            1\\n          \\n        \\n        )\\n        <\\n        r\\n        a\\n        n\\n        k\\n        (\\n        x\\n        )\\n        \\u2264\\n        r\\n        a\\n        n\\n        k\\n        (\\n        \\n          v\\n          \\n            j\\n          \\n        \\n        )\\n        }\\n      \\n    \\n    {\\\\textstyle S_{i,j}:=\\\\{x\\\\in S_{i}|rank(v_{j-1})<rank(x)\\\\leq rank(v_{j})\\\\}}\\n  .\\nFurthermore, the elements of \\n  \\n    \\n      \\n        \\n          S\\n          \\n            1\\n            ,\\n            i\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            p\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{1,i},...,S_{p,i}}\\n   are assigned to processor \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , means all elements between rank \\n  \\n    \\n      \\n        (\\n        i\\n        \\u2212\\n        1\\n        )\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle (i-1){\\\\frac {n}{p}}}\\n   and rank \\n  \\n    \\n      \\n        i\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle i{\\\\frac {n}{p}}}\\n  , which are distributed over all \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n  . Thus, each processor receives a sequence of sorted sequences. The fact that the rank \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   of the splitter elements \\n  \\n    \\n      \\n        \\n          v\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{i}}\\n   was chosen globally, provides two important properties: On the one hand, \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   was chosen so that each processor can still operate on \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\textstyle n/p}\\n   elements after assignment. The algorithm is perfectly load-balanced. On the other hand, all elements on processor \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   are less than or equal to all elements on processor \\n  \\n    \\n      \\n        i\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle i+1}\\n  . Hence, each processor performs the p-way merge locally and thus obtains a sorted sequence from its sub-sequences. Because of the second property, no further p-way-merge has to be performed, the results only have to be put together in the order of the processor number.\\n\\nMulti-sequence selection\\nIn its simplest form, given \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   sorted sequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{1},...,S_{p}}\\n   distributed evenly on \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   processors and a rank \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  , the task is to find an element \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   with a global rank \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   in the union of the sequences. Hence, this can be used to divide each \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   in two parts at a splitter index \\n  \\n    \\n      \\n        \\n          l\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle l_{i}}\\n  , where the lower part contains only elements which are smaller than \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  , while the elements bigger than \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   are located in the upper part.\\nThe presented sequential algorithm returns the indices of the splits in each sequence, e.g. the indices \\n  \\n    \\n      \\n        \\n          l\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle l_{i}}\\n   in sequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   such that \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        [\\n        \\n          l\\n          \\n            i\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle S_{i}[l_{i}]}\\n   has a global rank less than \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   and \\n  \\n    \\n      \\n        \\n          r\\n          a\\n          n\\n          k\\n        \\n        \\n          (\\n          \\n            \\n              S\\n              \\n                i\\n              \\n            \\n            [\\n            \\n              l\\n              \\n                i\\n              \\n            \\n            +\\n            1\\n            ]\\n          \\n          )\\n        \\n        \\u2265\\n        k\\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {rank} \\\\left(S_{i}[l_{i}+1]\\\\right)\\\\geq k}\\n  .\\nalgorithm msSelect(S : Array of sorted Sequences [S_1,..,S_p], k : int) is\\n    for i = 1 to p do \\n\\t(l_i, r_i) = (0, |S_i|-1)\\n\\t\\n    while there exists i: l_i < r_i do\\n\\t// pick Pivot Element in S_j[l_j], .., S_j[r_j], chose random j uniformly\\n\\tv := pickPivot(S, l, r)\\n\\tfor i = 1 to p do \\n\\t    m_i = binarySearch(v, S_i[l_i, r_i]) // sequentially\\n\\tif m_1 + ... + m_p >= k then // m_1+ ... + m_p is the global rank of v\\n\\t    r := m  // vector assignment\\n\\telse\\n\\t    l := m \\n\\t\\n    return l\\n\\nFor the complexity analysis the PRAM model is chosen. If the data is evenly distributed over all \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  , the p-fold execution of the binarySearch method has a running time of \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                n\\n                \\n                  /\\n                \\n                p\\n              \\n              )\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\log \\\\left(n/p\\\\right)\\\\right)}\\n  . The expected recursion depth is \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                \\n                  \\u2211\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  S\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n              \\n              )\\n            \\n          \\n          )\\n        \\n        =\\n        \\n          \\n            O\\n          \\n        \\n        (\\n        log\\n        \\u2061\\n        (\\n        n\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(\\\\log \\\\left(\\\\textstyle \\\\sum _{i}|S_{i}|\\\\right)\\\\right)={\\\\mathcal {O}}(\\\\log(n))}\\n   as in the ordinary Quickselect. Thus the overall expected running time is \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\log(n/p)\\\\log(n)\\\\right)}\\n  .\\nApplied on the parallel multiway merge sort, this algorithm has to be invoked in parallel such that all splitter elements of rank \\n  \\n    \\n      \\n        i\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle i{\\\\frac {n}{p}}}\\n   for \\n  \\n    \\n      \\n        i\\n        =\\n        1\\n        ,\\n        .\\n        .\\n        ,\\n        p\\n      \\n    \\n    {\\\\displaystyle i=1,..,p}\\n   are found simultaneously. These splitter elements can then be used to partition each sequence in \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   parts, with the same total running time of \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\,\\\\log(n/p)\\\\log(n)\\\\right)}\\n  .\\n\\nPseudocode\\nBelow, the complete pseudocode of the parallel multiway merge sort algorithm is given. We assume that there is a barrier synchronization before and after the multisequence selection such that every processor can determine the splitting elements and the sequence partition properly.\\n\\n/**\\n * d: Unsorted Array of Elements\\n * n: Number of Elements\\n * p: Number of Processors\\n * return Sorted Array\\n */\\nalgorithm parallelMultiwayMergesort(d : Array, n : int, p : int) is\\n    o := new Array[0, n]                         // the output array\\n    for i = 1 to p do in parallel                // each processor in parallel\\n        S_i := d[(i-1) * n/p, i * n/p] \\t         // Sequence of length n/p\\n\\tsort(S_i)                                // sort locally\\n        synch\\n\\tv_i := msSelect([S_1,...,S_p], i * n/p)          // element with global rank i * n/p\\n        synch\\n\\t(S_i,1, ..., S_i,p) := sequence_partitioning(si, v_1, ..., v_p) // split s_i into subsequences\\n\\t    \\n\\to[(i-1) * n/p, i * n/p] := kWayMerge(s_1,i, ..., s_p,i)  // merge and assign to output array\\n\\t\\n    return o\\n\\nAnalysis\\nFirstly, each processor sorts the assigned \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\displaystyle n/p}\\n   elements locally using a sorting algorithm with complexity \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            n\\n            \\n              /\\n            \\n            p\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(n/p\\\\;\\\\log(n/p)\\\\right)}\\n  . After that, the splitter elements have to be calculated in time \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\,\\\\log(n/p)\\\\log(n)\\\\right)}\\n  . Finally, each group of \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   splits have to be merged in parallel by each processor with a running time of \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        (\\n        log\\n        \\u2061\\n        (\\n        p\\n        )\\n        \\n        n\\n        \\n          /\\n        \\n        p\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}(\\\\log(p)\\\\;n/p)}\\n   using a sequential p-way merge algorithm. Thus, the overall running time is given by\\n\\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                n\\n                p\\n              \\n            \\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                \\n                  n\\n                  p\\n                \\n              \\n              )\\n            \\n            +\\n            p\\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                \\n                  n\\n                  p\\n                \\n              \\n              )\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n            +\\n            \\n              \\n                n\\n                p\\n              \\n            \\n            log\\n            \\u2061\\n            (\\n            p\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left({\\\\frac {n}{p}}\\\\log \\\\left({\\\\frac {n}{p}}\\\\right)+p\\\\log \\\\left({\\\\frac {n}{p}}\\\\right)\\\\log(n)+{\\\\frac {n}{p}}\\\\log(p)\\\\right)}\\n  .\\n\\nPractical adaption and application\\nThe multiway merge sort algorithm is very scalable through its high parallelization capability, which allows the use of many processors. This makes the algorithm a viable candidate for sorting large amounts of data, such as those processed in computer clusters. Also, since in such systems memory is usually not a limiting resource, the disadvantage of space complexity of merge sort is negligible. However, other factors become important in such systems, which are not taken into account when modelling on a PRAM. Here, the following aspects need to be considered: Memory hierarchy, when the data does not fit into the processors cache, or the communication overhead of exchanging data between processors, which could become a bottleneck when the data can no longer be accessed via the shared memory.\\nSanders et al. have presented in their paper a bulk synchronous parallel algorithm for multilevel multiway mergesort, which divides \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   processors into \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   groups of size \\n  \\n    \\n      \\n        \\n          p\\n          \\u2032\\n        \\n      \\n    \\n    {\\\\displaystyle p'}\\n  . All processors sort locally first. Unlike single level multiway mergesort, these sequences are then partitioned into \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   parts and assigned to the appropriate processor groups. These steps are repeated recursively in those groups. This reduces communication and especially avoids problems with many small messages. The hierarchical structure of the underlying real network can be used to define the processor groups (e.g. racks, clusters,...).\\n\\nFurther variants\\nMerge sort was one of the first sorting algorithms where optimal speed up was achieved, with Richard Cole using a clever subsampling algorithm to ensure O(1) merge. Other sophisticated parallel sorting algorithms can achieve the same or better time bounds with a lower constant. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW parallel random-access machine (PRAM) with n processors by performing partitioning implicitly. Powers further shows that a pipelined version of Batcher's Bitonic Mergesort at O((log n)2) time on a butterfly sorting network is in practice actually faster than his O(log n) sorts on a PRAM, and he provides detailed discussion of the hidden overheads in comparison, radix and parallel sorting.\\n\\nComparison with other sort algorithms\\nAlthough heapsort has the same time bounds as merge sort, it requires only \\u0398(1) auxiliary space instead of merge sort's \\u0398(n). On typical modern architectures, efficient quicksort implementations generally outperform merge sort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only \\u0398(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.\\nAs of Perl 5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl). In Java, the Arrays.sort() methods use merge sort or a tuned quicksort depending on the datatypes and for implementation efficiency switch to insertion sort when fewer than seven array elements are being sorted. The Linux kernel uses merge sort for its linked lists. Python uses Timsort, another tuned hybrid of merge sort and insertion sort, that has become the standard sort algorithm in Java SE 7 (for arrays of non-primitive types), on the Android platform, and in GNU Octave.\\n\\nNotes\\nReferences\\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2009) [1990]. Introduction to Algorithms (3rd ed.). MIT Press and McGraw-Hill. ISBN 0-262-03384-4.\\nKatajainen, Jyrki; Pasanen, Tomi; Teuhola, Jukka (1996). \\\"Practical in-place mergesort\\\". Nordic Journal of Computing. 3 (1): 27\\u201340. CiteSeerX 10.1.1.22.8523. ISSN 1236-6064. Archived from the original on 2011-08-07. Retrieved 2009-04-04.. Also Practical In-Place Mergesort. Also [3]\\nKnuth, Donald (1998). \\\"Section 5.2.4: Sorting by Merging\\\". Sorting and Searching. The Art of Computer Programming. Vol. 3 (2nd ed.). Addison-Wesley. pp. 158\\u2013168. ISBN 0-201-89685-0.\\nKronrod, M. A. (1969). \\\"Optimal ordering algorithm without operational field\\\". Soviet Mathematics - Doklady. 10: 744.\\nLaMarca, A.; Ladner, R. E. (1997). \\\"The influence of caches on the performance of sorting\\\". Proc. 8th Ann. ACM-SIAM Symp. On Discrete Algorithms (SODA97): 370\\u2013379. CiteSeerX 10.1.1.31.1153.\\nSkiena, Steven S. (2008). \\\"4.5: Mergesort: Sorting by Divide-and-Conquer\\\". The Algorithm Design Manual (2nd ed.). Springer. pp. 120\\u2013125. ISBN 978-1-84800-069-8.\\nSun Microsystems. \\\"Arrays API (Java SE 6)\\\". Retrieved 2007-11-19.\\nOracle Corp. \\\"Arrays (Java SE 10 & JDK 10)\\\". Retrieved 2018-07-23.\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Merge Sort at the Wayback Machine (archived 6 March 2015) \\u2013 graphical demonstration\\nOpen Data Structures - Section 11.1.1 - Merge Sort, Pat Morin\"}, {\"Merge-insertion sort\": \"In computer science, merge-insertion sort or the Ford\\u2013Johnson algorithm is a comparison sorting algorithm published in 1959 by L. R. Ford Jr. and Selmer M. Johnson. It uses fewer comparisons in the worst case than the best previously known algorithms, binary insertion sort and merge sort, and for 20 years it was the sorting algorithm with the fewest known comparisons. Although not of practical significance, it remains of theoretical interest in connection with the problem of sorting with a minimum number of comparisons. The same algorithm may have also been independently discovered by Stanis\\u0142aw Trybu\\u0142a and Czen Ping.\\n\\nAlgorithm\\nMerge-insertion sort performs the following steps, on an input \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements:\\nGroup the elements of \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   into \\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   pairs of elements, arbitrarily, leaving one element unpaired if there is an odd number of elements.\\nPerform \\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   comparisons, one per pair, to determine the larger of the two elements in each pair.\\nRecursively sort the \\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   larger elements from each pair, creating a sorted sequence \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   of \\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   of the input elements, in ascending order.\\nInsert at the start of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   the element that was paired with the first and smallest element of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n  .\\nInsert the remaining \\n  \\n    \\n      \\n        \\u2308\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u2309\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\lceil n/2\\\\rceil -1}\\n   elements of \\n  \\n    \\n      \\n        X\\n        \\u2216\\n        S\\n      \\n    \\n    {\\\\displaystyle X\\\\setminus S}\\n   into \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n  , one at a time, with a specially chosen insertion ordering described below. Use binary search in subsequences of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   (as described below) to determine the position at which each element should be inserted.The algorithm is designed to take advantage of the fact that the binary searches used to insert elements into \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   are most efficient (from the point of view of worst case analysis) when the length of the subsequence that is searched is one less than a power of two. This is because, for those lengths, all outcomes of the search use the same number of comparisons as each other. To choose an insertion ordering that produces these lengths, consider the sorted sequence \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   after step 4 of the outline above (before inserting the remaining elements),\\nand let \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   denote the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  th element of this sorted sequence. Thus,\\n\\n  \\n    \\n      \\n        S\\n        =\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        ,\\n        \\u2026\\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle S=(x_{1},x_{2},x_{3},\\\\dots ),}\\n  where each element \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   with \\n  \\n    \\n      \\n        i\\n        \\u2265\\n        3\\n      \\n    \\n    {\\\\displaystyle i\\\\geq 3}\\n   is paired with an element \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n        <\\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}<x_{i}}\\n   that has not yet been inserted. (There are no elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{1}}\\n   or \\n  \\n    \\n      \\n        \\n          y\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{2}}\\n   because \\n  \\n    \\n      \\n        \\n          x\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{1}}\\n   and \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{2}}\\n   were paired with each other.) If \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is odd, the remaining unpaired element should also be numbered as \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   with \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   larger than the indexes of the paired elements.\\nThen, the final step of the outline above can be expanded into the following steps:\\nPartition the uninserted elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   into groups with contiguous indexes. There are two elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{3}}\\n   and \\n  \\n    \\n      \\n        \\n          y\\n          \\n            4\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{4}}\\n   in the first group, and the sums of sizes of every two adjacent groups form a sequence of powers of two. Thus, the sizes of groups are: 2, 2, 6, 10, 22, 42, ...\\nOrder the uninserted elements by their groups (smaller indexes to larger indexes), but within each group order them from larger indexes to smaller indexes. Thus, the ordering becomes\\n  \\n    \\n      \\n        \\n          y\\n          \\n            4\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            3\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            6\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            5\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            12\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            11\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            10\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            9\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            8\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            7\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            22\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            21\\n          \\n        \\n        \\u2026\\n      \\n    \\n    {\\\\displaystyle y_{4},y_{3},y_{6},y_{5},y_{12},y_{11},y_{10},y_{9},y_{8},y_{7},y_{22},y_{21}\\\\dots }\\n  Use this ordering to insert the elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n   into \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n  . For each element \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n  , use a binary search from the start of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   up to but not including \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   to determine where to insert \\n  \\n    \\n      \\n        \\n          y\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{i}}\\n  .\\n\\nAnalysis\\nLet \\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle C(n)}\\n   denote the number of comparisons that merge-insertion sort makes, in the worst case, when sorting \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements.\\nThis number of comparisons can be broken down as the sum of three terms:\\n\\n  \\n    \\n      \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle \\\\lfloor n/2\\\\rfloor }\\n   comparisons among the pairs of items,\\n\\n  \\n    \\n      \\n        C\\n        (\\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n        )\\n      \\n    \\n    {\\\\displaystyle C(\\\\lfloor n/2\\\\rfloor )}\\n   comparisons for the recursive call, and\\nsome number of comparisons for the binary insertions used to insert the remaining elements.In the third term, the worst-case number of comparisons for the elements in the first group is two, because each is inserted into a subsequence of \\n  \\n    \\n      \\n        S\\n      \\n    \\n    {\\\\displaystyle S}\\n   of length at most three. First, \\n  \\n    \\n      \\n        \\n          y\\n          \\n            4\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{4}}\\n   is inserted into the three-element subsequence \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},x_{3})}\\n  . Then, \\n  \\n    \\n      \\n        \\n          y\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{3}}\\n   is inserted into some permutation of the three-element subsequence \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            4\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},y_{4})}\\n  , or in some cases into the two-element subsequence \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2})}\\n  . Similarly, the elements \\n  \\n    \\n      \\n        \\n          y\\n          \\n            6\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{6}}\\n   and \\n  \\n    \\n      \\n        \\n          y\\n          \\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{5}}\\n   of the second group are each inserted into a subsequence of length at most seven, using three comparisons. More generally, the worst-case number of comparisons for the elements in the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  th group is \\n  \\n    \\n      \\n        i\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle i+1}\\n  , because each is inserted into a subsequence of length at most \\n  \\n    \\n      \\n        \\n          2\\n          \\n            i\\n            +\\n            1\\n          \\n        \\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle 2^{i+1}-1}\\n  . By summing the number of comparisons used for all the elements and solving the resulting recurrence relation,\\nthis analysis can be used to compute the values of \\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle C(n)}\\n  , giving the formula\\n\\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          \\u2308\\n          \\n            \\n              log\\n              \\n                2\\n              \\n            \\n            \\u2061\\n            \\n              \\n                \\n                  3\\n                  i\\n                \\n                4\\n              \\n            \\n          \\n          \\u2309\\n        \\n        \\u2248\\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        1.415\\n        n\\n      \\n    \\n    {\\\\displaystyle C(n)=\\\\sum _{i=1}^{n}\\\\left\\\\lceil \\\\log _{2}{\\\\frac {3i}{4}}\\\\right\\\\rceil \\\\approx n\\\\log _{2}n-1.415n}\\n  or, in closed form,\\n\\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n        =\\n        n\\n        \\n          \\n            \\u2308\\n          \\n        \\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        \\n          \\n            \\n              3\\n              n\\n            \\n            4\\n          \\n        \\n        \\n          \\n            \\u2309\\n          \\n        \\n        \\u2212\\n        \\n          \\n            \\u230a\\n          \\n        \\n        \\n          \\n            \\n              2\\n              \\n                \\u230a\\n                \\n                  log\\n                  \\n                    2\\n                  \\n                \\n                \\u2061\\n                6\\n                n\\n                \\u230b\\n              \\n            \\n            3\\n          \\n        \\n        \\n          \\n            \\u230b\\n          \\n        \\n        +\\n        \\n          \\n            \\u230a\\n          \\n        \\n        \\n          \\n            \\n              \\n                log\\n                \\n                  2\\n                \\n              \\n              \\u2061\\n              6\\n              n\\n            \\n            2\\n          \\n        \\n        \\n          \\n            \\u230b\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle C(n)=n{\\\\biggl \\\\lceil }\\\\log _{2}{\\\\frac {3n}{4}}{\\\\biggr \\\\rceil }-{\\\\biggl \\\\lfloor }{\\\\frac {2^{\\\\lfloor \\\\log _{2}6n\\\\rfloor }}{3}}{\\\\biggr \\\\rfloor }+{\\\\biggl \\\\lfloor }{\\\\frac {\\\\log _{2}6n}{2}}{\\\\biggr \\\\rfloor }.}\\n  For \\n  \\n    \\n      \\n        n\\n        =\\n        1\\n        ,\\n        2\\n        ,\\n        \\u2026\\n      \\n    \\n    {\\\\displaystyle n=1,2,\\\\dots }\\n   the numbers of comparisons are\\n0, 1, 3, 5, 7, 10, 13, 16, 19, 22, 26, 30, 34, ... (sequence A001768 in the OEIS)\\n\\nRelation to other comparison sorts\\nThe algorithm is called merge-insertion sort because the initial comparisons that it performs before its recursive call (pairing up arbitrary items and comparing each pair) are the same as the initial comparisons of merge sort,\\nwhile the comparisons that it performs after the recursive call (using binary search to insert elements one by one into a sorted list) follow the same principle as insertion sort. In this sense, it is a hybrid algorithm that combines both merge sort and insertion sort.For small inputs (up to \\n  \\n    \\n      \\n        n\\n        =\\n        11\\n      \\n    \\n    {\\\\displaystyle n=11}\\n  ) its numbers of comparisons equal the lower bound on comparison sorting of \\n  \\n    \\n      \\n        \\u2308\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        !\\n        \\u2309\\n        \\u2248\\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        1.443\\n        n\\n      \\n    \\n    {\\\\displaystyle \\\\lceil \\\\log _{2}n!\\\\rceil \\\\approx n\\\\log _{2}n-1.443n}\\n  . However, for larger inputs the number of comparisons made by the merge-insertion algorithm is bigger than this lower bound.\\nMerge-insertion sort also performs fewer comparisons than the sorting numbers, which count the comparisons made by binary insertion sort or merge sort in the worst case. The sorting numbers fluctuate between \\n  \\n    \\n      \\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        0.915\\n        n\\n      \\n    \\n    {\\\\displaystyle n\\\\log _{2}n-0.915n}\\n   and \\n  \\n    \\n      \\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        n\\n      \\n    \\n    {\\\\displaystyle n\\\\log _{2}n-n}\\n  , with the same leading term but a worse constant factor in the lower-order linear term.Merge-insertion sort is the sorting algorithm with the minimum possible comparisons for \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   items whenever \\n  \\n    \\n      \\n        n\\n        \\u2264\\n        15\\n      \\n    \\n    {\\\\displaystyle n\\\\leq 15}\\n   or \\n  \\n    \\n      \\n        20\\n        \\u2264\\n        n\\n        \\u2264\\n        22\\n      \\n    \\n    {\\\\displaystyle 20\\\\leq n\\\\leq 22}\\n  , and it has the fewest comparisons known for \\n  \\n    \\n      \\n        n\\n        \\u2264\\n        46\\n      \\n    \\n    {\\\\displaystyle n\\\\leq 46}\\n  .\\nFor 20 years, merge-insertion sort was the sorting algorithm with the fewest comparisons known for all input lengths.\\nHowever, in 1979 Glenn Manacher published another sorting algorithm that used even fewer comparisons, for large enough inputs.\\nIt remains unknown exactly how many comparisons are needed for sorting, for all \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  , but Manacher's algorithm\\nand later record-breaking sorting algorithms have all used modifications of the merge-insertion sort ideas.\\n\\n\\n== References ==\"}, {\"Multi-key quicksort\": \"Multi-key quicksort, also known as three-way radix quicksort, is an algorithm for sorting strings. This hybrid of quicksort and radix sort was originally suggested by P. Shackleton, as reported in one of C.A.R. Hoare's seminal papers on quicksort;:\\u200a14\\u200a its modern incarnation was developed by Jon Bentley and Robert Sedgewick in the mid-1990s. The algorithm is designed to exploit the property that in many problems, strings tend to have shared prefixes.\\nOne of the algorithm's uses is the construction of suffix arrays, for which it was one of the fastest algorithms as of 2004.\\n\\nDescription\\nThe three-way radix quicksort algorithm sorts an array of N (pointers to) strings in lexicographic order. It is assumed that all strings are of equal length K; if the strings are of varying length, they must be padded with extra elements that are less than any element in the strings. The pseudocode for the algorithm is then\\nalgorithm sort(a : array of string, d : integer) is\\n    if length(a) \\u2264 1 or d \\u2265 K then\\n        return\\n    p := pivot(a, d)\\n    i, j := partition(a, d, p)   (Note a simultaneous assignment of two variables.)\\n    sort(a[0:i), d)\\n    sort(a[i:j), d + 1)\\n    sort(a[j:length(a)), d)\\n\\nUnlike most string sorting algorithms that look at many bytes in a string to decide if a string is less than, the same as, or equal to some other string; and then turning its focus to some other pair of strings, the multi-key quicksort initially looks at only one byte of every string in the array, byte d, initially the first byte of every string.\\nThe recursive call uses a new value of d and passes a subarray where every string in the subarray has exactly the same initial part -- the characters before character d.\\nThe pivot function must return a single character. Bentley and Sedgewick suggest either picking the median of a[0][d], ..., a[length(a)\\u22121][d] or some random character in that range. The partition function is a variant of the one used in ordinary three-way quicksort: it rearranges a so that all of a[0], ..., a[i\\u22121] have an element at position d that is less than p, a[i], ..., a[j\\u22121] have p at position d, and strings from j onward have a d'th element larger than p. (The original partitioning function suggested by Bentley and Sedgewick may be slow in the case of repeated elements; a Dutch national flag partitioning can be used to alleviate this.)\\nPractical implementations of multi-key quicksort can benefit from the same optimizations typically applied to quicksort: median-of-three pivoting, switching to insertion sort for small arrays, etc.\\n\\nSee also\\nAmerican flag sort \\u2013  another radix sort variant that is fast for string sorting\\nTernary search tree \\u2013  three-way radix quicksort is isomorphic to this data structure in the same way that quicksort is isomorphic to binary search trees\\n\\nNotes\\n\\n\\n== References ==\"}, {\"Odd\\u2013even sort\": \"In computing, an odd\\u2013even sort or odd\\u2013even transposition sort (also known as brick sort or parity sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections.  It is a comparison sort related to bubble sort, with which it shares many characteristics.  It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched.  The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.\\n\\nSorting on processor arrays\\nOn parallel processors, with one value per processor and only local left\\u2013right neighbor connections, the processors all concurrently do a compare\\u2013exchange operation with their neighbors, alternating between odd\\u2013even and even\\u2013odd pairings.  This algorithm was originally presented, and shown to be efficient on such processors, by Habermann in 1972.The algorithm extends efficiently to the case of multiple items per processor.  In the Baudet\\u2013Stevenson odd\\u2013even merge-splitting algorithm, each processor sorts its own sublist at each step, using any efficient sort algorithm, and then performs a merge splitting, or transposition\\u2013merge, operation with its neighbor, with neighbor pairing alternating between odd\\u2013even and even\\u2013odd on each step.\\n\\nBatcher's odd\\u2013even mergesort\\nA related but more efficient sort algorithm is the Batcher odd\\u2013even mergesort, using compare\\u2013exchange operations and perfect-shuffle operations.\\nBatcher's method is efficient on parallel processors with long-range connections.\\n\\nAlgorithm\\nThe single-processor algorithm, like bubblesort, is simple but not very efficient. Here a zero-based index is assumed:\\n\\nProof of correctness\\nClaim:  Let \\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{1},...,a_{n}}\\n   be a sequence of data ordered by <.  The odd\\u2013even sort algorithm correctly sorts this data in \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   passes.  (A pass here is defined to be a full sequence of odd\\u2013even, or even\\u2013odd comparisons.  The passes occur in order pass 1: odd\\u2013even, pass 2: even\\u2013odd, etc.)\\nProof:\\nThis proof is based loosely on one by Thomas Worsch.Since the sorting algorithm only involves comparison-swap operations and is oblivious (the order of comparison-swap operations does not depend on the data), by Knuth's 0\\u20131 sorting principle, it suffices to check correctness when each \\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{i}}\\n   is either 0 or 1. Assume that there are \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n   1s.\\nObserve that the rightmost 1 can be either in an even or odd position, so it might not be moved by the first odd\\u2013even pass. But after the first odd\\u2013even pass, the rightmost 1 will be in an even position. It follows that it will be moved to the right by all remaining passes. Since the rightmost one starts in position greater than or equal to \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n  , it must be moved at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n      \\n    \\n    {\\\\displaystyle n-e}\\n   steps. It follows that it takes at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle n-e+1}\\n   passes to move the rightmost 1 to its correct position.\\nNow, consider the second rightmost 1. After two passes, the 1 to its right will have moved right by at least one step. It follows that, for all remaining passes, we can view the second rightmost 1 as the rightmost 1. The second rightmost 1 starts in position at least \\n  \\n    \\n      \\n        e\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle e-1}\\n   and must be moved to position at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n  , so it must be moved at most \\n  \\n    \\n      \\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\u2212\\n        (\\n        e\\n        \\u2212\\n        1\\n        )\\n        =\\n        n\\n        \\u2212\\n        e\\n      \\n    \\n    {\\\\displaystyle (n-1)-(e-1)=n-e}\\n   steps. After at most 2 passes, the rightmost 1 will have already moved, so the entry to the right of the second rightmost 1 will be 0.  Hence, for all passes after the first two, the second rightmost 1 will move to the right. It thus takes at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        2\\n      \\n    \\n    {\\\\displaystyle n-e+2}\\n   passes to move the second rightmost 1 to its correct position.\\nContinuing in this manner, by induction it can be shown that the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th rightmost 1 is moved to its correct position in at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        i\\n      \\n    \\n    {\\\\displaystyle n-e+i}\\n   passes. Since \\n  \\n    \\n      \\n        i\\n        \\u2264\\n        e\\n      \\n    \\n    {\\\\displaystyle i\\\\leq e}\\n  , it follows that the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th rightmost 1 is moved to its correct position in at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        e\\n        =\\n        n\\n      \\n    \\n    {\\\\displaystyle n-e+e=n}\\n   passes. The list is thus correctly sorted in \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   passes. QED.\\nWe remark that each pass takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   steps, so this algorithm has \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   complexity.\\n\\n\\n== References ==\"}, {\"Oscillating merge sort\": \"Oscillating merge sort or oscillating sort is a variation of merge sort used with tape drives that can read backwards.  Instead of doing a complete distribution as is done in a tape merge, the distribution of the input and the merging of runs are interspersed.  The oscillating merge sort does not waste rewind time or have tape drives sit idle as in the conventional tape merge.\\nThe oscillating merge sort \\\"was designed for tapes that can be read backward and is more efficient generally than either the polyphase or cascade merges.\\\"\\n\\nReferences\\nBradley, James (1982), File and Data Base Techniques, Holt, Rinehart and Winston, ISBN 0-03-058673-9\\n\\nFurther reading\\nFlores, Ivan (1969), Computer Sorting, Prentice-Hall, ISBN 978-0-13165746-5\\nKnuth, D. E. (1975), Sorting and Searching, The Art of Computer Programming, vol. 3, Addison Wesley\\nLowden, B. G. T., \\\"A note on the oscillating sort\\\" (PDF), The Computer Journal, 20 (1): 92, doi:10.1093/comjnl/20.1.92\\nMartin, W. A. (1971), \\\"Sorting\\\", Computing Surveys, ACM, 3 (4): 147\\u2013174, doi:10.1145/356593.356594\\nSobel, Sheldon (July 1962), \\\"Oscillating Sort\\u2013A New Sort Merging Technique\\\", Journal of the ACM, New York, NY: ACM, 9 (3): 372\\u2013374, doi:10.1145/321127.321133, S2CID 11554742\\n\\nExternal links\\nMihaldinecz, Maximilian (2016), \\\"A variation of Oscillating Merge Sort implemented in Matlab\\\", GitHub\"}, {\"Patience sorting\": \"In computer science, patience sorting is a sorting algorithm inspired by, and named after, the card game patience. A variant of the algorithm efficiently computes the length of a longest increasing subsequence in a given array.\\n\\nOverview\\nThe algorithm's name derives from a simplified variant of the patience card game. The game begins with a shuffled deck of cards. The cards are dealt one by one into a sequence of piles on the table, according to the following rules.\\nInitially, there are no piles. The first card dealt forms a new pile consisting of the single card.\\nEach subsequent card is placed on the leftmost existing pile whose top card has a value greater than or equal to the new card's value, or to the right of all of the existing piles, thus forming a new pile.\\nWhen there are no more cards remaining to deal, the game ends.This card game is turned into a two-phase sorting algorithm, as follows. Given an array of n elements from some totally ordered domain, consider this array as a collection of cards and simulate the patience sorting game. When the game is over, recover the sorted sequence by repeatedly picking off the minimum visible card; in other words, perform a k-way merge of the p piles, each of which is internally sorted.\\n\\nAnalysis\\nThe first phase of patience sort, the card game simulation, can be implemented to take O(n log n) comparisons in the worst case for an n-element input array: there will be at most n piles, and by construction, the top cards of the piles form an increasing sequence from left to right, so the desired pile can be found by binary search. The second phase, the merging of piles, can be done in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   time as well using a priority queue.When the input data contain natural \\\"runs\\\", i.e., non-decreasing subarrays, then performance can be strictly better. In fact, when the input array is already sorted, all values form a single pile and both phases run in O(n) time. The average-case complexity is still O(n log n): any uniformly random sequence of values will produce an expected number of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O({\\\\sqrt {n}})}\\n   piles, which take \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        \\n          \\n            n\\n          \\n        \\n        )\\n        =\\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log {\\\\sqrt {n}})=O(n\\\\log n)}\\n   time to produce and merge.An evaluation of the practical performance of patience sort is given by Chandramouli and Goldstein, who show that a naive version is about ten to twenty times slower than a state-of-the-art quicksort on their benchmark problem. They attribute this to the relatively small amount of research put into patience sort, and develop several optimizations that bring its performance to within a factor of two of that of quicksort.If values of cards are in the range 1, . . . , n, there is an efficient implementation with \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   worst-case running time for putting the cards into piles, relying on a Van Emde Boas tree.\\n\\nRelations to other problems\\nPatience sorting is closely related to a card game called Floyd's game. This game is very similar to the game sketched earlier:\\nThe first card dealt forms a new pile consisting of the single card.\\nEach subsequent card is placed on some existing pile whose top card has a value no less than the new card's value, or to the right of all of the existing piles, thus forming a new pile.\\nWhen there are no more cards remaining to deal, the game ends.The object of the game is to finish with as few piles as possible. The difference with the patience sorting algorithm is that there is no requirement to place a new card on the leftmost pile where it is allowed. Patience sorting constitutes a greedy strategy for playing this game.\\nAldous and Diaconis suggest defining 9 or fewer piles as a winning outcome for n = 52, which happens with approximately 5% probability.\\n\\nAlgorithm for finding a longest increasing subsequence\\nFirst, execute the sorting algorithm as described above. The number of piles is the length of a longest subsequence. Whenever a card is placed on top of a pile, put a back-pointer to the top card in the previous pile (that, by assumption, has a lower value than the new card has). In the end, follow the back-pointers from the top card in the last pile to recover a decreasing subsequence of the longest length; its reverse is an answer to the longest increasing subsequence algorithm.\\nS. Bespamyatnikh and M. Segal give a description of an efficient implementation of the algorithm, incurring no additional asymptotic cost over the sorting one (as the back-pointers storage, creation and traversal require linear time and space). They further show how to report all the longest increasing subsequences from the same resulting data structures.\\n\\nHistory\\nPatience sorting was named by C. L. Mallows, who attributed its invention to A.S.C. Ross in the early 1960s.\\nAccording to Aldous and Diaconis, patience sorting was first recognized as an algorithm to compute the longest increasing subsequence length by Hammersley. A.S.C. Ross and independently Robert W. Floyd recognized it as a sorting algorithm. Initial analysis was done by Mallows. Floyd's game was developed by Floyd in correspondence with Donald Knuth.\\n\\nUse\\nThe patience sorting algorithm can be applied to process control. Within a series of measurements, the existence of a long increasing subsequence can be used as a trend marker. A 2002 article in SQL Server magazine includes a SQL implementation, in this context, of the patience sorting algorithm for the length of the longest increasing subsequence.\\n\\n\\n== References ==\"}, {\"Polyphase merge sort\": \"A polyphase merge sort is a variation of a bottom-up merge sort that sorts a list using an initial uneven distribution of sub-lists (runs), primarily used for external sorting, and is more efficient than an ordinary merge sort when there are fewer than eight external working files (such as a tape drive or a file on a hard drive). A polyphase merge sort is not a stable sort.\\n\\nOrdinary merge sort\\nA merge sort splits the records of a dataset into sorted runs of records and then repeatedly merges sorted runs into larger sorted runs until only one run, the sorted dataset, remains.\\nAn ordinary merge sort using four working files organizes them as a pair of input files and a pair of output files. The dataset is distributed evenly between two of the working files, either as sorted runs or in the simplest case, single records, which can be considered to be sorted runs of size 1. Once all of the dataset is transferred to the two working files, those two working files become the input files for the first merge iteration. Each merge iteration merges runs from the two input working files, alternating the merged output between the two output files, again distributing the merged runs evenly between the two output files (until the final merge iteration). Once all of the runs from the two inputs files are merged and output, then the output files become the input files and vice versa for the next merge iteration. The number of runs decreases by a factor of 2 at each iteration, such as 64, 32, 16, 8, 4, 2, 1. For the final merge iteration, the two input files only have one sorted run (1/2 of the dataset) each, and the merged result is a single sorted run (the sorted dataset) on one of the output files. This is also described at Merge sort \\u00a7 Use with tape drives.\\nIf there are only three working files, then an ordinary merge sort merges sorted runs from two working files onto a single working file, then distributes the runs evenly between the two output files. The merge iteration reduces run count by a factor of 2, the redistribute iteration doesn't reduce run count (the factor is 1). Each iteration could be considered to reduce the run count by an average factor of \\u221a2 \\u2248 1.41. If there are 5 working files, then the pattern alternates between a 3 way merge and a 2 way merge, for an average factor of \\u221a6 \\u2248 2.45.\\nIn general, for an even number N of working files, each iteration of an ordinary merge sort reduces run count by a factor of N/2, while for an odd number N of working files, each iteration reduces the run count by an average factor of \\u221a(N2\\u22121)/4 = \\u221aN2\\u22121/2.\\n\\nPolyphase merge\\nFor N < 8 working files, a polyphase merge sort achieves a higher effective run count reduction factor by unevenly distributing sorted runs between N\\u22121 working files (explained in next section). Each iteration merges runs from N\\u22121 working files onto a single output working file. When the end of one of the N\\u22121 working files is reached, then it becomes the new output file and what was the output file becomes one of the N\\u22121 working input files, starting a new iteration of polyphase merge sort. Each iteration merges only a fraction of the dataset (about 1/2 to 3/4), except for the last iteration which merges all of the dataset into a single sorted run. The initial distribution is set up so that only one input working file is emptied at a time, except for the final merge iteration which merges N\\u22121 single runs (of varying size, this is explained next) from the N\\u22121 input working files to the single output file, resulting in a single sorted run, the sorted dataset.\\nFor each polyphase iteration, the total number of runs follows a pattern similar to a reversed Fibonacci numbers of higher order sequence. With 4 files, and a dataset consisting of 57 runs, the total run count on each iteration would be 57, 31, 17, 9, 5, 3, 1. Note that except for the last iteration, the run count reduction factor is a bit less than 2, 57/31, 31/17, 17/9, 9/5, 5/3, 3/1, about 1.84 for a 4 file case, but each iteration except the last reduced the run count while processing about 65% of the dataset, so the run count reduction factor per dataset processed during the intermediate iterations is about 1.84 / 0.65 = 2.83. For a dataset consisting of 57 runs of 1 record each, then after the initial distribution, polyphase merge sort moves 232 records during the 6 iterations it takes to sort the dataset, for an overall reduction factor of 2.70 (this is explained in more detail later).\\nAfter the first polyphase iteration, what was the output file now contains the results of merging N\\u22121 original runs, but the remaining N\\u22122 input working files still contain the remaining original runs, so the second merge iteration produces runs of size (N\\u22121) + (N\\u22122) = (2N \\u2212 3) original runs. The third iteration produces runs of size (4N \\u2212 7) original runs. With 4 files, the first iteration creates runs of size 3 original runs, the second iteration 5 original runs, the third iteration 9 original runs and so on, following the Fibonacci like pattern, 1, 3, 5, 9, 17, 31, 57, ... , so the increase in run size follows the same pattern as the decrease in run count in reverse. In the example case of 4 files and 57 runs of 1 record each, the last iteration merges 3 runs of size 31, 17, 9, resulting in a single sorted run of size 31+17+9 = 57 records, the sorted dataset. An example of the run counts and run sizes for 4 files, 31 records can be found in table 4.3 of.\\n\\nPerfect 3 file polyphase merge sort\\nIt is easiest to look at the polyphase merge starting from its ending conditions and working backwards. At the start of each iteration, there will be two input files and one output file. At the end of the iteration, one input file will have been completely consumed and will become the output file for the next iteration. The current output file will become an input file for the next iteration. The remaining files (just one in the 3 file case) have only been partially consumed and their remaining runs will be input for the next iteration.\\nFile 1 just emptied and became the new output file. One run is left on each input tape, and merging those runs together will make the sorted file.\\n\\nFile 1 (out):                                           <1 run> *        (the sorted file)\\nFile 2 (in ): ... | <1 run> *               -->     ... <1 run> | *          (consumed)\\nFile 3 (in ):     | <1 run> *                           <1 run> | *          (consumed)\\n\\n...  possible runs that have already been read\\n|    marks the read pointer of the file\\n*    marks end of file\\n\\nStepping back to the previous iteration, we were reading from 1 and 2. One run is merged from 1 and 2 before file 1 goes empty.  Notice that file 2 is not completely consumed\\u2014it has one run left to match the final merge (above).\\n\\nFile 1 (in ): ... | <1 run> *                      ... <1 run> | *\\nFile 2 (in ):     | <2 run> *           -->            <1 run> | <1 run> *\\nFile 3 (out):                                          <1 run> *\\n\\nStepping back another iteration, 2 runs are merged from 1 and 3 before file 3 goes empty.\\n\\nFile 1 (in ):     | <3 run>                        ... <2 run> | <1 run> *\\nFile 2 (out):                               -->        <2 run> *\\nFile 3 (in ): ... | <2 run> *                          <2 run> | *\\n\\nStepping back another iteration, 3 runs are merged from 2 and 3 before file 2 goes empty.\\n\\nFile 1 (out):                                          <3 run> *\\nFile 2 (in ): ... | <3 run> *               -->    ... <3 run> | *\\nFile 3 (in ):     | <5 run> *                          <3 run> | <2 run> *\\n\\nStepping back another iteration, 5 runs are merged from 1 and 2 before file 1 goes empty.\\n\\nFile 1 (in ): ... | <5 run> *                      ... <5 run> | *\\nFile 2 (in ):     | <8 run> *               -->        <5 run> | <3 run> *\\nFile 3 (out):                                          <5 run> *\\n\\nDistribution for polyphase merge sort\\nLooking at the perfect 3 file case, the number of runs for merged working backwards: 1, 1, 2, 3, 5, ... reveals a Fibonacci sequence. The sequence for more than 3 files is a bit more complicated; for 4 files, starting at the final state and working backwards, the run count pattern is {1,0,0,0}, {0,1,1,1}, {1,0,2,2}, {3,2,0,4}, {7,6,4,0}, {0,13,11,7}, {13,0,24,20}, ... .\\nFor everything to work out optimally, the last merge phase should have exactly one run on each input file.  If any input file has more than one run, then another phase would be required. Consequently, the polyphase merge sort needs to be clever about the initial distribution of the input data's runs to the initial output files.  For example, an input file with 13 runs would write 5 runs to file 1 and 8 runs to file 2.\\nIn practice, the input file will not have the exact number of runs needed for a perfect distribution. One way to deal with this is by padding the actual distribution with imaginary \\\"dummy runs\\\" to simulate an ideal run distribution. A dummy run behaves like a run with no records in it. Merging one or more dummy runs with one or more real runs just merges the real runs, and merging one or more dummy runs with no real runs results in a single dummy run. Another approach is to emulate dummy runs as needed during the merge operations.\\\"Optimal\\\" distribution algorithms require knowing the number of runs in advance. Otherwise, in the more common case where the number of runs is not known in advance, \\\"near optimal\\\" distribution algorithms are used. Some distribution algorithms include rearranging runs. If the number of runs is known in advance, only a partial distribution is needed before starting the merge phases. For example, consider the 3 file case, starting with n runs in File_1. Define Fi = Fi\\u22121 + Fi\\u22122 as the ith Fibonacci number.  If n = Fi, then move Fi\\u22122 runs to File_2, leaving Fi\\u22121 runs remaining on File_1, a perfect run distribution. If Fi < n < Fi+1, move n\\u2212Fi runs to File_2 and Fi+1\\u2212n runs to File_3. The first merge iteration merges n\\u2212Fi runs from File_1 and File_2, appending the n\\u2212Fi merged runs to the Fi+1\\u2212n runs already moved to File_3. File_1 ends up with Fi\\u22122 runs remaining, File_2 is emptied, and File_3 ends up with Fi\\u22121 runs, again a perfect run distribution. For 4 or more files, the math is more complicated, but the concept is the same.\\n\\nComparison versus ordinary merge sort\\nAfter the initial distribution, an ordinary merge sort using 4 files will sort 16 single record runs in 4 iterations of the entire dataset, moving a total of 64 records in order to sort the dataset after the initial distribution. A polyphase merge sort using 4 files will sort 17 single record runs in 4 iterations, but since each iteration but the last iteration only moves a fraction of the dataset, it only moves a total of 48 records in order to sort the dataset after the initial distribution. In this case, ordinary merge sort factor is 2.0, while polyphase overall factor is \\u22482.73.\\nTo explain how the reduction factor is related to sort performance, the reduction factor equations are:\\n\\nreduction_factor = exp(number_of_runs*log(number_of_runs)/run_move_count)\\nrun_move_count = number_of_runs * log(number_of_runs)/log(reduction_factor)\\nrun_move_count = number_of_runs * log_reduction_factor(number_of_runs)\\n\\nUsing the run move count equation for the above examples: \\n\\nordinary merge sort \\u2192 \\n  \\n    \\n      \\n        16\\n        \\u00d7\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        16\\n        )\\n        =\\n        64\\n      \\n    \\n    {\\\\displaystyle 16\\\\times \\\\log _{2}(16)=64}\\n  ,\\npolyphase merge sort \\u2192 \\n  \\n    \\n      \\n        17\\n        \\u00d7\\n        \\n          log\\n          \\n            2.73\\n          \\n        \\n        \\u2061\\n        (\\n        17\\n        )\\n        =\\n        48\\n      \\n    \\n    {\\\\displaystyle 17\\\\times \\\\log _{2.73}(17)=48}\\n  .Here is a table of effective reduction factors for polyphase and ordinary merge sort listed by number of files, based on actual sorts of a few million records. This table roughly corresponds to the reduction factor per dataset moved tables shown in fig 3 and fig 4 of polyphase merge sort.pdf\\n\\n# files\\n|     average fraction of data per iteration\\n|     |     polyphase reduction factor on ideal sized data\\n|     |     |     ordinary reduction factor on ideal sized data\\n|     |     |     |\\n3     .73   1.94  1.41  (sqrt  2)\\n4     .63   2.68  2.00\\n5     .58   3.20  2.45  (sqrt  6)\\n6     .56   3.56  3.00\\n7     .55   3.80  3.46  (sqrt 12)\\n8     .54   3.95  4.00\\n9     .53   4.07  4.47  (sqrt 20)\\n10    .53   4.15  5.00\\n11    .53   4.22  5.48  (sqrt 30)\\n12    .53   4.28  6.00\\n32    .53   4.87 16.00\\n\\nIn general, polyphase merge sort is better than ordinary merge sort when there are fewer than 8 files, while ordinary merge sort starts to become better at around 8 or more files.\\n\\nReferences\\nFurther reading\\nBradley, James (1982), File and Data Base Techniques, Holt, Rinehart and Winston, ISBN 0-03-058673-9\\nReynolds, Samuel W. (August 1961), \\\"A generalized polyphase merge algorithm\\\", Communications of the ACM, New York, NY: ACM, 4 (8): 347\\u2013349, doi:10.1145/366678.366689, S2CID 28416100\\nSedgewick, Robert (1983), Algorithms, Addison-Wesley, pp. 163\\u2013165, ISBN 0-201-06672-6\\n\\n\\n== External links ==\"}, {\"Proportion extend sort\": \"Proportion extend sort (abbreviated as PESort) is an in-place, comparison-based sorting algorithm which attempts to improve on the performance, particularly the worst-case performance, of quicksort.\\nThe basic partitioning operation in quicksort has a linear access pattern which is extremely efficient on modern memory hierarchies, but the performance of the algorithm is critically dependent on the choice of a pivot value.  A good pivot will divide the data to be sorted into nearly-equal halves.  A poor choice will result in a grossly lopsided division, leaving one part almost as large as the original problem and causing O(n2) performance.\\nProportion extend sort begins with a sorted prefix of k elements, then uses the median of that sample to partition the following pk elements.  By bounding the size ratio p between the sample and the data being partitioned (i.e. the proportion by which the sorted prefix is extended), the imbalance is limited.  In this, it has some similarities to samplesort.\\n\\nHistory\\nProportion extend sort was published by Jing-Chao Chen in 2001 as an improvement on his earlier proportion split sort design.  Its average-case performance, which was only experimentally measured in the original paper, was analyzed by Richard Cole and David C. Kandathil in 2004 and by Chen in 2006, and shown to require log2n + O(n) comparisons on average.  A slightly refined variant, symmetry partition sort, was published in 2007.\\n\\nAlgorithm\\nThe algorithm begins with an array divided into a sorted part S adjacent to an unsorted part U.  (The original proportion extend sort always had the sorted part precede the unsorted part; the symmetric variant allows either order.)  It is possible to begin with the first element as the sorted part (a single element is always sorted), or to sort a small number of elements using a simpler insertion sort.  The initially sorted elements may also be taken from across the array to improve performance in the case of pre-sorted data.\\nNext, and most critically, the length of the unsorted part |U| is bounded to a multiple p of the length of the sorted part |S|.  Specifically, if |U| > p2|S|, then recursively sort S and the adjacent p|S| elements of U, make the result (p+1 times longer than the original) the new S, and repeat until the condition is satisfied.\\nIf there is no limit on the unsorted part (p=\\u221e), then the algorithm is equivalent to quicksort.  If the unsorted part is of length 1 (p=0, almost), then the algorithm is equivalent to binary insertion sort.  Values around p\\u224816 give the best average-case performance, competitive with quicksort,:\\u200a764\\u200a while smaller values improve worst-case performance.Eliezer Albacea published a similar algorithm in 1995 called Leapfrogging samplesort where the size is limited so |U| \\u2264 |S|+1, later generalized to (2k\\u22121)(|S|+1).The sorted part of the array is divided in half (at the median), and one half is moved (by exchanging it with unsorted elements) to the far end of the array, so we have an initial partially-partitioned array of the form LUR, where L is the left half of the sorted part, U is the bounded-length unsorted part, and R is the right half of the sorted part.\\nThen the standard quicksort partitioning step is performed on U, dividing it (in place) into UL and UR.  UL and UR are not sorted, but every element of UL is less than or equal to the median, and every element of UR is greater or equal.  The final result LULURR consists of two arrays of the necessary form (a sorted part adjacent to an unsorted part) and are sorted recursively.\\nLeapfrogging samplesort and the original proportion extend sort have the sorted part always precede the unsorted part, achieved by partitioning U before moving R, resulting in LRULUR, and then exchanging R with the end of UL, resulting in LULRUR.  While the symmetric version is a bit trickier, it has the advantage that the L and R parts act as sentinel values for the partitioning loops, eliminating the need to test in the loop if the bounds of U have been reached.[1]Most of the implementation refinements used for quicksort can be applied, including techniques for detecting and efficiently handling mostly-sorted inputs.  In particular, sub-sorts below a certain size threshold are usually implemented using a simple insertion sort.\\nAs with quicksort, the number of recursive levels can be limited to log2n if the smaller sub-sort is done first and the larger is implemented as a tail call.  Unlike quicksort, the number of levels is bounded by O(log n) even if this is not done.:\\u200a781\\n\\nNotes\\nReferences\\nExternal links\\nhttps://github.com/jingchaochen/Symmetry-Partition-Sort Example code\"}, {\"Quicksort\": \"Quicksort is an efficient, general-purpose sorting algorithm. Quicksort was developed by British computer scientist Tony Hoare in 1959 and published in 1961. It is still a commonly used algorithm for sorting. Overall, it is slightly faster than merge sort and heapsort for randomized data, particularly on larger distributions.Quicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. For this reason, it is sometimes called partition-exchange sort. The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting.\\nQuicksort is a comparison sort, meaning that it can sort items of any type for which a \\\"less-than\\\" relation (formally, a total order) is defined. Most implementations of quicksort are not stable, meaning that the relative order of equal sort items is not preserved.\\nMathematical analysis of quicksort shows that, on average, the algorithm takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        \\n          n\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log {n})}\\n   comparisons to sort n items. In the worst case, it makes \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   comparisons.\\n\\nHistory\\nThe quicksort algorithm was developed in 1959 by Tony Hoare while he was a visiting student at Moscow State University. At that time, Hoare was working on a machine translation project for the National Physical Laboratory. As a part of the translation process, he needed to sort the words in Russian sentences before looking them up in a Russian-English dictionary, which was in alphabetical order on magnetic tape. After recognizing that his first idea, insertion sort, would be slow, he came up with a new idea. He wrote the partition part in Mercury Autocode but had trouble dealing with the list of unsorted segments. On return to England, he was asked to write code for Shellsort. Hoare mentioned to his boss that he knew of a faster algorithm and his boss bet a sixpence that he did not. His boss ultimately accepted that he had lost the bet. Later, Hoare learned about ALGOL and its ability to do recursion that enabled him to publish the code in Communications of the Association for Computing Machinery, the premier computer science journal of the time.Quicksort gained widespread adoption, appearing, for example, in Unix as the default library sort subroutine. Hence, it lent its name to the C standard library subroutine qsort and in the reference implementation of Java.\\nRobert Sedgewick's PhD thesis in 1975 is considered a milestone in the study of Quicksort where he resolved many open problems related to the analysis of various pivot selection schemes including Samplesort, adaptive partitioning by Van Emden as well as derivation of expected number of comparisons and swaps. Jon Bentley and Doug McIlroy in 1993 incorporated various improvements for use in programming libraries, including a technique to deal with equal elements and a pivot scheme known as pseudomedian of nine, where a sample of nine elements is divided into groups of three and then the median of the three medians from three groups is chosen. Bentley described another simpler and compact partitioning scheme in his book Programming Pearls that he attributed to Nico Lomuto. Later Bentley wrote that he used Hoare's version for years but never really understood it but Lomuto's version was simple enough to prove correct. Bentley described Quicksort as the \\\"most beautiful code I had ever written\\\" in the same essay. Lomuto's partition scheme was also popularized by the textbook Introduction to Algorithms although it is inferior to Hoare's scheme because it does three times more swaps on average and degrades to O(n2) runtime when all elements are equal. McIlroy would further produce an AntiQuicksort (aqsort) function in 1998, which consistently drives even his 1993 variant of Quicksort into quadratic behavior by producing adversarial data on-the-fly.\\n\\nAlgorithm\\nQuicksort is a type of divide and conquer algorithm for sorting an array, based on a partitioning routine; the details of this partitioning can vary somewhat, so that quicksort is really a family of closely related algorithms. Applied to a range of at least two elements, partitioning produces a division into two consecutive non empty sub-ranges, in such a way that no element of the first sub-range is greater than any element of the second sub-range. After applying this partition, quicksort then recursively sorts the sub-ranges, possibly after excluding from them an element at the point of division that is at this point known to be already in its final location. Due to its recursive nature, quicksort (like  the partition routine) has to be formulated so as to be callable for a range within a larger array, even if the ultimate goal is to sort a complete array. The steps for in-place quicksort are:\\n\\nIf the range has fewer than two elements, return immediately as there is nothing to do. Possibly for other very short lengths a special-purpose sorting method is applied and the remainder of these steps skipped.\\nOtherwise pick a value, called a pivot, that occurs in the range (the precise manner of choosing depends on the partition routine, and can involve randomness).\\nPartition the range: reorder its elements, while determining a point of division, so that all elements with values less than the pivot come before the division, while all elements with values greater than the pivot come after it; elements that are equal to the pivot can go either way. Since at least one instance of the pivot is present,  most partition routines ensure that the value that ends up at the point of division is equal to the pivot, and is now in its final position (but termination of quicksort does not depend on this, as long as sub-ranges strictly smaller than the original are produced).\\nRecursively apply the quicksort to the sub-range up to the point of division and to the sub-range after it, possibly excluding from both ranges the element equal to the pivot at the point of division. (If the partition produces a possibly larger sub-range near the boundary where all elements are known to be equal to the pivot, these can be excluded as well.)The choice of partition routine (including the pivot selection) and other details not entirely specified above can affect the algorithm's performance, possibly to a great extent for specific input arrays. In discussing the efficiency of quicksort, it is therefore necessary to specify these choices first. Here we mention two specific partition methods.\\n\\nLomuto partition scheme\\nThis scheme is attributed to Nico Lomuto and popularized by Bentley in his book Programming Pearls and Cormen et al. in their book Introduction to Algorithms. In most formulations this scheme chooses as the pivot the last element in the array. The algorithm maintains index i as it scans the array using another index j such that the elements at lo through i-1 (inclusive) are less than the pivot, and the elements at i through j (inclusive) are equal to or greater than the pivot. As this scheme is more compact and easy to understand, it is frequently used in introductory material, although it is less efficient than Hoare's original scheme e.g., when all elements are equal. The complexity of Quicksort with this scheme degrades to O(n2) when the array is already in order, due to the partition being the worst possible one. There have been various variants proposed to boost performance including various ways to select the pivot, deal with equal elements, use other sorting algorithms such as insertion sort for small arrays, and so on. In pseudocode, a quicksort that sorts elements at lo through hi (inclusive) of an array A can be expressed as:\\n// Sorts a (portion of an) array, divides it into partitions, then sorts those\\nalgorithm quicksort(A, lo, hi) is \\n  // Ensure indices are in correct order\\n  if lo >= hi || lo < 0 then \\n    return\\n    \\n  // Partition array and get the pivot index\\n  p := partition(A, lo, hi) \\n      \\n  // Sort the two partitions\\n  quicksort(A, lo, p - 1) // Left side of pivot\\n  quicksort(A, p + 1, hi) // Right side of pivot\\n\\n// Divides array into two partitions\\nalgorithm partition(A, lo, hi) is \\n  pivot := A[hi] // Choose the last element as the pivot\\n\\n  // Temporary pivot index\\n  i := lo - 1\\n\\n  for j := lo to hi - 1 do \\n    // If the current element is less than or equal to the pivot\\n    if A[j] <= pivot then \\n      // Move the temporary pivot index forward\\n      i := i + 1\\n      // Swap the current element with the element at the temporary pivot index\\n      swap A[i] with A[j]\\n\\n  // Move the pivot element to the correct pivot position (between the smaller and larger elements)\\n  i := i + 1\\n  swap A[i] with A[hi]\\n  return i // the pivot index\\n\\nSorting the entire array is accomplished by quicksort(A, 0, length(A) - 1).\\n\\nHoare partition scheme\\nThe original partition scheme described by Tony Hoare uses two pointers (indices into the range) that start at both ends of the array being partitioned, then move toward each other, until they detect an inversion: a pair of elements, one greater than the bound (Hoare's terms for the pivot value) at the first pointer, and one less than the bound at the second pointer; if at this point the first pointer is still before the second, these elements are in the wrong order relative to each other, and they are then exchanged. After this the pointers are moved inwards, and the search for an inversion is repeated; when eventually the pointers cross (the first points after the second), no exchange is performed; a valid partition is found, with the point of division between the crossed pointers (any entries that might be strictly between the crossed pointers are equal to the pivot and can be excluded from both sub-ranges formed). With this formulation it is possible that one sub-range turns out to be the whole original range, which would prevent the algorithm from advancing. Hoare therefore stipulates that at the end, the sub-range containing the pivot element (which still is at its original position) can be decreased in size by excluding that pivot, after (if necessary) exchanging it with the sub-range element closest to the separation; thus, termination of quicksort is ensured.\\nWith respect to this original description, implementations often make minor but important variations. Notably, the scheme as presented below includes elements equal to the pivot among the candidates for an inversion (so \\\"greater than or equal\\\" and \\\"less than or equal\\\" tests are used instead of \\\"greater than\\\" and \\\"less than\\\" respectively; since the formulation uses do...while rather than repeat...until which is actually reflected by the use of strict comparison operators). While there is no reason to exchange elements equal to the bound, this change allows tests on the pointers themselves to be omitted, which are otherwise needed to ensure they do not run out of range. Indeed, since at least one instance of the pivot value is present in the range, the first advancement of either pointer cannot pass across this instance if an inclusive test is used; once an exchange is performed, these exchanged elements are now both strictly ahead of the pointer that found them, preventing that pointer from running off. (The latter is true independently of the test used, so it would be possible to use the inclusive test only when looking for the first inversion. However, using an inclusive test throughout also ensures that a division near the middle is found when all elements in the range are equal, which gives an important efficiency gain for sorting arrays with many equal elements.) The risk of producing a non-advancing separation is avoided in a different manner than described by Hoare. Such a separation can only result when no inversions are found, with both pointers advancing to the pivot element at the first iteration (they are then considered to have crossed, and no exchange takes place). The division returned is after the final position of the second pointer, so the case to avoid is where the pivot is the final element of the range and all others are smaller than it. Therefore, the pivot choice must avoid the final element (in Hoare's description it could be any element in the range); this is done here by rounding down the middle position, using the floor function. This illustrates that the argument for correctness of an implementation of the Hoare partition scheme can be subtle, and it is easy to get it wrong.\\nIn pseudocode,\\n// Sorts a (portion of an) array, divides it into partitions, then sorts those\\nalgorithm quicksort(A, lo, hi) is \\n  if lo >= 0 && hi >= 0 && lo < hi then\\n    p := partition(A, lo, hi) \\n    quicksort(A, lo, p) // Note: the pivot is now included\\n    quicksort(A, p + 1, hi) \\n\\n// Divides array into two partitions\\nalgorithm partition(A, lo, hi) is \\n  // Pivot value\\n  pivot := A[ floor((hi - lo)/2) + lo ] // The value in the middle of the array\\n\\n  // Left index\\n  i := lo - 1 \\n\\n  // Right index\\n  j := hi + 1\\n\\n  loop forever \\n    // Move the left index to the right at least once and while the element at\\n    // the left index is less than the pivot\\n    do i := i + 1 while A[i] < pivot\\n    \\n    // Move the right index to the left at least once and while the element at\\n    // the right index is greater than the pivot\\n    do j := j - 1 while A[j] > pivot\\n\\n    // If the indices crossed, return\\n    if i >= j then return j\\n    \\n    // Swap the elements at the left and right indices\\n    swap A[i] with A[j]\\n\\nThe entire array is sorted by quicksort(A, 0, length(A) - 1).\\nHoare's scheme is more efficient than Lomuto's partition scheme because it does three times fewer swaps on average. Also, as mentioned, the implementation given creates a balanced partition even when all values are equal., which Lomuto's scheme does not. Like Lomuto's partition scheme, Hoare's partitioning also would cause Quicksort to degrade to O(n2) for already sorted input, if the pivot was chosen as the first or the last element. With the middle element as the pivot, however, sorted data results with (almost) no swaps in equally sized partitions leading to best case behavior of Quicksort, i.e. O(n log(n)). Like others, Hoare's partitioning doesn't produce a stable sort. In this scheme, the pivot's final location is not necessarily at the index that is returned, as the pivot and elements equal to the pivot can end up anywhere within the partition after a partition step, and may not be sorted until the base case of a partition with a single element is reached via recursion. The next two segments that the main algorithm recurs on are (lo..p) (elements \\u2264 pivot) and (p+1..hi) (elements \\u2265 pivot) as opposed to (lo..p-1) and (p+1..hi) as in Lomuto's scheme.Subsequent recursions (expansion on previous paragraph)\\nLet's expand a little bit on the next two segments that the main algorithm recurs on. Because we are using strict comparators (>, <) in the \\\"do...while\\\" loops to prevent ourselves from running out of range, there's a chance that the pivot itself gets swapped with other elements in the partition function. Therefore, the index returned in the partition function isn't necessarily where the actual pivot is. Consider the example of [5, 2, 3, 1, 0], following the scheme, after the first partition the array becomes [0, 2, 1, 3, 5], the \\\"index\\\" returned is 2, which is the number 1, when the real pivot, the one we chose to start the partition with was the number 3. With this example, we see how it is necessary to include the returned index of the partition function in our subsequent recursions. As a result, we are presented with the choices of either recursing on (lo..p) and (p+1..hi), or (lo..p - 1) and (p..hi). Which of the two options we choose depends on which index (i or j) we return in the partition function when the indices cross, and how we choose our pivot in the partition function (floor v.s. ceiling).\\nLet's first examine the choice of recursing on (lo..p) and (p+1..hi), with the example of sorting an array where multiple identical elements exist [0, 0]. If index i (the \\\"latter\\\" index) is returned after indices cross in the partition function, the index 1 would be returned after the first partition. The subsequent recursion on (lo..p)would be on (0, 1), which corresponds to the exact same array [0, 0]. A non-advancing separation that causes infinite recursion is produced. It is therefore obvious that when recursing on (lo..p) and (p+1..hi), because the left half of the recursion includes the returned index, it is the partition function's job to exclude the \\\"tail\\\" in non-advancing scenarios. Which is to say, index j (the \\\"former\\\" index when indices cross) should be returned instead of i. Going with a similar logic, when considering the example of an already sorted array [0, 1], the choice of pivot needs to be \\\"floor\\\" to ensure that the pointers stop on the \\\"former\\\" instead of the \\\"latter\\\" (with \\\"ceiling\\\" as the pivot, the index 1 would be returned and included in (lo..p) causing infinite recursion). It is for the exact same reason why choice of the last element as pivot must be avoided.\\nThe choice of recursing on (lo..p - 1) and (p..hi) follows the exact same logic as above. Because the right half of the recursion includes the returned index, it is the partition function's job to exclude the \\\"head\\\" in non-advancing scenarios. The index i (the \\\"latter\\\" index after the indices cross) in the partition function needs to be returned, and \\\"ceiling\\\" needs to be chosen as the pivot. The two nuances are clear, again, when considering the examples of sorting an array where multiple identical elements exist ([0, 0]), and an already sorted array [0, 1] respectively. It is noteworthy that with version of recursion, for the same reason, choice of the first element as pivot must be avoided.\\n\\nImplementation issues\\nChoice of pivot\\nIn the very early versions of quicksort, the leftmost element of the partition would often be chosen as the pivot element. Unfortunately, this causes worst-case behavior on already sorted arrays, which is a rather common use-case. The problem was easily solved by choosing either a random index for the pivot, choosing the middle index of the partition or (especially for longer partitions) choosing the median of the first, middle and last element of the partition for the pivot (as recommended by Sedgewick). This \\\"median-of-three\\\" rule counters the case of sorted (or reverse-sorted) input, and gives a better estimate of the optimal pivot (the true median) than selecting any single element, when no information about the ordering of the input is known.\\nMedian-of-three code snippet for Lomuto partition:\\n\\nmid := \\u230a(lo + hi) / 2\\u230b\\nif A[mid] < A[lo]\\n    swap A[lo] with A[mid]\\nif A[hi] < A[lo]\\n    swap A[lo] with A[hi]\\nif A[mid] < A[hi]\\n    swap A[mid] with A[hi]\\npivot := A[hi]\\n\\nIt puts a median into A[hi] first, then that new value of A[hi] is used for a pivot, as in a basic algorithm presented above.\\nSpecifically, the expected number of comparisons needed to sort n elements (see \\u00a7 Analysis of randomized quicksort) with random pivot selection is 1.386 n log n. Median-of-three pivoting brings this down to Cn, 2 \\u2248 1.188 n log n, at the expense of a three-percent increase in the expected number of swaps. An even stronger pivoting rule, for larger arrays, is to pick the ninther, a recursive median-of-three (Mo3), defined as\\nninther(a) = median(Mo3(first 1/3 of a), Mo3(middle 1/3 of a), Mo3(final 1/3 of a))Selecting a pivot element is also complicated by the existence of integer overflow. If the boundary indices of the subarray being sorted are sufficiently large, the na\\u00efve expression for the middle index, (lo + hi)/2, will cause overflow and provide an invalid pivot index. This can be overcome by using, for example, lo + (hi\\u2212lo)/2 to index the middle element, at the cost of more complex arithmetic. Similar issues arise in some other methods of selecting the pivot element.\\n\\nRepeated elements\\nWith a partitioning algorithm such as the Lomuto partition scheme described above (even one that chooses good pivot values), quicksort exhibits poor performance for inputs that contain many repeated elements. The problem is clearly apparent when all the input elements are equal: at each recursion, the left partition is empty (no input values are less than the pivot), and the right partition has only decreased by one element (the pivot is removed). Consequently, the Lomuto partition scheme takes quadratic time to sort an array of equal values. However, with a partitioning algorithm such as the Hoare partition scheme, repeated elements generally results in better partitioning, and although needless swaps of elements equal to the pivot may occur, the running time generally decreases as the number of repeated elements increases (with memory cache reducing the swap overhead). In the case where all elements are equal, Hoare partition scheme needlessly swaps elements, but the partitioning itself is best case, as noted in the Hoare partition section above.\\nTo solve the Lomuto partition scheme problem (sometimes called the Dutch national flag problem), an alternative linear-time partition routine can be used that separates the values into three groups: values less than the pivot, values equal to the pivot, and values greater than the pivot. (Bentley and McIlroy call this a \\\"fat partition\\\" and it was already implemented in the qsort of Version 7 Unix.) The values equal to the pivot are already sorted, so only the less-than and greater-than partitions need to be recursively sorted. In pseudocode, the quicksort algorithm becomes\\n\\nalgorithm quicksort(A, lo, hi) is\\n    if lo < hi then\\n        p := pivot(A, lo, hi)\\n        left, right := partition(A, p, lo, hi)  // note: multiple return values\\n        quicksort(A, lo, left - 1)\\n        quicksort(A, right + 1, hi)\\n\\nThe partition algorithm returns indices to the first ('leftmost') and to the last ('rightmost') item of the middle partition. Every item of the partition is equal to p and is therefore sorted. Consequently, the items of the partition need not be included in the recursive calls to quicksort.\\nThe best case for the algorithm now occurs when all elements are equal (or are chosen from a small set of k \\u226a n elements). In the case of all equal elements, the modified quicksort will perform only two recursive calls on empty subarrays and thus finish in linear time (assuming the partition subroutine takes no longer than linear time).\\n\\nOptimizations\\nTwo other important optimizations, also suggested by Sedgewick and widely used in practice, are:\\nTo make sure at most O(log n) space is used, recur first into the smaller side of the partition, then use a tail call to recur into the other, or update the parameters to no longer include the now sorted smaller side, and iterate to sort the larger side.\\nWhen the number of elements is below some threshold (perhaps ten elements), switch to a non-recursive sorting algorithm such as insertion sort that performs fewer swaps, comparisons or other operations on such small arrays. The ideal 'threshold' will vary based on the details of the specific implementation.\\nAn older variant of the previous optimization: when the number of elements is less than the threshold k, simply stop; then after the whole array has been processed, perform insertion sort on it. Stopping the recursion early leaves the array k-sorted, meaning that each element is at most k positions away from its final sorted position. In this case, insertion sort takes O(kn) time to finish the sort, which is linear if k is a constant.:\\u200a117\\u200a Compared to the \\\"many small sorts\\\" optimization, this version may execute fewer instructions, but it makes suboptimal use of the cache memories in modern computers.\\n\\nParallelization\\nQuicksort's divide-and-conquer formulation makes it amenable to parallelization using task parallelism. The partitioning step is accomplished through the use of a parallel prefix sum algorithm to compute an index for each array element in its section of the partitioned array. Given an array of size n, the partitioning step performs O(n) work in O(log n) time and requires O(n) additional scratch space. After the array has been partitioned, the two partitions can be sorted recursively in parallel. Assuming an ideal choice of pivots, parallel quicksort sorts an array of size n in O(n log n) work in O(log2 n) time using O(n) additional space.\\nQuicksort has some disadvantages when compared to alternative sorting algorithms, like merge sort, which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, it is difficult to parallelize the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.\\nOther more sophisticated parallel sorting algorithms can achieve even better time bounds. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW (concurrent read and concurrent write) PRAM (parallel random-access machine) with n processors by performing partitioning implicitly.\\n\\nFormal analysis\\nWorst-case analysis\\nThe most unbalanced partition occurs when one of the sublists returned by the partitioning routine is of size n \\u2212 1. This may occur if the pivot happens to be the smallest or largest element in the list, or in some implementations (e.g., the Lomuto partition scheme as described above) when all the elements are equal.\\nIf this happens repeatedly in every partition, then each recursive call processes a list of size one less than the previous list. Consequently, we can make n \\u2212 1 nested calls before we reach a list of size 1. This means that the call tree is a linear chain of n \\u2212 1 nested calls. The ith call does O(n \\u2212 i) work to do the partition, and \\n  \\n    \\n      \\n        \\n          \\n            \\u2211\\n            \\n              i\\n              =\\n              0\\n            \\n            \\n              n\\n            \\n          \\n          (\\n          n\\n          \\u2212\\n          i\\n          )\\n          =\\n          O\\n          (\\n          \\n            n\\n            \\n              2\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\textstyle \\\\sum _{i=0}^{n}(n-i)=O(n^{2})}\\n  , so in that case quicksort takes O(n2) time.\\n\\nBest-case analysis\\nIn the most balanced case, each time we perform a partition we divide the list into two nearly equal pieces. This means each recursive call processes a list of half the size. Consequently, we can make only log2 n nested calls before we reach a list of size 1. This means that the depth of the call tree is log2 n. But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only O(n) time all together (each call has some constant overhead, but since there are only O(n) calls at each level, this is subsumed in the O(n) factor). The result is that the algorithm uses only O(n log n) time.\\n\\nAverage-case analysis\\nTo sort an array of n distinct elements, quicksort takes O(n log n) time in expectation, averaged over all n! permutations of n elements with equal probability. Alternatively, if the algorithm selects the pivot uniformly at random from the input array, the same analysis can be used to bound the expected running time for any input sequence; the expectation is then take over the random choices made by the algorithm (Cormen et al., Introduction to Algorithms, Section 7.3).\\nWe list here three common proofs to this claim providing different insights into quicksort's workings.\\n\\nUsing percentiles\\nIf each pivot has rank somewhere in the middle 50 percent, that is, between the 25th percentile and the 75th percentile, then it splits the elements with at least 25% and at most 75% on each side. If we could consistently choose such pivots, we would only have to split the list at most \\n  \\n    \\n      \\n        \\n          log\\n          \\n            4\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        \\u2061\\n        n\\n      \\n    \\n    {\\\\displaystyle \\\\log _{4/3}n}\\n   times before reaching lists of size 1, yielding an O(n log n) algorithm.\\nWhen the input is a random permutation, the pivot has a random rank, and so it is not guaranteed to be in the middle 50 percent. However, when we start from a random permutation, in each recursive call the pivot has a random rank in its list, and so it is in the middle 50 percent about half the time. That is good enough. Imagine that a coin is flipped: heads means that the rank of the pivot is in the middle 50 percent, tail means that it isn't. Now imagine that the coin is flipped over and over until it gets k heads. Although this could take a long time, on average only 2k flips are required, and the chance that the coin won't get k heads after 100k flips is highly improbable (this can be made rigorous using Chernoff bounds). By the same argument, Quicksort's recursion will terminate on average at a call depth of only \\n  \\n    \\n      \\n        2\\n        \\n          log\\n          \\n            4\\n            \\n              /\\n            \\n            3\\n          \\n        \\n        \\u2061\\n        n\\n      \\n    \\n    {\\\\displaystyle 2\\\\log _{4/3}n}\\n  . But if its average call depth is O(log n), and each level of the call tree processes at most n elements, the total amount of work done on average is the product, O(n log n). The algorithm does not have to verify that the pivot is in the middle half\\u2014if we hit it any constant fraction of the times, that is enough for the desired complexity.\\n\\nUsing recurrences\\nAn alternative approach is to set up a recurrence relation for the T(n) factor, the time needed to sort a list of size n. In the most unbalanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size 0 and n\\u22121, so the recurrence relation is\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        =\\n        O\\n        (\\n        n\\n        )\\n        +\\n        T\\n        (\\n        0\\n        )\\n        +\\n        T\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        =\\n        O\\n        (\\n        n\\n        )\\n        +\\n        T\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle T(n)=O(n)+T(0)+T(n-1)=O(n)+T(n-1).}\\n  This is the same relation as for insertion sort and selection sort, and it solves to worst case T(n) = O(n2).\\nIn the most balanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size n/2, so the recurrence relation is\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        =\\n        O\\n        (\\n        n\\n        )\\n        +\\n        2\\n        T\\n        \\n          (\\n          \\n            \\n              n\\n              2\\n            \\n          \\n          )\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle T(n)=O(n)+2T\\\\left({\\\\frac {n}{2}}\\\\right).}\\n  The master theorem for divide-and-conquer recurrences tells us that T(n) = O(n log n).\\nThe outline of a formal proof of the O(n log n) expected time complexity follows. Assume that there are no duplicates as duplicates could be handled with linear time pre- and post-processing, or considered cases easier than the analyzed. When the input is a random permutation, the rank of the pivot is uniform random from 0 to n \\u2212 1. Then the resulting parts of the partition have sizes i and n \\u2212 i \\u2212 1, and i is uniform random from 0 to n \\u2212 1. So, averaging over all possible splits and noting that the number of comparisons for the partition is n \\u2212 1, the average number of comparisons over all permutations of the input sequence can be estimated accurately by solving the recurrence relation:\\n\\n  \\n    \\n      \\n        C\\n        (\\n        n\\n        )\\n        =\\n        n\\n        \\u2212\\n        1\\n        +\\n        \\n          \\n            1\\n            n\\n          \\n        \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        (\\n        C\\n        (\\n        i\\n        )\\n        +\\n        C\\n        (\\n        n\\n        \\u2212\\n        i\\n        \\u2212\\n        1\\n        )\\n        )\\n        =\\n        n\\n        \\u2212\\n        1\\n        +\\n        \\n          \\n            2\\n            n\\n          \\n        \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        C\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle C(n)=n-1+{\\\\frac {1}{n}}\\\\sum _{i=0}^{n-1}(C(i)+C(n-i-1))=n-1+{\\\\frac {2}{n}}\\\\sum _{i=0}^{n-1}C(i)}\\n  \\n\\n  \\n    \\n      \\n        n\\n        C\\n        (\\n        n\\n        )\\n        =\\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        +\\n        2\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        C\\n        (\\n        i\\n        )\\n      \\n    \\n    {\\\\displaystyle nC(n)=n(n-1)+2\\\\sum _{i=0}^{n-1}C(i)}\\n  \\n  \\n    \\n      \\n        n\\n        C\\n        (\\n        n\\n        )\\n        \\u2212\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        C\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        =\\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\u2212\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        (\\n        n\\n        \\u2212\\n        2\\n        )\\n        +\\n        2\\n        C\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle nC(n)-(n-1)C(n-1)=n(n-1)-(n-1)(n-2)+2C(n-1)}\\n  \\n  \\n    \\n      \\n        n\\n        C\\n        (\\n        n\\n        )\\n        =\\n        (\\n        n\\n        +\\n        1\\n        )\\n        C\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        +\\n        2\\n        n\\n        \\u2212\\n        2\\n      \\n    \\n    {\\\\displaystyle nC(n)=(n+1)C(n-1)+2n-2}\\n  \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      )\\n                    \\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      \\u2212\\n                      1\\n                      )\\n                    \\n                    n\\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      (\\n                      n\\n                      +\\n                      1\\n                      )\\n                    \\n                  \\n                \\n                \\u2264\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      \\u2212\\n                      1\\n                      )\\n                    \\n                    n\\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      \\u2212\\n                      2\\n                      )\\n                    \\n                    \\n                      n\\n                      \\u2212\\n                      1\\n                    \\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    n\\n                  \\n                \\n                \\u2212\\n                \\n                  \\n                    2\\n                    \\n                      (\\n                      n\\n                      \\u2212\\n                      1\\n                      )\\n                      n\\n                    \\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n                \\u2264\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      n\\n                      \\u2212\\n                      2\\n                      )\\n                    \\n                    \\n                      n\\n                      \\u2212\\n                      1\\n                    \\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    n\\n                  \\n                \\n                +\\n                \\n                  \\n                    2\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                 \\n                 \\n                \\u22ee\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    \\n                      C\\n                      (\\n                      1\\n                      )\\n                    \\n                    2\\n                  \\n                \\n                +\\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    2\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    2\\n                    \\n                      i\\n                      +\\n                      1\\n                    \\n                  \\n                \\n                \\u2264\\n                2\\n                \\n                  \\u2211\\n                  \\n                    i\\n                    =\\n                    1\\n                  \\n                  \\n                    n\\n                    \\u2212\\n                    1\\n                  \\n                \\n                \\n                  \\n                    1\\n                    i\\n                  \\n                \\n                \\u2248\\n                2\\n                \\n                  \\u222b\\n                  \\n                    1\\n                  \\n                  \\n                    n\\n                  \\n                \\n                \\n                  \\n                    1\\n                    x\\n                  \\n                \\n                \\n                  d\\n                \\n                x\\n                =\\n                2\\n                ln\\n                \\u2061\\n                n\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}{\\\\frac {C(n)}{n+1}}&={\\\\frac {C(n-1)}{n}}+{\\\\frac {2}{n+1}}-{\\\\frac {2}{n(n+1)}}\\\\leq {\\\\frac {C(n-1)}{n}}+{\\\\frac {2}{n+1}}\\\\\\\\&={\\\\frac {C(n-2)}{n-1}}+{\\\\frac {2}{n}}-{\\\\frac {2}{(n-1)n}}+{\\\\frac {2}{n+1}}\\\\leq {\\\\frac {C(n-2)}{n-1}}+{\\\\frac {2}{n}}+{\\\\frac {2}{n+1}}\\\\\\\\&\\\\ \\\\ \\\\vdots \\\\\\\\&={\\\\frac {C(1)}{2}}+\\\\sum _{i=2}^{n}{\\\\frac {2}{i+1}}\\\\leq 2\\\\sum _{i=1}^{n-1}{\\\\frac {1}{i}}\\\\approx 2\\\\int _{1}^{n}{\\\\frac {1}{x}}\\\\mathrm {d} x=2\\\\ln n\\\\end{aligned}}}\\n  Solving the recurrence gives C(n) = 2n ln n \\u2248 1.39n log2 n.\\nThis means that, on average, quicksort performs only about 39% worse than in its best case. In this sense, it is closer to the best case than the worst case. A comparison sort cannot use less than log2(n!) comparisons on average to sort n items (as explained in the article Comparison sort) and in case of large n, Stirling's approximation yields log2(n!) \\u2248 n(log2 n \\u2212 log2 e), so quicksort is not much worse than an ideal comparison sort. This fast average runtime is another reason for quicksort's practical dominance over other sorting algorithms.\\n\\nUsing a binary search tree\\nThe following binary search tree (BST) corresponds to each execution of quicksort: the initial pivot is the root node; the pivot of the left half is the root of the left subtree, the pivot of the right half is the root of the right subtree, and so on. The number of comparisons of the execution of quicksort equals the number of comparisons during the construction of the BST by a sequence of insertions. So, the average number of comparisons for randomized quicksort equals the average cost of constructing a BST when the values inserted \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},\\\\ldots ,x_{n})}\\n   form a random permutation.\\nConsider a BST created by insertion of a sequence \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},\\\\ldots ,x_{n})}\\n   of values forming a random permutation. Let C denote the cost of creation of the BST. We have \\n  \\n    \\n      \\n        C\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        \\n          \\u2211\\n          \\n            j\\n            <\\n            i\\n          \\n        \\n        \\n          c\\n          \\n            i\\n            ,\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C=\\\\sum _{i}\\\\sum _{j<i}c_{i,j}}\\n  , where \\n  \\n    \\n      \\n        \\n          c\\n          \\n            i\\n            ,\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle c_{i,j}}\\n   is a binary random variable expressing whether during the insertion of \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   there was a comparison to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}}\\n  .\\nBy linearity of expectation, the expected value \\n  \\n    \\n      \\n        E\\n        \\u2061\\n        [\\n        C\\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {E} [C]}\\n   of C is \\n  \\n    \\n      \\n        E\\n        \\u2061\\n        [\\n        C\\n        ]\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        \\n          \\u2211\\n          \\n            j\\n            <\\n            i\\n          \\n        \\n        Pr\\n        (\\n        \\n          c\\n          \\n            i\\n            ,\\n            j\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {E} [C]=\\\\sum _{i}\\\\sum _{j<i}\\\\Pr(c_{i,j})}\\n  .\\nFix i and j<i. The values \\n  \\n    \\n      \\n        \\n          \\n            x\\n            \\n              1\\n            \\n          \\n          ,\\n          \\n            x\\n            \\n              2\\n            \\n          \\n          ,\\n          \\u2026\\n          ,\\n          \\n            x\\n            \\n              j\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {x_{1},x_{2},\\\\ldots ,x_{j}}}\\n  , once sorted, define j+1 intervals. The core structural observation is that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is compared to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}}\\n   in the algorithm if and only if \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   falls inside one of the two intervals adjacent to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}}\\n  .\\nObserve that since \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},\\\\ldots ,x_{n})}\\n   is a random permutation, \\n  \\n    \\n      \\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\u2026\\n        ,\\n        \\n          x\\n          \\n            j\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (x_{1},x_{2},\\\\ldots ,x_{j},x_{i})}\\n   is also a random permutation, so the probability that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{i}}\\n   is adjacent to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{j}}\\n   is exactly \\n  \\n    \\n      \\n        \\n          \\n            2\\n            \\n              j\\n              +\\n              1\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {2}{j+1}}}\\n  .\\nWe end with a short calculation:\\n\\n  \\n    \\n      \\n        E\\n        \\u2061\\n        [\\n        C\\n        ]\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n          \\n        \\n        \\n          \\u2211\\n          \\n            j\\n            <\\n            i\\n          \\n        \\n        \\n          \\n            2\\n            \\n              j\\n              +\\n              1\\n            \\n          \\n        \\n        =\\n        O\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n              \\n            \\n            log\\n            \\u2061\\n            i\\n          \\n          )\\n        \\n        =\\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {E} [C]=\\\\sum _{i}\\\\sum _{j<i}{\\\\frac {2}{j+1}}=O\\\\left(\\\\sum _{i}\\\\log i\\\\right)=O(n\\\\log n).}\\n\\nSpace complexity\\nThe space used by quicksort depends on the version used.\\nThe in-place version of quicksort has a space complexity of O(log n), even in the worst case, when it is carefully implemented using the following strategies.\\n\\nIn-place partitioning is used. This unstable partition requires O(1) space.\\nAfter partitioning, the partition with the fewest elements is (recursively) sorted first, requiring at most O(log n) space. Then the other partition is sorted using tail recursion or iteration, which doesn't add to the call stack. This idea, as discussed above, was described by R. Sedgewick, and keeps the stack depth bounded by O(log n).Quicksort with in-place and unstable partitioning uses only constant additional space before making any recursive call. Quicksort must store a constant amount of information for each nested recursive call. Since the best case makes at most O(log n) nested recursive calls, it uses O(log n) space. However, without Sedgewick's trick to limit the recursive calls, in the worst case quicksort could make O(n) nested recursive calls and need O(n) auxiliary space.\\nFrom a bit complexity viewpoint, variables such as lo and hi do not use constant space; it takes O(log n) bits to index into a list of n items. Because there are such variables in every stack frame, quicksort using Sedgewick's trick requires O((log n)2) bits of space. This space requirement isn't too terrible, though, since if the list contained distinct elements, it would need at least O(n log n) bits of space.\\nAnother, less common, not-in-place, version of quicksort uses O(n) space for working storage and can implement a stable sort. The working storage allows the input array to be easily partitioned in a stable manner and then copied back to the input array for successive recursive calls. Sedgewick's optimization is still appropriate.\\n\\nRelation to other algorithms\\nQuicksort is a space-optimized version of the binary tree sort. Instead of inserting items sequentially into an explicit tree, quicksort organizes them concurrently into a tree that is implied by the recursive calls. The algorithms make exactly the same comparisons, but in a different order. An often desirable property of a sorting algorithm is stability \\u2013 that is the order of elements that compare equal is not changed, allowing controlling order of multikey tables (e.g. directory or folder listings) in a natural way. This property is hard to maintain for in-place quicksort (that uses only constant additional space for pointers and buffers, and O(log n) additional space for the management of explicit or implicit recursion). For variant quicksorts involving extra memory due to representations using pointers (e.g. lists or trees) or files (effectively lists), it is trivial to maintain stability. The more complex, or disk-bound, data structures tend to increase time cost, in general making increasing use of virtual memory or disk.\\nThe most direct competitor of quicksort is heapsort. Heapsort's running time is O(n log n), but heapsort's average running time is usually considered slower than in-place quicksort. This result is debatable; some publications indicate the opposite. Introsort is a variant of quicksort that switches to heapsort when a bad case is detected to avoid quicksort's worst-case running time. Major programming languages, such as C++ (in the GNU and LLVM implementations), use introsort.Quicksort also competes with merge sort, another O(n log n) sorting algorithm. Standard merge sort is an out-of-place stable sort, unlike standard in-place quicksort and heapsort, and has excellent worst-case performance.  The main disadvantage of mergesort is that, when operating on arrays, efficient implementations require O(n) auxiliary space, whereas the variant of quicksort with in-place partitioning and tail recursion uses only O(log n) space.\\nMergesort works very well on linked lists, requiring only a small, constant amount of auxiliary storage.  Although quicksort can be implemented as a stable sort using linked lists, it will often suffer from poor pivot choices without random access.  Mergesort is also the algorithm of choice for external sorting of very large data sets stored on slow-to-access media such as disk storage or network-attached storage.\\nBucket sort with two buckets is very similar to quicksort; the pivot in this case is effectively the value in the middle of the value range, which does well on average for uniformly distributed inputs.\\n\\nSelection-based pivoting\\nA selection algorithm chooses the kth smallest of a list of numbers; this is an easier problem in general than sorting. One simple but effective selection algorithm works nearly in the same manner as quicksort, and is accordingly known as quickselect. The difference is that instead of making recursive calls on both sublists, it only makes a single tail-recursive call on the sublist that contains the desired element. This change lowers the average complexity to linear or O(n) time, which is optimal for selection, but the selection algorithm is still O(n2) in the worst case.\\nA variant of quickselect, the median of medians algorithm, chooses pivots more carefully, ensuring that the pivots are near the middle of the data (between the 30th and 70th percentiles), and thus has guaranteed linear time \\u2013 O(n). This same pivot strategy can be used to construct a variant of quicksort (median of medians quicksort) with O(n log n) time. However, the overhead of choosing the pivot is significant, so this is generally not used in practice.\\nMore abstractly, given an O(n) selection algorithm, one can use it to find the ideal pivot (the median) at every step of quicksort and thus produce a sorting algorithm with O(n log n) running time. Practical implementations of this variant are considerably slower on average, but they are of theoretical interest because they show an optimal selection algorithm can yield an optimal sorting algorithm.\\n\\nVariants\\nMulti-pivot quicksort\\nInstead of partitioning into two subarrays using a single pivot, multi-pivot quicksort (also multiquicksort) partitions its input into some s number of subarrays using s \\u2212 1 pivots. While the dual-pivot case (s = 3) was considered by Sedgewick and others already in the mid-1970s, the resulting algorithms were not faster in practice than the \\\"classical\\\" quicksort. A 1999 assessment of a multiquicksort with a variable number of pivots, tuned to make efficient use of processor caches, found it to increase the instruction count by some 20%, but simulation results suggested that it would be more efficient on very large inputs. A version of dual-pivot quicksort developed by Yaroslavskiy in 2009 turned out to be fast enough to warrant implementation in Java 7, as the standard algorithm to sort arrays of primitives (sorting arrays of objects is done using Timsort). The performance benefit of this algorithm was subsequently found to be mostly related to cache performance, and experimental results indicate that the three-pivot variant may perform even better on modern machines.\\n\\nExternal quicksort\\nFor disk files, an external sort based on partitioning similar to quicksort is possible. It is slower than external merge sort, but doesn't require extra disk space. 4 buffers are used, 2 for input, 2 for output. Let N = number of records in the file, B = the number of records per buffer, and M = N/B = the number of buffer segments in the file. Data is read (and written) from both ends of the file inwards. Let X represent the segments that start at the beginning of the file and Y represent segments that start at the end of the file. Data is read into the X and Y read buffers. A pivot record is chosen and the records in the X and Y buffers other than the pivot record are copied to the X write buffer in ascending order and Y write buffer in descending order based comparison with the pivot record. Once either X or Y buffer is filled, it is written to the file and the next X or Y buffer is read from the file. The process continues until all segments are read and one write buffer remains. If that buffer is an X write buffer, the pivot record is appended to it and the X buffer written. If that buffer is a Y write buffer, the pivot record is prepended to the Y buffer and the Y buffer written. This constitutes one partition step of the file, and the file is now composed of two subfiles. The start and end positions of each subfile are pushed/popped to a stand-alone stack or the main stack via recursion. To limit stack space to O(log2(n)), the smaller subfile is processed first. For a stand-alone stack, push the larger subfile parameters onto the stack, iterate on the smaller subfile. For recursion, recurse on the smaller subfile first, then iterate to handle the larger subfile. Once a sub-file is less than or equal to 4 B records, the subfile is sorted in-place via quicksort and written. That subfile is now sorted and in place in the file. The process is continued until all sub-files are sorted and in place. The average number of passes on the file is approximately 1 + ln(N+1)/(4 B), but worst case pattern is N passes (equivalent to O(n^2) for worst case internal sort).\\n\\nThree-way radix quicksort\\nThis algorithm is a combination of radix sort and quicksort. Pick an element from the array (the pivot) and consider the first character (key) of the string (multikey). Partition the remaining elements into three sets: those whose corresponding character is less than, equal to, and greater than the pivot's character. Recursively sort the \\\"less than\\\" and \\\"greater than\\\" partitions on the same character. Recursively sort the \\\"equal to\\\" partition by the next character (key). Given we sort using bytes or words of length W bits, the best case is O(KN) and the worst case O(2KN) or at least O(N2) as for standard quicksort, given for unique keys N<2K, and K is a hidden constant in all standard comparison sort algorithms including quicksort. This is a kind of three-way quicksort in which the middle partition represents a (trivially) sorted subarray of elements that are exactly equal to the pivot.\\n\\nQuick radix sort\\nAlso developed by Powers as an O(K) parallel PRAM algorithm. This is again a combination of radix sort and quicksort but the quicksort left/right partition decision is made on successive bits of the key, and is thus O(KN) for N K-bit keys. All comparison sort algorithms implicitly assume the transdichotomous model with K in \\u0398(log N), as if K is smaller we can sort in O(N) time using a hash table or integer sorting.  If K \\u226b log N but elements are unique within O(log N) bits, the remaining bits will not be looked at by either quicksort or quick radix sort.  Failing that, all comparison sorting algorithms will also have the same overhead of looking through O(K) relatively useless bits but quick radix sort will avoid the worst case O(N2) behaviours of standard quicksort and radix quicksort, and will be faster even in the best case of those comparison algorithms under these conditions of uniqueprefix(K) \\u226b log N. See Powers for further discussion of the hidden overheads in comparison, radix and parallel sorting.\\n\\nBlockQuicksort\\nIn any comparison-based sorting algorithm, minimizing the number of comparisons requires maximizing the amount of information gained from each comparison, meaning that the comparison results are unpredictable.  This causes frequent branch mispredictions, limiting performance. BlockQuicksort rearranges the computations of quicksort to convert unpredictable branches to data dependencies.  When partitioning, the input is divided into moderate-sized blocks (which fit easily into the data cache), and two arrays are filled with the positions of elements to swap.  (To avoid conditional branches, the position is unconditionally stored at the end of the array, and the index of the end is incremented if a swap is needed.) A second pass exchanges the elements at the positions indicated in the arrays.  Both loops have only one conditional branch, a test for termination, which is usually taken.\\nThe BlockQuicksort technique is incorporated into LLVM's C++ STL implementation, libcxx, providing a 50% improvement on random integer sequences. Pattern-defeating quicksort (pdqsort), a version of introsort, also incorporates this technique.\\n\\nPartial and incremental quicksort\\nSeveral variants of quicksort exist that separate the k smallest or largest elements from the rest of the input.\\n\\nGeneralization\\nRichard Cole and David C. Kandathil, in 2004, discovered a one-parameter family of sorting algorithms, called partition sorts, which on average (with all input orderings equally likely) perform at most \\n  \\n    \\n      \\n        n\\n        log\\n        \\u2061\\n        n\\n        +\\n        \\n          O\\n        \\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle n\\\\log n+{O}(n)}\\n   comparisons (close to the information theoretic lower bound) and \\n  \\n    \\n      \\n        \\n          \\u0398\\n        \\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\Theta }(n\\\\log n)}\\n   operations; at worst they perform \\n  \\n    \\n      \\n        \\n          \\u0398\\n        \\n        (\\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\Theta }(n\\\\log ^{2}n)}\\n   comparisons (and also operations); these are in-place, requiring only additional \\n  \\n    \\n      \\n        \\n          O\\n        \\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle {O}(\\\\log n)}\\n   space. Practical efficiency and smaller variance in performance were demonstrated against optimised quicksorts (of Sedgewick and Bentley-McIlroy).\\n\\nTradeoffs\\nQuicksort has a few tradeoffs that should be taken into consideration during implementation. For example, quicksort requires a lot of comparisons and it\\u2019s not stable since it swaps non-adjacent elements. Additionally, if the input is already sorted or reverse sorted, the pivot element will be one of the extreme elements causing the performance to degrade to \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          N\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(N^{2})}\\n  . As such, algorithms like Mergesort are a good choice if stability is important or the input is already partially sorted.\\n\\nUpdates on the latest research\\nThere is a new Quicksort algorithm which improves the worst time complexity from \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          N\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(N^{2})}\\n   to \\n  \\n    \\n      \\n        O\\n        (\\n        N\\n        l\\n        o\\n        g\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle O(NlogN)}\\n   by avoiding picking the maximum or minimum value as the pivot. \\nCode implemented in python:    quicksort(arr, lo, hi)\\n\\n   if len(arr) <= 100:\\n       insertion_sort(arr, hi-lo+1)\\n       return\\n   if len(arr) >= 500:\\n       arrange_element(arr, lo, hi)\\n       arrange_element(arr, lo, ((hi+lo)/2)-1)\\n       arrange_element(arr, (hi+lo)/2, hi)\\n  \\n   start = (hi-lo+1)*0.25+lo\\n   end = hi-(hi-lo+1)*0.25\\n \\n   pivot = arr[(start-end+1)/2]\\n   i = lo-1\\n   j = lo\\n   while j < hi:\\n       if arr[j] < pivot:\\n           i += 1\\n           arr[i], arr[j] = arr[j], arr[i]\\n       j += 1\\n   arr[i+1], arr[hi] = arr[hi], arr[i+1]\\n   pi = i+1\\n   quick_sort(arr, lo, pi-1)\\n   quick_sort(arr, pi+1, hi)\\n\\narrange_element(arr, lo, hi)\\n\\n   if (hi-lo+1) % 2 == 0:\\n       mid = (hi-lo+1)/2\\n       i, j = lo, mid\\n       while i < mid:\\n           if arr[i] > arr[j]:\\n               arr[i], arr[j] = arr[j], arr[i]\\n               i += 1\\n               j += 1\\n   else:\\n       mid = (hi-lo+1)/2\\n       i, j = lo, mid\\n       while i < mid:\\n           if arr[i] > arr[j]:\\n               arr[i], arr[j] = arr[j], arr[i]\\n               i += 1\\n               j += 1\\n       if arr[lo] > arr[hi]:\\n           arr[lo], arr[hi] = arr[hi], arr[lo]\\n\\ninsertion_sort(arr, n):\\n\\n   i = 1\\n   while i < n:\\n       key = arr[i]\\n       j = i-1\\n       while (j >= 0) and (arr[j] > key):\\n           arr[j+1] = arr[j]\\n           j -=1\\n       arr[j+1] = key\\n       i += 1\\n\\nSee also\\nIntrosort \\u2013 Hybrid sorting algorithm\\n\\nNotes\\nReferences\\nSedgewick, R. (1978). \\\"Implementing Quicksort programs\\\". Comm. ACM. 21 (10): 847\\u2013857. doi:10.1145/359619.359631. S2CID 10020756.\\nDean, B. C. (2006). \\\"A simple expected running time analysis for randomized 'divide and conquer' algorithms\\\". Discrete Applied Mathematics. 154: 1\\u20135. doi:10.1016/j.dam.2005.07.005.\\nHoare, C. A. R. (1961). \\\"Algorithm 63: Partition\\\". Comm. ACM. 4 (7): 321. doi:10.1145/366622.366642. S2CID 52800011.\\nHoare, C. A. R. (1961). \\\"Algorithm 65: Find\\\". Comm. ACM. 4 (7): 321\\u2013322. doi:10.1145/366622.366647.\\nHoare, C. A. R. (1962). \\\"Quicksort\\\". Comput. J. 5 (1): 10\\u201316. doi:10.1093/comjnl/5.1.10. (Reprinted in Hoare and Jones: Essays in computing science, 1989.)\\nMusser, David R. (1997). \\\"Introspective Sorting and Selection Algorithms\\\". Software: Practice and Experience. 27 (8): 983\\u2013993. doi:10.1002/(SICI)1097-024X(199708)27:8<983::AID-SPE117>3.0.CO;2-#.\\nDonald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Pages 113\\u2013122 of section 5.2.2: Sorting by Exchanging.\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Chapter 7: Quicksort, pp. 145\\u2013164.\\nFaron Moller. Analysis of Quicksort. CS 332: Designing Algorithms. Department of Computer Science, Swansea University.\\nMart\\u00ednez, C.; Roura, S. (2001). \\\"Optimal Sampling Strategies in Quicksort and Quickselect\\\". SIAM J. Comput. 31 (3): 683\\u2013705. CiteSeerX 10.1.1.17.4954. doi:10.1137/S0097539700382108.\\nBentley, J. L.; McIlroy, M. D. (1993). \\\"Engineering a sort function\\\". Software: Practice and Experience. 23 (11): 1249\\u20131265. CiteSeerX 10.1.1.14.8162. doi:10.1002/spe.4380231105. S2CID 8822797.\\n\\nExternal links\\n\\n\\\"Animated Sorting Algorithms: Quick Sort\\\". Archived from the original on 2 March 2015. Retrieved 25 November 2008. \\u2013 graphical demonstration\\n\\\"Animated Sorting Algorithms: Quick Sort (3-way partition)\\\". Archived from the original on 6 March 2015. Retrieved 25 November 2008.\\nOpen Data Structures \\u2013 Section 11.1.2 \\u2013 Quicksort, Pat Morin\\nInteractive illustration of Quicksort, with code walkthrough\"}, {\"Selection sort\": \"In computer science, selection sort is an in-place comparison sorting algorithm. It has an O(n2) time complexity, which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.\\nThe algorithm divides the input list into two parts: a sorted sublist of items which is built up from left to right at the front (left) of the list and a sublist of the remaining unsorted items that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right. \\nThe time efficiency of selection sort is quadratic, so there are a number of sorting techniques which have better time complexity than selection sort.\\n\\nExample\\nHere is an example of this sort algorithm sorting five elements:\\n\\n(Nothing appears changed on these last two lines because the last two numbers were already in order.)\\nSelection sort can also be used on list structures that make add and remove efficient, such as a linked list. In this case it is more common to remove the minimum element from the remainder of the list, and then insert it at the end of the values sorted so far. For example:\\n\\narr[] = 64 25 12 22 11\\n\\n// Find the minimum element in arr[0...4]\\n// and place it at beginning\\n11 25 12 22 64\\n\\n// Find the minimum element in arr[1...4]\\n// and place it at beginning of arr[1...4]\\n11 12 25 22 64\\n\\n// Find the minimum element in arr[2...4]\\n// and place it at beginning of arr[2...4]\\n11 12 22 25 64\\n\\n// Find the minimum element in arr[3...4]\\n// and place it at beginning of arr[3...4]\\n11 12 22 25 64\\n\\nImplementations\\nBelow is an implementation in C.\\n\\nComplexity\\nSelection sort is not difficult to analyze compared to other sorting algorithms, since none of the loops depend on the data in the array. Selecting the minimum requires scanning \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements (taking \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   comparisons) and then swapping it into the first position. Finding the next lowest element requires scanning the remaining \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        2\\n      \\n    \\n    {\\\\displaystyle n-2}\\n   elements and so on. Therefore, the total number of comparisons is\\n\\n  \\n    \\n      \\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        +\\n        (\\n        n\\n        \\u2212\\n        2\\n        )\\n        +\\n        .\\n        .\\n        .\\n        +\\n        1\\n        =\\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        i\\n      \\n    \\n    {\\\\displaystyle (n-1)+(n-2)+...+1=\\\\sum _{i=1}^{n-1}i}\\n  By arithmetic progression,\\n\\n  \\n    \\n      \\n        \\n          \\u2211\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n            \\u2212\\n            1\\n          \\n        \\n        i\\n        =\\n        \\n          \\n            \\n              (\\n              n\\n              \\u2212\\n              1\\n              )\\n              +\\n              1\\n            \\n            2\\n          \\n        \\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        =\\n        \\n          \\n            1\\n            2\\n          \\n        \\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        =\\n        \\n          \\n            1\\n            2\\n          \\n        \\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        \\u2212\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i=1}^{n-1}i={\\\\frac {(n-1)+1}{2}}(n-1)={\\\\frac {1}{2}}n(n-1)={\\\\frac {1}{2}}(n^{2}-n)}\\n  which is of complexity \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   in terms of number of comparisons. Each of these scans requires one swap for \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   elements (the final element is already in place).\\n\\nComparison to other sorting algorithms\\nAmong quadratic sorting algorithms (sorting algorithms with a simple average-case of \\u0398(n2)), selection sort almost always outperforms bubble sort and gnome sort. Insertion sort is very similar in that after the kth iteration, the first \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   elements in the array are in sorted order. Insertion sort's advantage is that it only scans as many elements as it needs in order to place the \\n  \\n    \\n      \\n        k\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle k+1}\\n  st element, while selection sort must scan all remaining elements to find the \\n  \\n    \\n      \\n        k\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle k+1}\\n  st element.\\nSimple calculation shows that insertion sort will therefore usually perform about half as many comparisons as selection sort, although it can perform just as many or far fewer depending on the order the array was in prior to sorting. It can be seen as an advantage for some real-time applications that selection sort will perform identically regardless of the order of the array, while insertion sort's running time can vary considerably. However, this is more often an advantage for insertion sort in that it runs much more efficiently if the array is already sorted or \\\"close to sorted.\\\"\\nWhile selection sort is preferable to insertion sort in terms of number of writes (\\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   swaps versus up to \\n  \\n    \\n      \\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\n          /\\n        \\n        2\\n      \\n    \\n    {\\\\displaystyle n(n-1)/2}\\n   swaps, with each swap being two writes), this is roughly twice the theoretical minimum achieved by cycle sort, which performs at most n writes.  This can be important if writes are significantly more expensive than reads, such as with EEPROM or Flash memory, where every write lessens the lifespan of the memory.\\nSelection sort can be implemented without unpredictable branches for the benefit of CPU branch predictors, by finding the location of the minimum with branch-free code and then performing the swap unconditionally.\\nFinally, selection sort is greatly outperformed on larger arrays by \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n\\\\log n)}\\n   divide-and-conquer algorithms such as mergesort. However, insertion sort or selection sort are both typically faster for small arrays (i.e. fewer than 10\\u201320 elements). A useful optimization in practice for the recursive algorithms is to switch to insertion sort or selection sort for \\\"small enough\\\" sublists.\\n\\nVariants\\nHeapsort greatly improves the basic algorithm by using an implicit heap data structure to speed up finding and removing the lowest datum. If implemented correctly, the heap will allow finding the next lowest element in \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (\\\\log n)}\\n   time instead of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n)}\\n   for the inner loop in normal selection sort, reducing the total running time to \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n\\\\log n)}\\n  .\\nA bidirectional variant of selection sort (called double selection sort or sometimes cocktail sort due to its similarity to cocktail shaker sort) finds both the minimum and maximum values in the list in every pass. This requires three comparisons per two items (a pair of elements is compared, then the greater is compared to the maximum and the lesser is compared to the minimum) rather than regular selection sort's one comparison per item, but requires only half as many passes, a net 25% savings.\\nSelection sort can be implemented as a stable sort if, rather than swapping in step 2, the minimum value is inserted into the first position and the intervening values shifted up. However, this modification either requires a data structure that supports efficient insertions or deletions, such as a linked list, or it leads to performing \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n^{2})}\\n   writes.\\nIn the bingo sort variant, items are sorted by repeatedly looking through the remaining items to find the greatest value and moving all items with that value to their final location. Like counting sort, this is an efficient variant if there are many duplicate values: selection sort does one pass through the remaining items for each item moved, while Bingo sort does one pass for each value.  After an initial pass to find the greatest value, subsequent passes move every item with that value to its final location while finding the next value as in the following pseudocode (arrays are zero-based and the for-loop includes both the top and bottom limits, as in Pascal):\\n\\nThus, if on average there are more than two items with the same value, bingo sort can be expected to be faster because it executes the inner loop fewer times than selection sort.\\n\\nSee also\\nSelection algorithm\\n\\nReferences\\nExternal links\\n\\nAnimated Sorting Algorithms: Selection Sort at the Wayback Machine (archived 7 March 2015) \\u2013 graphical demonstration\"}, {\"Shellsort\": \"Shellsort, also known as Shell sort or Shell's method, is an in-place comparison sort. It can be seen as either a generalization of sorting by exchange (bubble sort) or sorting by insertion (insertion sort). The method starts by sorting pairs of elements far apart from each other, then progressively reducing the gap between elements to be compared. By starting with far apart elements, it can move some out-of-place elements into position faster than a simple nearest neighbor exchange. Donald Shell published the first version of this sort in 1959. The running time of Shellsort is heavily dependent on the gap sequence it uses. For many practical variants, determining their time complexity remains an open problem.\\n\\nDescription\\nShellsort is an optimization of insertion sort that allows the exchange of items that are far apart. The idea is to arrange the list of elements so that, starting anywhere, taking every hth element produces a sorted list. Such a list is said to be h-sorted. It can also be thought of as h interleaved lists, each individually sorted. Beginning with large values of h allows elements to move long distances in the original list, reducing large amounts of disorder quickly, and leaving less work for smaller h-sort steps to do. If the list is then k-sorted for some smaller integer k, then the list remains h-sorted. Following this idea for a decreasing sequence of h values ending in 1 is guaranteed to leave a sorted list in the end.In simplistic terms, this means if we have an array of 1024 numbers, our first gap (h) could be 512. We then run through the list comparing each element in the first half to the element in the second half. Our second gap (k) is 256, which breaks the array into four sections (starting at 0,256,512,768), and we make sure the first items in each section are sorted relative to each other, then the second item in each section, and so on. In practice the gap sequence could be anything, but the last gap is always 1 to finish the sort (effectively finishing with an ordinary insertion sort).\\nAn example run of Shellsort with gaps 5, 3 and 1 is shown below.\\n\\nThe first pass, 5-sorting, performs insertion sort on five  separate subarrays (a1, a6, a11), (a2, a7, a12), (a3, a8), (a4, a9), (a5, a10). For instance, it changes the subarray (a1, a6, a11) from (62, 17, 25) to (17, 25, 62). The next pass, 3-sorting, performs insertion sort on the three subarrays (a1, a4, a7, a10), (a2, a5, a8, a11), (a3, a6, a9, a12). The last pass, 1-sorting, is an ordinary insertion sort of the entire array (a1,..., a12).\\nAs the example illustrates, the subarrays that Shellsort operates on are initially short; later they are longer but almost ordered. In both cases insertion sort works efficiently.\\nShellsort is not stable: it may change the relative order of elements with equal values. It is an adaptive sorting algorithm in that it executes faster when the input is partially sorted.\\n\\nPseudocode\\nUsing Marcin Ciura's gap sequence, with an inner insertion sort.\\n\\nGap sequences\\nThe question of deciding which gap sequence to use is difficult. Every gap sequence that contains 1 yields a correct sort (as this makes the final pass an ordinary insertion sort); however, the properties of thus obtained versions of Shellsort may be very different. Too few gaps slows down the passes, and too many gaps produces an overhead.\\nThe table below compares most proposed gap sequences published so far. Some of them have decreasing elements that depend on the size of the sorted array (N). Others are increasing infinite sequences, whose elements less than N should be used in reverse order.\\n\\nWhen the binary representation of N contains many consecutive zeroes, Shellsort using Shell's original gap sequence makes \\u0398(N2) comparisons in the worst case. For instance, this case occurs for N equal to a power of two when elements greater and smaller than the median occupy odd and even positions respectively, since they are compared only in the last pass.\\nAlthough it has higher complexity than the O(N log N) that is optimal for comparison sorts, Pratt's version lends itself to sorting networks and has the same asymptotic gate complexity as Batcher's bitonic sorter.\\nGonnet and Baeza-Yates observed that Shellsort makes the fewest comparisons on average when the ratios of successive gaps are roughly equal to 2.2. This is why their sequence with ratio 2.2 and Tokuda's sequence with ratio 2.25 prove efficient. However, it is not known why this is so. Sedgewick recommends using gaps which have low greatest common divisors or are pairwise coprime.With respect to the average number of comparisons, Ciura's sequence has the best known performance; gaps from 701 were not determined but the sequence can be further extended according to the recursive formula \\n  \\n    \\n      \\n        \\n          h\\n          \\n            k\\n          \\n        \\n        =\\n        \\u230a\\n        2.25\\n        \\n          h\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle h_{k}=\\\\lfloor 2.25h_{k-1}\\\\rfloor }\\n  .\\nTokuda's sequence, defined by the simple formula \\n  \\n    \\n      \\n        \\n          h\\n          \\n            k\\n          \\n        \\n        =\\n        \\u2308\\n        \\n          h\\n          \\n            k\\n          \\n          \\u2032\\n        \\n        \\u2309\\n      \\n    \\n    {\\\\displaystyle h_{k}=\\\\lceil h'_{k}\\\\rceil }\\n  , where \\n  \\n    \\n      \\n        \\n          h\\n          \\n            k\\n          \\n          \\u2032\\n        \\n        =\\n        2.25\\n        \\n          h\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n          \\u2032\\n        \\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle h'_{k}=2.25h'_{k-1}+1}\\n  , \\n  \\n    \\n      \\n        \\n          h\\n          \\n            1\\n          \\n          \\u2032\\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle h'_{1}=1}\\n  , can be recommended for practical applications.\\nIf the maximum input size is small, as may occur if Shellsort is used on small subarrays by another recursive sorting algorithm such as quicksort or merge sort, then it is possible to tabulate an optimal sequence for each input size.\\n\\nComputational complexity\\nThe following property holds: after h2-sorting of any h1-sorted array, the array remains h1-sorted. Every h1-sorted and h2-sorted array is also (a1h1+a2h2)-sorted, for any nonnegative integers a1 and a2. The worst-case complexity of Shellsort is therefore connected with the Frobenius problem: for given integers h1,..., hn with gcd = 1, the Frobenius number g(h1,..., hn) is the greatest integer that cannot be represented as a1h1+ ... +anhn with nonnegative integer a1,..., an. Using known formulae for Frobenius numbers, we can determine the worst-case complexity of Shellsort for several classes of gap sequences. Proven results are shown in the above table.\\nMark Allen Weiss proved that Shellsort runs in O(N log N) time when the input array is in reverse order.With respect to the average number of operations, none of the proven results concerns a practical gap sequence. For gaps that are powers of two, Espelid computed this average as \\n  \\n    \\n      \\n        0.5349\\n        N\\n        \\n          \\n            N\\n          \\n        \\n        \\u2212\\n        0.4387\\n        N\\n        \\u2212\\n        0.097\\n        \\n          \\n            N\\n          \\n        \\n        +\\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle 0.5349N{\\\\sqrt {N}}-0.4387N-0.097{\\\\sqrt {N}}+O(1)}\\n  . Knuth determined the average complexity of sorting an N-element array with two gaps (h, 1) to be \\n  \\n    \\n      \\n        \\n          \\n            \\n              2\\n              \\n                N\\n                \\n                  2\\n                \\n              \\n            \\n            h\\n          \\n        \\n        +\\n        \\n          \\n            \\u03c0\\n            \\n              N\\n              \\n                3\\n              \\n            \\n            h\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {2N^{2}}{h}}+{\\\\sqrt {\\\\pi N^{3}h}}}\\n  . It follows that a two-pass Shellsort with h = \\u0398(N1/3) makes on average O(N5/3) comparisons/inversions/running time. Yao found the average complexity of a three-pass Shellsort. His result was refined by Janson and Knuth: the average number of comparisons/inversions/running time made during a Shellsort with three gaps (ch, cg, 1), where h and g are coprime, is \\n  \\n    \\n      \\n        \\n          \\n            \\n              N\\n              \\n                2\\n              \\n            \\n            \\n              4\\n              c\\n              h\\n            \\n          \\n        \\n        +\\n        O\\n        (\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {N^{2}}{4ch}}+O(N)}\\n   in the first pass, \\n  \\n    \\n      \\n        \\n          \\n            1\\n            \\n              8\\n              g\\n            \\n          \\n        \\n        \\n          \\n            \\n              \\u03c0\\n              \\n                c\\n                h\\n              \\n            \\n          \\n        \\n        (\\n        h\\n        \\u2212\\n        1\\n        )\\n        \\n          N\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        O\\n        (\\n        h\\n        N\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {1}{8g}}{\\\\sqrt {\\\\frac {\\\\pi }{ch}}}(h-1)N^{3/2}+O(hN)}\\n   in the second pass and \\n  \\n    \\n      \\n        \\u03c8\\n        (\\n        h\\n        ,\\n        g\\n        )\\n        N\\n        +\\n        \\n          \\n            1\\n            8\\n          \\n        \\n        \\n          \\n            \\n              \\u03c0\\n              c\\n            \\n          \\n        \\n        (\\n        c\\n        \\u2212\\n        1\\n        )\\n        \\n          N\\n          \\n            3\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        +\\n        O\\n        \\n          (\\n          \\n            (\\n            c\\n            \\u2212\\n            1\\n            )\\n            g\\n            \\n              h\\n              \\n                1\\n                \\n                  /\\n                \\n                2\\n              \\n            \\n            N\\n          \\n          )\\n        \\n        +\\n        O\\n        \\n          (\\n          \\n            \\n              c\\n              \\n                2\\n              \\n            \\n            \\n              g\\n              \\n                3\\n              \\n            \\n            \\n              h\\n              \\n                2\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\psi (h,g)N+{\\\\frac {1}{8}}{\\\\sqrt {\\\\frac {\\\\pi }{c}}}(c-1)N^{3/2}+O\\\\left((c-1)gh^{1/2}N\\\\right)+O\\\\left(c^{2}g^{3}h^{2}\\\\right)}\\n   in the third pass. \\u03c8(h, g) in the last formula is a complicated function asymptotically equal to \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\u03c0\\n                h\\n              \\n              128\\n            \\n          \\n        \\n        g\\n        +\\n        O\\n        \\n          (\\n          \\n            \\n              g\\n              \\n                \\u2212\\n                1\\n                \\n                  /\\n                \\n                2\\n              \\n            \\n            \\n              h\\n              \\n                1\\n                \\n                  /\\n                \\n                2\\n              \\n            \\n          \\n          )\\n        \\n        +\\n        O\\n        \\n          (\\n          \\n            g\\n            \\n              h\\n              \\n                \\u2212\\n                1\\n                \\n                  /\\n                \\n                2\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {\\\\frac {\\\\pi h}{128}}}g+O\\\\left(g^{-1/2}h^{1/2}\\\\right)+O\\\\left(gh^{-1/2}\\\\right)}\\n  . In particular, when h = \\u0398(N7/15) and g = \\u0398(N1/5), the average time of sorting is O(N23/15).\\nBased on experiments, it is conjectured that Shellsort with Hibbard's gap sequence runs in O(N5/4) average time, and that Gonnet and Baeza-Yates's sequence requires on average 0.41N ln N (ln ln N + 1/6) element moves. Approximations of the average number of operations formerly put forward for other sequences fail when sorted arrays contain millions of elements.\\nThe graph below shows the average number of element comparisons in various variants of Shellsort, divided by the theoretical lower bound, i.e. log2N!, where the sequence 1, 4, 10, 23, 57, 132, 301, 701 has been extended according to the formula \\n  \\n    \\n      \\n        \\n          h\\n          \\n            k\\n          \\n        \\n        =\\n        \\u230a\\n        2.25\\n        \\n          h\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        \\u230b\\n      \\n    \\n    {\\\\displaystyle h_{k}=\\\\lfloor 2.25h_{k-1}\\\\rfloor }\\n  .\\n\\nApplying the theory of Kolmogorov complexity, Jiang, Li, and Vit\\u00e1nyi \\n proved the following lower bound for the order of the average number of operations/running time in a p-pass Shellsort: \\u03a9(pN1+1/p) when p \\u2264 log2N and \\u03a9(pN) when p > log2N.\\nTherefore, Shellsort has prospects of running in an average time that asymptotically grows like N logN only when using gap sequences whose number of gaps grows in proportion to the logarithm of the array size. It is, however, unknown whether Shellsort can reach this asymptotic order of average-case complexity, which is optimal for comparison sorts. The lower bound was improved by Vit\\u00e1nyi for every number of passes \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   to  \\n\\n  \\n    \\n      \\n        \\u03a9\\n        (\\n        N\\n        \\n          \\u2211\\n          \\n            k\\n            =\\n            1\\n          \\n          \\n            p\\n          \\n        \\n        \\n          h\\n          \\n            k\\n            \\u2212\\n            1\\n          \\n        \\n        \\n          /\\n        \\n        \\n          h\\n          \\n            k\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Omega (N\\\\sum _{k=1}^{p}h_{k-1}/h_{k})}\\n  \\nwhere \\n  \\n    \\n      \\n        \\n          h\\n          \\n            0\\n          \\n        \\n        =\\n        N\\n      \\n    \\n    {\\\\displaystyle h_{0}=N}\\n  . This result implies for example the Jiang-Li-Vit\\u00e1nyi lower bound for all \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  -pass increment sequences and improves that lower bound for particular increment sequences. In fact all bounds (lower and upper) currently known for the average case are precisely matched by this lower bound. For example, this gives the new result that the Janson-Knuth upper bound is matched by the resulting lower bound for the used increment sequence, showing that three pass Shellsort for this increment sequence uses \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        \\n          N\\n          \\n            23\\n            \\n              /\\n            \\n            15\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (N^{23/15})}\\n   comparisons/inversions/running time.\\nThe formula allows us to search for increment sequences that yield lower bounds which are unknown; for example an increment sequence for four passes which has a lower bound greater than\\n\\n  \\n    \\n      \\n        \\u03a9\\n        (\\n        p\\n        \\n          n\\n          \\n            1\\n            +\\n            1\\n            \\n              /\\n            \\n            p\\n          \\n        \\n        )\\n        =\\n        \\u03a9\\n        (\\n        \\n          n\\n          \\n            5\\n            \\n              /\\n            \\n            4\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Omega (pn^{1+1/p})=\\\\Omega (n^{5/4})}\\n   for the increment sequence\\n\\n  \\n    \\n      \\n        \\n          h\\n          \\n            1\\n          \\n        \\n        =\\n        \\n          n\\n          \\n            11\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle h_{1}=n^{11/16},}\\n   \\n  \\n    \\n      \\n        \\n          h\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          n\\n          \\n            7\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle h_{2}=n^{7/16},}\\n   \\n  \\n    \\n      \\n        \\n          h\\n          \\n            3\\n          \\n        \\n        =\\n        \\n          n\\n          \\n            3\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle h_{3}=n^{3/16},}\\n   \\n  \\n    \\n      \\n        \\n          h\\n          \\n            4\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle h_{4}=1}\\n  . The lower bound becomes\\n\\n  \\n    \\n      \\n        T\\n        =\\n        \\u03a9\\n        (\\n        n\\n        \\u22c5\\n        (\\n        \\n          n\\n          \\n            1\\n            \\u2212\\n            11\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        +\\n        \\n          n\\n          \\n            11\\n            \\n              /\\n            \\n            16\\n            \\u2212\\n            7\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        +\\n        \\n          n\\n          \\n            7\\n            \\n              /\\n            \\n            16\\n            \\u2212\\n            3\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        +\\n        \\n          n\\n          \\n            3\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        )\\n        =\\n        \\u03a9\\n        (\\n        \\n          n\\n          \\n            1\\n            +\\n            5\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        )\\n        =\\n        \\u03a9\\n        (\\n        \\n          n\\n          \\n            21\\n            \\n              /\\n            \\n            16\\n          \\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle T=\\\\Omega (n\\\\cdot (n^{1-11/16}+n^{11/16-7/16}+n^{7/16-3/16}+n^{3/16})=\\\\Omega (n^{1+5/16})=\\\\Omega (n^{21/16}).}\\n  \\nThe worst-case complexity of any version of Shellsort is of higher order: Plaxton, Poonen, and Suel showed that it grows at least as rapidly as \\n  \\n    \\n      \\n        \\u03a9\\n        \\n          (\\n          \\n            N\\n            \\n              \\n                (\\n                \\n                  \\n                    \\n                      log\\n                      \\u2061\\n                      N\\n                    \\n                    \\n                      log\\n                      \\u2061\\n                      log\\n                      \\u2061\\n                      N\\n                    \\n                  \\n                \\n                )\\n              \\n              \\n                2\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega \\\\left(N\\\\left({\\\\log N \\\\over \\\\log \\\\log N}\\\\right)^{2}\\\\right)}\\n  .\\nRobert Cypher proved a stronger lower bound: \\n  \\n    \\n      \\n        \\u03a9\\n        \\n          (\\n          \\n            N\\n            \\n              \\n                \\n                  (\\n                  log\\n                  \\u2061\\n                  N\\n                  \\n                    )\\n                    \\n                      2\\n                    \\n                  \\n                \\n                \\n                  log\\n                  \\u2061\\n                  log\\n                  \\u2061\\n                  N\\n                \\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega \\\\left(N{{(\\\\log N)^{2}} \\\\over {\\\\log \\\\log N}}\\\\right)}\\n   when \\n  \\n    \\n      \\n        \\n          h\\n          \\n            s\\n            +\\n            1\\n          \\n        \\n        >\\n        \\n          h\\n          \\n            s\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle h_{s+1}>h_{s}}\\n   for all \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  .\\n\\nApplications\\nShellsort performs more operations and has higher cache miss ratio than quicksort. However, since it can be implemented using little code and does not use the call stack, some implementations of the qsort function in the C standard library targeted at embedded systems use it instead of quicksort. Shellsort is, for example, used in the uClibc library. For similar reasons, in the past, Shellsort was used in the Linux kernel.Shellsort can also serve as a sub-algorithm of introspective sort, to sort short subarrays and to prevent a slowdown when the recursion depth exceeds a given limit. This principle is employed, for instance, in the bzip2 compressor.\\n\\nSee also\\nComb sort\\n\\nReferences\\nBibliography\\nKnuth, Donald E. (1997). \\\"Shell's method\\\". The Art of Computer Programming. Volume 3: Sorting and Searching (2nd ed.). Reading, Massachusetts: Addison-Wesley. pp. 83\\u201395. ISBN 978-0-201-89685-5.\\nAnalysis of Shellsort and Related Algorithms, Robert Sedgewick, Fourth European Symposium on Algorithms, Barcelona, September 1996.\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Shell Sort at the Wayback Machine (archived 10 March 2015) \\u2013 graphical demonstration\\nShellsort with gaps 5, 3, 1 as a Hungarian folk dance\"}, {\"Smoothsort\": \"In computer science, smoothsort is a comparison-based sorting algorithm. A variant of heapsort, it was invented and published by Edsger Dijkstra in 1981. Like heapsort, smoothsort is an in-place algorithm with an upper bound of O(n log n), but it is not a stable sort.  The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n log n) regardless of the initial sorted state.\\n\\nOverview\\nLike heapsort, smoothsort organizes the input into a priority queue and then repeatedly extracts the maximum.  Also like heapsort, the priority queue is an implicit heap data structure (a heap-ordered implicit binary tree), which occupies a prefix of the array.  Each extraction shrinks the prefix and adds the extracted element to a growing sorted suffix.  When the prefix has shrunk to nothing, the array is completely sorted.\\nHeapsort maps the binary tree to the array using a top-down breadth-first traversal of the tree; the array begins with the root of the tree, then its two children, then four grandchildren, and so on.  Every element has a well-defined depth below the root of the tree, and every element except the root has its parent earlier in the array.  Its height above the leaves, however, depends on the size of the array.  This has the disadvantage that every element must be moved as part of the sorting process: it must pass through the root before being moved to its final location.\\nSmoothsort uses a different mapping, a bottom-up depth-first post-order traversal.  A left child is followed by the subtree rooted at its sibling, and a right child is followed by its parent.  Every element has a well-defined height above the leaves, and every non-leaf element has its children earlier in the array.  Its depth below the root, however, depends on the size of the array.  The algorithm is organized so the root is at the end of the heap, and at the moment that an element is extracted from the heap it is already in its final location and does not need to be moved.  Also, a sorted array is already a valid heap, and many sorted intervals are valid heap-ordered subtrees.\\nMore formally, every position i is the root of a unique subtree, whose nodes occupy a contiguous interval that ends at i.  An initial prefix of the array (including the whole array), might be such an interval corresponding to a subtree, but in general decomposes as a union of a number of successive such subtree intervals, which Dijkstra calls \\\"stretches\\\". Any subtree without a parent (i.e. rooted at a position whose parent lies beyond the prefix under consideration) gives a stretch in the decomposition of that interval, which decomposition is therefore unique. When a new node is appended to the prefix, one of two cases occurs: either the position is a leaf and adds a stretch of length 1 to the decomposition, or it combines with the last two stretches, becoming the parent of their respective roots, thus replacing the two stretches by a new stretch containing their union plus the new (root) position.\\nDijkstra noted that the obvious rule would be to combine stretches if and only if they have equal size, in which case all subtrees would be perfect binary trees of size 2k\\u22121.  However, he chose a different rule, which gives more possible tree sizes.  This has the same asymptotic efficiency, but gains a small constant factor in efficiency by requiring fewer stretches to cover each interval.\\nThe rule Dijkstra uses is that the last two stretches are combined if and only if their sizes are consecutive Leonardo numbers L(i+1) and L(i) (in that order), which numbers are recursively defined, in a manner very similar to the Fibonacci numbers, as:\\n\\nL(0) = L(1) = 1\\nL(k+2) = L(k+1) + L(k) + 1As a consequence, the size of any subtree is a Leonardo number. The sequence of stretch sizes decomposing the first n positions, for any n, can be found in a greedy manner: the first size is the largest Leonardo number not exceeding n, and the remainder (if any) is decomposed recursively. The sizes of stretches are decreasing, strictly so except possibly for two final sizes 1, and avoiding successive Leonardo numbers except possibly for the final two sizes.\\nIn addition to each stretch being a heap-ordered tree, the roots of the trees are maintained in sorted order.  This effectively adds a third child (which Dijkstra calls a \\\"stepson\\\") to each root linking it to the preceding root.  This combines all of the trees together into one global heap. with the global maximum at the end.\\nAlthough the location of each node's stepson is fixed, the link only exists for tree roots, meaning that links are removed whenever trees are merged.  This is different from ordinary children, which are linked as long as the parent exists.\\nIn the first (heap growing) phase of sorting, an increasingly large initial part of the array is reorganized so that the subtree for each of its stretches is a max-heap: the entry at any non-leaf position is at least as large as the entries at the positions that are its children.  In addition, all roots are at least as large as their stepsons.\\nIn the second (heap shrinking) phase, the maximal node is detached from the end of the array (without needing to move it) and the heap invariants are re-established among its children.  (Specifically, among the newly created stepsons.)\\nPractical implementation frequently needs to compute Leonardo numbers L(k).  Dijkstra provides clever code which uses a fixed number of integer variables to efficiently compute the values needed at the time they are needed.  Alternatively, if there is a finite bound N on the size of arrays to be sorted, a precomputed table of Leonardo numbers can be stored in O(log N) space.\\n\\nOperations\\nWhile the two phases of the sorting procedure are opposite to each other as far as the evolution of the sequence-of-heaps structure is concerned, they are implemented using one core primitive, equivalent to the \\n\\\"sift down\\\" operation in a binary max-heap.\\n\\nSifting down\\nThe core sift-down operation (which Dijkstra calls \\\"trinkle\\\") restores the heap invariant when it is possibly violated only at the root node.  If the root node is less than any of its children, it is swapped with its greatest child and the process repeated with the root node in its new subtree.\\nThe difference between smoothsort and a binary max-heap is that the root of each stretch must be ordered with respect to a third \\\"stepson\\\": the root of the preceding stretch.  So the sift-down procedure starts with a series of four-way comparisons (the root node and three children) until the stepson is not the maximal element, then a series of three-way comparisons (the root plus two children) until the root node finds its final home and the invariants are re-established.\\nEach tree is a full binary tree: each node has two children or none. There is no need to deal with the special case of one child which occurs in a standard implicit binary heap.  (But the special case of stepson links more than makes up for this saving.)\\nBecause there are O(log n) stretches, each of which is a tree of depth O(log n), the time to perform each sifting-down operation is bounded by O(log n).\\n\\nGrowing the heap region by incorporating an element to the right\\nWhen an additional element is considered for incorporation into the sequence of stretches (list of disjoint heap structures) it either forms a new one-element stretch, or it combines the two rightmost stretches by becoming the parent of both their roots and forming a new stretch that replaces the two in the sequence. Which of the two happens depends only on the sizes of the stretches currently present (and ultimately only on the index of the element added); Dijkstra stipulated that stretches are combined if and only if their sizes are L(k+1) and L(k) for some k, i.e., consecutive Leonardo numbers; the new stretch will have size L(k+2).\\nIn either case, the new element must be sifted down to its correct place in the heap structure.  Even if the new node is a one-element stretch, it must still be sorted relative to the preceding stretch's root.\\n\\nOptimization\\nDijkstra's algorithm saves work by observing that the full heap invariant is required at the end of the growing phase, but it is not required at every intermediate step.  In particular, the requirement that an element be greater than its stepson is only important for the elements which are the final tree roots.\\nTherefore, when an element is added, compute the position of its future parent.  If this is within the range of remaining values to be sorted, act as if there is no stepson and only sift down within the current tree.\\n\\nShrinking the heap region by separating the rightmost element from it\\nDuring this phase, the form of the sequence of stretches goes through the changes of the growing phase in reverse. No work at all is needed when separating off a leaf node, but for a non-leaf node its two children become roots of new stretches, and need to be moved to their proper place in the sequence of roots of stretches. This can be obtained by applying sift-down twice: first for the left child, and then for the right child (whose stepson was the left child).\\nBecause half of all nodes in a full binary tree are leaves, this performs an average of one sift-down operation per node.\\n\\nOptimization\\nIt is already known that the newly exposed roots are correctly ordered with respect to their normal children; it is only the ordering relative to their stepsons which is in question.  Therefore, while shrinking the heap, the first step of sifting down can be simplified to a single comparison with the stepson.  If a swap occurs, subsequent steps must do the full four-way comparison.\\n\\nAnalysis\\nSmoothsort takes O(n) time to process a presorted array, O(n log  n) in the worst case, and achieves nearly-linear performance on many nearly-sorted inputs.  However, it does not handle all nearly-sorted sequences optimally.  Using the count of inversions as a measure of un-sortedness (the number of pairs of indices i and j with i < j and A[i] > A[j]; for randomly sorted input this is approximately n2/4), there are possible input sequences with O(n log n) inversions which cause it to take \\u03a9(n log n) time, whereas other adaptive sorting algorithms can solve these cases in O(n log log n) time.The smoothsort algorithm needs to be able to hold in memory the sizes of all of the trees in the Leonardo heap.  Since they are sorted by order and all orders are distinct, this is usually done using a bit vector indicating which orders are present.  Moreover, since the largest order is at most O(log n), these bits can be encoded in O(1) machine words, assuming a transdichotomous machine model.\\nNote that O(1) machine words is not the same thing as one machine word.  A 32-bit vector would only suffice for sizes less than L(32) = 7049155.  A 64-bit vector will do for sizes less than L(64) = 34335360355129 \\u2248 245.  In general, it takes 1/log2(\\u03c6) \\u2248 1.44 bits of vector per bit of size.\\n\\nPoplar sort\\nA simpler algorithm inspired by smoothsort is poplar sort.  Named after the rows of trees of decreasing size often seen in Dutch polders, it performs fewer comparisons than smoothsort for inputs that are not mostly sorted, but cannot achieve linear time for sorted inputs.\\nThe significant change made by poplar sort in that the roots of the various trees are not kept in sorted order; there are no \\\"stepson\\\" links tying them together into a single heap.  Instead, each time the heap is shrunk in the second phase, the roots are searched to find the maximum entry.\\nBecause there are n shrinking steps, each of which must search O(log n) tree roots for the maximum, the best-case run time for poplar sort is  O(n log n).\\nThe authors also suggest using perfect binary trees rather than Leonardo trees to provide further simplification, but this is a less significant change.\\nThe same structure has been proposed as a general-purpose priority queue under the name post-order heap, achieving O(1) amortized insertion time in a structure simpler than an implicit binomial heap.\\n\\nApplications\\nThe musl C library uses smoothsort for its implementation of qsort().\\n\\nReferences\\nExternal links\\nCommented transcription of EWD796a, 16-Aug-1981\\nDetailed modern explanation of Smoothsort\\nwikibooks:Algorithm Implementation/Sorting/Smoothsort\\nDescription and example implementation of Poplar heap\\nNoshita, Kohei; Nakatani, Yoshinobu (April 1985). \\\"On the Nested Heap Structure in Smoothsort\\\". Mathematical Foundations of Computer Science and Their Applications (Japanese: \\u6570\\u7406\\u89e3\\u6790\\u7814\\u7a76\\u6240\\u8b1b\\u7a76\\u9332). 556: 1\\u201316.\"}, {\"Sorting number\": \"In mathematics and computer science, the sorting numbers are a sequence of numbers introduced in 1950 by Hugo Steinhaus for the analysis of comparison sort algorithms. These numbers give the worst-case number of comparisons used by both binary insertion sort and merge sort. However, there are other algorithms that use fewer comparisons.\\n\\nFormula and examples\\nThe \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  th sorting number is given by the formula\\n\\n  \\n    \\n      \\n        n\\n        \\n        S\\n        (\\n        n\\n        )\\n        \\u2212\\n        \\n          2\\n          \\n            S\\n            (\\n            n\\n            )\\n          \\n        \\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle n\\\\,S(n)-2^{S(n)}+1}\\n  where\\n\\n  \\n    \\n      \\n        S\\n        (\\n        n\\n        )\\n        =\\n        \\u230a\\n        1\\n        +\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u230b\\n        .\\n      \\n    \\n    {\\\\displaystyle S(n)=\\\\lfloor 1+\\\\log _{2}n\\\\rfloor .}\\n  The sequence of numbers given by this formula (starting with \\n  \\n    \\n      \\n        n\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle n=1}\\n  ) is\\n\\n0, 1, 3, 5, 8, 11, 14, 17, 21, 25, 29, 33, 37, 41, ... (sequence A001855 in the OEIS).The same sequence of numbers can also be obtained from the recurrence relation\\n\\n  \\n    \\n      \\n        A\\n        (\\n        n\\n        )\\n        =\\n        A\\n        \\n          \\n            (\\n          \\n        \\n        \\u230a\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u230b\\n        \\n          \\n            )\\n          \\n        \\n        +\\n        A\\n        \\n          \\n            (\\n          \\n        \\n        \\u2308\\n        n\\n        \\n          /\\n        \\n        2\\n        \\u2309\\n        \\n          \\n            )\\n          \\n        \\n        +\\n        n\\n        \\u2212\\n        1.\\n      \\n    \\n    {\\\\displaystyle A(n)=A{\\\\bigl (}\\\\lfloor n/2\\\\rfloor {\\\\bigr )}+A{\\\\bigl (}\\\\lceil n/2\\\\rceil {\\\\bigr )}+n-1.}\\n  It is an example of a 2-regular sequence.Asymptotically, the value of the \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  th sorting number fluctuates between approximately\\n\\n  \\n    \\n      \\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        n\\n      \\n    \\n    {\\\\displaystyle n\\\\log _{2}n-n}\\n  \\nand\\n\\n  \\n    \\n      \\n        n\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        n\\n        \\u2212\\n        0.915\\n        n\\n      \\n    \\n    {\\\\displaystyle n\\\\log _{2}n-0.915n}\\n  \\ndepending on the ratio between \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   and the nearest power of two.\\n\\nApplication to sorting\\nIn 1950, Hugo Steinhaus observed that these numbers count the number of comparisons used by binary insertion sort, and conjectured (incorrectly) that they give the minimum number of comparisons needed to sort \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   items using any comparison sort. The conjecture was disproved in 1959 by L. R. Ford Jr. and Selmer M. Johnson, who found a different sorting algorithm, the Ford\\u2013Johnson merge-insert sort, using fewer comparisons.The same sequence of sorting numbers also gives the worst-case number of comparisons used by merge sort to sort \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   items.\\n\\nOther applications\\nThe sorting numbers (shifted by one position) also give the sizes of the shortest possible superpatterns for the layered permutations.\\n\\n\\n== References ==\"}, {\"Stooge sort\": \"Stooge sort is a recursive sorting algorithm. It is notable for its exceptionally bad time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...).\\nThe running time of the algorithm is thus slower compared to reasonable sorting algorithms, and is slower than bubble sort, a canonical example of a fairly inefficient sort. It is however more efficient than Slowsort. The name comes from The Three Stooges.The algorithm is defined as follows:\\n\\nIf the value at the start is larger than the value at the end, swap them.\\nIf there are 3 or more elements in the list, then:\\nStooge sort the initial 2/3 of the list\\nStooge sort the final 2/3 of the list\\nStooge sort the initial 2/3 of the list againIt is important to get the integer sort size used in the recursive calls by rounding the 2/3 upwards, e.g. rounding 2/3 of 5 should give 4 rather than 3, as otherwise the sort can fail on certain data.\\n\\nImplementation\\nReferences\\nSources\\nBlack, Paul E. \\\"stooge sort\\\". Dictionary of Algorithms and Data Structures. National Institute of Standards and Technology. Retrieved 18 June 2011.\\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001) [1990]. \\\"Problem 7-3\\\". Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. pp. 161\\u2013162. ISBN 0-262-03293-7.\\n\\nExternal links\\nSorting Algorithms (including Stooge sort)\\nStooge sort \\u2013 implementation and comparison\"}, {\"Timsort\": \"Timsort is a hybrid, stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. It was implemented by Tim Peters in 2002 for use in the Python programming language. The algorithm finds subsequences of the data that are already ordered (runs) and uses them to sort the remainder more efficiently. This is done by merging runs until certain criteria are fulfilled. Timsort has been Python's standard sorting algorithm since version 2.3. It is also used to sort arrays of non-primitive type in Java SE 7, on the Android platform, in GNU Octave, on V8, Swift, and Rust.It uses techniques from Peter McIlroy's 1993 paper \\\"Optimistic Sorting and Information Theoretic Complexity\\\".\\n\\nOperation\\nTimsort was designed to take advantage of runs of consecutive ordered elements that already exist in most real-world data, natural runs. It iterates over the data collecting elements into runs and simultaneously putting those runs in a stack. Whenever the runs on the top of the stack match a merge criterion, they are merged. This goes on until all data is traversed; then, all runs are merged two at a time and only one sorted run remains. The advantage of merging ordered runs instead of merging fixed size sub-lists (as done by traditional mergesort) is that it decreases the total number of comparisons needed to sort the entire list.\\nEach run has a minimum size, which is based on the size of the input and is defined at the start of the algorithm. If a run is smaller than this minimum run size, insertion sort is used to add more elements to the run until the minimum run size is reached.\\n\\nMerge criteria\\nTimsort is a stable sorting algorithm (order of elements with same key is kept) and strives to perform balanced merges (a merge thus merges runs of similar sizes).\\nIn order to achieve sorting stability, only consecutive runs are merged. Between two non-consecutive runs, there can be an element with the same key inside the runs. Merging those two runs would change the order of equal keys. Example of this situation ([] are ordered runs):  [1 2 2] 1 4 2 [0 1 2]\\nIn pursuit of balanced merges, Timsort considers three runs on the top of the stack, X, Y, Z, and maintains the invariants:\\n\\nIf any of these invariants is violated, Y is merged with the smaller of X or Z and the invariants are checked again. Once the invariants hold, the search for a new run in the data can start. These invariants maintain merges as being approximately balanced while maintaining a compromise between delaying merging for balance, exploiting fresh occurrence of runs in cache memory and making merge decisions relatively simple.\\n\\nMerge space overhead\\nThe original merge sort implementation is not in-place and it has a space overhead of N (data size). In-place merge sort implementations exist, but have a high time overhead. In order to achieve a middle term, Timsort performs a merge sort with a small time overhead and smaller space overhead than N.\\nFirst, Timsort performs a binary search to find the location where the first element of the second run would be inserted in the first ordered run, keeping it ordered. Then, it performs the same algorithm to find the location where the last element of the first run would be inserted in the second ordered run, keeping it ordered. Elements before and after these locations are already in their correct place and do not need to be merged. Then, the smaller of the remaining elements of the two runs is copied into temporary memory, and elements are merged with the larger run into the now free space.  If the first run is smaller, the merge starts at the beginning; if the second is smaller, the merge starts at the end. This optimization reduces the number of required element movements, the running time and the temporary space overhead in the general case.\\nExample: two runs [1, 2, 3, 6, 10] and [4, 5, 7, 9, 12, 14, 17] must be merged. Note that both runs are already sorted individually. The smallest element of the second run is 4 and it would have to be added at the fourth position of the first run in order to preserve its order (assuming that the first position of a run is 1). The largest element of the first run is 10 and it would have to be added at the fifth position of the second run in order to preserve its order. Therefore, [1, 2, 3] and [12, 14, 17] are already in their final positions and the runs in which elements movements are required are [6, 10] and [4, 5, 7, 9]. With this knowledge, we only need to allocate a temporary buffer of size 2 instead of 4.\\n\\nMerge direction\\nMerging can be done in both directions: left-to-right, as in the traditional mergesort, or right-to-left.\\n\\nGalloping mode during merge\\nAn individual merge of runs R1 and R2 keeps the count of consecutive elements selected from a run. When this number reaches the minimum galloping threshold (min_gallop), Timsort considers that it is likely that many consecutive elements may still be selected from that run and switches to the galloping mode. Let us assume that R1 is responsible for triggering it. In this mode, the algorithm performs an exponential search, also known as galloping search, for the next element x of the run R2 in the run R1. This is done in two stages: the first one finds the range (2k \\u2212 1, 2k+1 - 1) where x is. The second stage performs a binary search for the element x in the range found in the first stage. The galloping mode is an attempt to adapt the merge algorithm to the pattern of intervals between elements in runs.\\n\\nGalloping is not always efficient. In some cases galloping mode requires more comparisons than a simple linear search. According to benchmarks done by the developer, galloping is beneficial only when the initial element of one run is not one of the first seven elements of the other run. This implies an initial threshold of 7. To avoid the drawbacks of galloping mode, two actions are taken: (1) When galloping is found to be less efficient than binary search, galloping mode is exited. (2)\\nThe success or failure of galloping is used to adjust min_gallop. If the selected element is from the same array that returned an element previously, min_gallop is reduced by one, thus encouraging the return to galloping mode. Otherwise, the value is incremented by one, thus discouraging a return to galloping mode. In the case of random data, the value of min_gallop becomes so large that galloping mode never recurs.\\n\\nDescending runs\\nIn order to also take advantage of data sorted in descending order, Timsort reverses strictly descending runs when it finds them and adds them to the stack of runs. Since descending runs are later blindly reversed, excluding runs with equal elements maintains the algorithm's stability; i.e., equal elements won't be reversed.\\n\\nMinimum run size\\nBecause merging is most efficient when the number of runs is equal to, or slightly less than, a power of two, and notably less efficient when the number of runs is slightly more than a power of two, Timsort chooses minrun to try to ensure the former condition.Minrun is chosen from the range 32 to 64 inclusive, such that the size of the data, divided by minrun, is equal to, or slightly less than, a power of two.  The final algorithm takes the six most significant bits of the size of the array, adds one if any of the remaining bits are set, and uses that result as the minrun.  This algorithm works for all arrays, including those smaller than 64; for arrays of size 63 or less, this sets minrun equal to the array size and Timsort reduces to an insertion sort.\\n\\nAnalysis\\nIn the worst case, Timsort takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   comparisons to sort an array of n elements. In the best case, which occurs when the input is already sorted, it runs in linear time, meaning that it is an adaptive sorting algorithm.It is superior to Quicksort for sorting object references or pointers because these require expensive memory indirection to access data and perform comparisons and Quicksort's cache coherence benefits are greatly reduced.\\n\\nFormal verification\\nIn 2015, Dutch and German researchers in the EU FP7 ENVISAGE project found a bug in the standard implementation of Timsort. It was fixed in 2015 in Python, Java and Android.\\nSpecifically, the invariants on stacked run sizes ensure a tight upper bound on the maximum size of the required stack.  The implementation preallocated a stack sufficient to sort 264 bytes of input, and avoided further overflow checks.\\nHowever, the guarantee requires the invariants to apply to every group of three consecutive runs, but the implementation only checked it for the top three.  Using the KeY tool for formal verification of Java software, the researchers found that this check is not sufficient, and they were able to find run lengths (and inputs which generated those run lengths) which would result in the invariants being violated deeper in the stack after the top of the stack was merged.As a consequence, for certain inputs the allocated size is not sufficient to hold all unmerged runs. In Java, this generates for those inputs an array-out-of-bound exception. The smallest input that triggers this exception in Java and Android v7 is of size 67108864 (226). (Older Android versions already triggered this exception for certain inputs of size 65536 (216))\\nThe Java implementation was corrected by increasing the size of the preallocated stack based on an updated worst-case analysis.  The article also showed by formal methods how to establish the intended invariant by checking that the four topmost runs in the stack satisfy the two rules above.  This approach was adopted by Python and Android.\\n\\nReferences\\nFurther reading\\nAuger, Nicolas; Nicaud, Cyril; Pivoteau, Carine (2015). \\\"Merge Strategies: from Merge Sort to TimSort\\\". hal-01212839.\\nAuger, Jug\\u00e9, Nicaud, Pivoteau (2018). \\\"On the Worst-Case Complexity of TimSort\\\". ESA 2018.\\nSam Buss, Alexander Knop. \\\"Strategies for Stable Merge Sorting.\\\" SODA 2019.\\n\\nExternal links\\ntimsort.txt \\u2013 original explanation by Tim Peters\"}, {\"Weak heap\": \"In computer science, a weak heap is a data structure for priority queues, combining features of the binary heap and binomial heap.  It can be stored in an array as an implicit binary tree like a binary heap, and has the efficiency guarantees of binomial heaps.\\nA sorting algorithm using weak heaps, weak-heapsort, uses a number of comparisons that is close to the theoretical lower bound on the number of comparisons required to sort a list, so is particularly useful when comparison is expensive, such as when comparing strings using the full Unicode collation algorithm.\\n\\nDescription\\nA weak heap is most easily understood as a heap-ordered multi-way tree stored as a binary tree using the \\\"right-child left-sibling\\\" convention.  (This is equivalent to, but reversed from, the usual left-child right-sibling binary tree.)\\nIn the multi-way tree, and assuming a max-heap, each parent's key is greater than or equal to (\\u2265) all the child keys (and thus, by induction, all members of the subtree).\\nExpressed as a binary tree, this translates to the following invariants:\\nThe root node has no left child\\nFor every node, the value associated with that node is greater than or equal to the values associated with all nodes in its right subtree.\\nThe leaves of the tree have heights that are all within one of each other.The last condition is a consequence of the fact that an implicit binary tree is a complete binary tree.\\nThe structure of this tree maps very neatly onto the traditional 1-based (Ahnentafel) implicit binary tree arrangement, where node k has a next sibling (left child) numbered 2k and a first child (right child) numbered 2k + 1, by adding an additional root numbered 0.  This root has no siblings, only a first child, which is node 1 (2\\u00d70 + 1).\\nThis structure is very similar to that of a binomial heap, with a tree of height h being composed of a root plus trees of heights h \\u2212 1, h \\u2212 2, ..., 1.  A perfect (no missing leaves) weak heap with 2n elements is exactly isomorphic to a binomial heap of the same size, but the two algorithms handle sizes which are not a power of 2 differently: a binomial heap uses multiple perfect trees, while a weak heap uses a single imperfect tree.\\nWeak heaps require the ability to exchange the left and right children (and associated subtrees) of a node.  In an explicit (pointer-based) representation of the tree, this is straightforward.  In an implicit (array) representation, this requires one \\\"reverse bit\\\" per internal node to indicate which child is considered the left child.  A weak heap is thus not a strictly implicit data structure since it requires O(n) additional space (1/2 bit per node).  However, it is often possible to find space for this extra bit within the node structure, such as by tagging a pointer which is already present.\\nIn the implicit binary tree, node k with reverse bit rk has parent \\u230ak/2\\u230b, left child 2k + rk, and right child 2k + 1 \\u2212 rk.\\nViewed as a multi-way tree, each node in a weak heap is linked to two others: a \\\"next sibling\\\" and a \\\"first child\\\".  In the implicit tree, the links are fixed, so which of the two links is the sibling and which the first child is indicated by the reverse bit.\\n\\nOperations on weak heaps\\nNote that every node in a weak heap can be considered the root of a smaller weak heap by ignoring its next sibling.  Nodes with no first child are automatically valid weak heaps.\\nA node of height h has h \\u2212 1 children: a first child of height h \\u2212 1, a second child of height h \\u2212 2, and so on to the last child of height 1.  These may be found by following the first child link and then successive next sibling links.\\nIt also has next siblings of height h \\u2212 1, h \\u2212 2, etc.\\nA node's parent in the multi-way tree is called its \\\"distinguished ancestor\\\".  To find this in the binary tree, find the node's binary parent.  If the node is the right child (first child), the parent is the distinguished ancestor.  If the node is the left child (next sibling), its distinguished ancestor is the same as its binary parent's.  In the implicit tree, finding the binary parent is easy, but its reverse bit must be consulted to determine which type of child the node is.  (Early papers used the term \\\"grandparent\\\" for the distinguished ancestor, a meaning confusingly different from the usual \\\"parent of parent\\\".)\\nAlthough the distinguished ancestor may be log2n levels high in the tree, the average distance is 2.  (It's at least 1, and half of the time we recurse, so D = 1 + D/2, meaning that D = 2.)  Thus, even a simple iterative algorithm for finding the distinguished ancestor is sufficient.\\nLike binomial heaps, the fundamental operation on weak heaps is merging two heaps of equal height h, to make a weak heap of height h+1.  This requires exactly one comparison, between the roots.  Whichever root is greater (assuming a max-heap) is the final root.  Its first child is the losing root, which retains its children (right subtree).  The winning root's children are installed as siblings of the losing root.\\nThis operation can be performed on the implicit tree structure because the heaps being merged are never arbitrary.  Rather, the two heaps are formed as part of sifting a node up the multi-way tree:\\n\\nThe first is a normal weak heap (whose next sibling link exists, but is ignored).\\nThe second is the imaginary heap formed by linking the first root's distinguished ancestor (multi-way parent) to the first root's following siblings.At the beginning, the heap invariants apply everywhere except possibly between the first root and its distinguished ancestor.  All other nodes are less than or equal to their distinguished ancestors.\\nAfter comparing the two roots, the merge proceeds in one of two ways:\\n\\n(The distinguished ancestor is greater or equal.)  Nothing needs to be moved, and the result of the merge is the distinguished ancestor.\\n(The first root is greater.)  The first root's binary children (first child and next sibling) are exchanged (using the reverse bit), and then the first root and its distinguished ancestor are exchanged (by copying).The second case works because, in the multi-way tree, each node keeps its children with it.  The first root is promoted up the tree because it is greater than its distinguished ancestor.  Thus, it is safely greater than all of the ancestor's previous children.\\nThe previous ancestor, however, is not a safe parent for the first root's old children, because it is less than the first root and so it's not guaranteed to be greater than or equal to all of its children.\\nBy swapping the binary children, the appropriate subset of the demoted ancestor's old children (which are safely less than or equal to it) are demoted with it.  The demoted ancestor's new siblings are the first root's old children, promoted, which are safely less than or equal to the promoted first root.\\nAfter this operation, it is uncertain whether the invariant is maintained between the new distinguished ancestor and its distinguished ancestor, so the operation is repeated until the root is reached.\\n\\nWeak-heap sort\\nWeak heaps may be used to sort an array, in essentially the same way as a conventional heapsort.  First, a weak heap is built out of all of the elements of the array, and then the root is repeatedly exchanged with the last element, which is sifted down to its proper place.\\nA weak heap of n elements can be formed in n \\u2212 1 merges.  It can be done on various orders, but a simple bottom-up implementation works from the end of the array to the beginning, merging each node with its distinguished ancestor.  Note that finding the distinguished ancestor is simplified because the reverse bits in all parents of the heaps being merged are unmodified from their initial state (\\\"not reversed\\\"), and so do not need to be consulted.\\nAs with heapsort, if the array to be sorted is larger than the CPU cache, performance is improved if subtrees are merged as soon as two of the same size become available, rather than merging all subtrees on one level before proceeding to the next.Sifting down in a weak heap can be done in h = \\u2308log2n\\u2309 comparisons, as opposed to 2\\u2009log2n for a binary heap, or 1.5\\u2009log2n for the \\\"bottom-up heapsort\\\" variant.  This is done by \\\"merging up\\\": after swapping the root with the last element of the heap, find the last (height 1) child of the root.  Merge this with the root (its distinguished ancestor), resulting in a valid height-2 heap at the global root.  Then go to the previous sibling (binary parent) of the last merged node, and merge again.  Repeat until the root is reached, when it will be correct for the complete tree.\\n\\nPriority queue operations\\nIn a weak max-heap, the maximum value can be found (in constant time) as the value associated with the root node; similarly, in a weak min-heap, the minimum value can be found at the root.\\nAs with binary heaps, weak heaps can support the typical operations of a priority queue data structure: insert, delete-min, delete, or decrease-key, in logarithmic time per operation.\\nSifting up is done using the same process as in binary heaps.  The new node is added at the leaf level, then compared with its distinguished ancestor and swapped if necessary (the merge operation).  This is repeated until no more swaps are necessary or the root is reached.\\nVariants of the weak heap structure allow constant amortized time insertions and decrease-keys, matching the time for Fibonacci heaps.\\n\\nHistory and applications\\nWeak heaps were introduced by Dutton (1993), as part of a variant heap sort algorithm that (unlike the standard heap sort using binary heaps) could be used to sort n items using only n log2n + O(n) comparisons. They were later investigated as a more generally applicable priority queue data structure.\\n\\nReferences\\nFurther reading\\nEdelkamp, Stefan; Elmasry, Amr; Katajainen, Jyrki (November 2013). \\\"Weak heaps engineered\\\" (PDF). Journal of Discrete Algorithms. 23: 83\\u201397. doi:10.1016/j.jda.2013.07.002. We provide a catalogue of algorithms that optimize the standard algorithms in various ways. As the optimization criteria, we consider the worst-case running time, the number of instructions, branch mispredictions, cache misses, element comparisons, and element moves.\\nEdelkamp, Stefan; Elmasry, Amr; Katajainen, Jyrki; Wei\\u00df, Armin (July 2013). Weak Heaps and Friends: Recent Developments (PDF). Combinatorial Algorithms - 24th International Workshop. Rouen, France. doi:10.1007/978-3-642-45278-9_1.\"}, {\"Category:Online sorts\": \"These sorts can start sorting their input without having received all of it, and are therefore classified as online algorithms.\"}, {\"Cubesort\": \"Cubesort is a parallel sorting algorithm that builds a self-balancing multi-dimensional array from the keys to be sorted. As the axes are of similar length the structure resembles a cube. After each key is inserted the cube can be rapidly converted to an array.A cubesort implementation written in C was published in 2014.\\n\\nOperation\\nCubesort's algorithm uses a specialized binary search on each axis to find the location to insert an element. When an axis grows too large it is split. Locality of reference is optimal as only four binary searches are performed on small arrays for each insertion. By using many small dynamic arrays the high cost for insertion on single large arrays is avoided.\\n\\nReferences\\nExternal links\\nCubesort description and implementation in C\\nNiedermeier, Rolf (1996). \\\"Recursively divisible problems\\\". Algorithms and Computation. Lecture Notes in Computer Science. Vol. 1178. Springer Berlin Heidelberg. pp. 187\\u2013188. doi:10.1007/BFb0009494. eISSN 1611-3349. ISBN 978-3-540-62048-8. ISSN 0302-9743. (passing mention)\"}, {\"Insertion sort\": \"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time by comparisons. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:\\n\\nSimple implementation: Jon Bentley shows a three-line C/C++ version that is five lines when optimized.\\nEfficient for (quite) small data sets, much like other quadratic (i.e., O(n2)) sorting algorithms\\nMore efficient in practice than most other simple quadratic algorithms such as selection sort or bubble sort\\nAdaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(kn) when each element in the input is no more than k places away from its sorted position\\nStable; i.e., does not change the relative order of elements with equal keys\\nIn-place; i.e., only requires a constant amount O(1) of additional memory space\\nOnline; i.e., can sort a list as it receives itWhen people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.\\n\\nAlgorithm\\nInsertion sort iterates, consuming one input element each repetition, and grows a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.\\nSorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.\\nThe resulting array after k iterations has the property where the first k + 1 entries are sorted (\\\"+1\\\" because the first entry is skipped). In each iteration the first remaining entry of the input is removed, and inserted into the result at the correct position, thus extending the result:\\n\\nbecomes\\n\\nwith each element greater than x copied to the right as it is compared against x.\\nThe most common variant of insertion sort, which operates on arrays, can be described as follows:\\n\\nSuppose there exists a function called Insert designed to insert a value into a sorted sequence at the beginning of an array. It operates by beginning at the end of the sequence and shifting each element one place to the right until a suitable position is found for the new element. The function has the side effect of overwriting the value stored immediately after the sorted sequence in the array.\\nTo perform an insertion sort, begin at the left-most element of the array and invoke Insert to insert each element encountered into its correct position. The ordered sequence into which the element is inserted is stored at the beginning of the array in the set of indices already examined. Each insertion overwrites a single value: the value being inserted.Pseudocode of the complete algorithm follows, where the arrays are zero-based:\\ni \\u2190 1\\nwhile i < length(A)\\n    j \\u2190 i\\n    while j > 0 and A[j-1] > A[j]\\n        swap A[j] and A[j-1]\\n        j \\u2190 j - 1\\n    end while\\n    i \\u2190 i + 1\\nend while\\n\\nThe outer loop runs over all the elements except the first one, because the single-element prefix A[0:1] is trivially sorted, so the invariant that the first i entries are sorted is true from the start. The inner loop moves element A[i] to its correct place so that after the loop, the first i+1 elements are sorted. Note that the and-operator in the test must use short-circuit evaluation, otherwise the test might result in an array bounds error, when j=0 and it tries to evaluate A[j-1] > A[j] (i.e. accessing A[-1] fails).\\nAfter expanding the swap operation in-place as x \\u2190 A[j]; A[j] \\u2190 A[j-1]; A[j-1] \\u2190 x (where x is a temporary variable), a slightly faster version can be produced that moves A[i] to its position in one go and only performs one assignment in the inner loop body:\\ni \\u2190 1\\nwhile i < length(A)\\n    x \\u2190 A[i]\\n    j \\u2190 i - 1\\n    while j >= 0 and A[j] > x\\n        A[j+1] \\u2190 A[j]\\n        j \\u2190 j - 1\\n    end while\\n    A[j+1] \\u2190 x\\n    i \\u2190 i + 1\\nend while\\n\\nThe new inner loop shifts elements to the right to clear a spot for x = A[i].\\nThe algorithm can also be implemented in a recursive way. The recursion just replaces the outer loop, calling itself and storing successively smaller values of n on the stack until n equals 0, where the function then returns up the call chain to execute the code after each recursive call starting with n equal to 1, with n increasing by 1 as each instance of the function returns to the prior instance. The initial call would be insertionSortR(A, length(A)-1).\\n\\nfunction insertionSortR(array A, int n)\\n    if n > 0\\n        insertionSortR(A, n-1)\\n        x \\u2190 A[n]\\n        j \\u2190 n-1\\n        while j >= 0 and A[j] > x\\n            A[j+1] \\u2190 A[j]\\n            j \\u2190 j-1\\n        end while\\n        A[j+1] \\u2190 x\\n    end if\\nend function\\n\\nIt does not make the code any shorter, it also doesn't reduce the execution time, but it increases the additional memory consumption from O(1) to O(N) (at the deepest level of recursion the stack contains N references to the A array, each with accompanying value of variable n from N down to 1).\\n\\nBest, worst, and average cases\\nThe best case input is an array that is already sorted. In this case insertion sort has a linear running time (i.e., O(n)). During each iteration, the first remaining element of the input is only compared with the right-most element of the sorted subsection of the array.\\nThe simplest worst case input is an array sorted in reverse order. The set of all worst case inputs consists of all arrays where each element is the smallest or second-smallest of the elements before it. In these cases every iteration of the inner loop will scan and shift the entire sorted subsection of the array before inserting the next element. This gives insertion sort a quadratic running time (i.e., O(n2)).\\nThe average case is also quadratic, which makes insertion sort impractical for sorting large arrays. However, insertion sort is one of the fastest algorithms for sorting very small arrays, even faster than quicksort; indeed, good quicksort implementations use insertion sort for arrays smaller than a certain threshold, also when arising as subproblems; the exact threshold must be determined experimentally and depends on the machine, but is commonly around ten.\\nExample: The following table shows the steps for sorting the sequence {3, 7, 4, 9, 5, 2, 6, 1}.  In each step, the key under consideration is underlined. The key that was moved (or left in place because it was the biggest yet considered) in the previous step is marked with an asterisk.\\n\\n3  7  4  9  5  2  6  1\\n3* 7  4  9  5  2  6  1\\n3  7* 4  9  5  2  6  1\\n3  4* 7  9  5  2  6  1\\n3  4  7  9* 5  2  6  1\\n3  4  5* 7  9  2  6  1\\n2* 3  4  5  7  9  6  1\\n2  3  4  5  6* 7  9  1\\n1* 2  3  4  5  6  7  9\\n\\nRelation to other sorting algorithms\\nInsertion sort is very similar to selection sort. As in selection sort, after k passes through the array, the first k elements are in sorted order. However, the fundamental difference between the two algorithms is that insertion sort scans backwards from the current key, while selection sort scans forwards.  This results in selection sort making the first k elements the k smallest elements of the unsorted input, while in insertion sort they are simply the first k elements of the input.\\nThe primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the (k + 1)-st element is greater than the k-th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort. On average (assuming the rank of the (k + 1)-st element rank is random), insertion sort will require comparing and shifting half of the previous k elements, meaning that insertion sort will perform about half as many comparisons as selection sort on average.\\nIn the worst case for insertion sort (when the input array is reverse-sorted), insertion sort performs just as many comparisons as selection sort. However, a disadvantage of insertion sort over selection sort is that it requires more writes due to the fact that, on each iteration, inserting the (k + 1)-st element into the sorted portion of the array requires many element swaps to shift all of the following elements, while only a single swap is required for each iteration of selection sort. In general, insertion sort will write to the array O(n2) times, whereas selection sort will write only O(n) times. For this reason selection sort may be preferable in cases where writing to memory is significantly more expensive than reading, such as with EEPROM or flash memory.\\nWhile some divide-and-conquer algorithms such as quicksort and mergesort outperform insertion sort for larger arrays, non-recursive sorting algorithms such as insertion sort or selection sort are generally faster for very small arrays (the exact size varies by environment and implementation, but is typically between 7 and 50 elements). Therefore, a useful optimization in the implementation of those algorithms is a hybrid approach, using the simpler algorithm when the array has been divided to a small size.\\n\\nVariants\\nD.L. Shell made substantial improvements to the algorithm; the modified version is called Shell sort.  The sorting algorithm compares elements separated by a distance that decreases on each pass. Shell sort has distinctly improved running times in practical work, with two simple variants requiring O(n3/2) and O(n4/3) running time.If the cost of comparisons exceeds the cost of swaps, as is the case for example with string keys stored by reference or with human interaction (such as choosing one of a pair displayed side-by-side), then using binary insertion sort may yield better performance. Binary insertion sort employs a binary search to determine the correct location to insert new elements, and therefore performs \\u2308log2 n\\u2309 comparisons in the worst case. When each element in the array is searched for and inserted this is O(n log n). The algorithm as a whole still has a running time of O(n2) on average because of the series of swaps required for each insertion.The number of swaps can be reduced by calculating the position of multiple elements before moving them. For example, if the target position of two elements is calculated before they are moved into the proper position, the number of swaps can be reduced by about 25% for random data. In the extreme case, this variant works similar to merge sort.\\nA variant named binary merge sort uses a binary insertion sort to sort groups of 32 elements, followed by a final sort using merge sort. It combines the speed of insertion sort on small data sets with the speed of merge sort on large data sets.To avoid having to make a series of swaps for each insertion, the input could be stored in a linked list, which allows elements to be spliced into or out of the list in constant time when the position in the list is known.  However, searching a linked list requires sequentially following the links to the desired position: a linked list does not have random access, so it cannot use a faster method such as binary search.  Therefore, the running time required for searching is O(n), and the time for sorting is O(n2). If a more sophisticated data structure (e.g., heap or binary tree) is used, the time required for searching and insertion can be reduced significantly; this is the essence of heap sort and binary tree sort.\\nIn 2006 Bender, Martin Farach-Colton, and Mosteiro published a new variant of insertion sort called library sort or gapped insertion sort that leaves a small number of unused spaces (i.e., \\\"gaps\\\") spread throughout the array. The benefit is that insertions need only shift elements over until a gap is reached. The authors show that this sorting algorithm runs with high probability in O(n log n) time.If a skip list is used, the insertion time is brought down to O(log n), and swaps are not needed because the skip list is implemented on a linked list structure.  The final running time for insertion would be O(n log n).\\n\\nList insertion sort code in C\\nIf the items are stored in a linked list, then the list can be sorted with O(1) additional space.  The algorithm starts with an initially empty (and therefore trivially sorted) list. The input items are taken off the list one at a time, and then inserted in the proper place in the sorted list. When the input list is empty, the sorted list has the desired result.\\n\\nThe algorithm below uses a trailing pointer for the insertion into the sorted list. A simpler recursive method rebuilds the list each time (rather than splicing) and can use O(n) stack space.\\n\\nReferences\\nFurther reading\\nKnuth, Donald (1998), \\\"5.2.1: Sorting by Insertion\\\", The Art of Computer Programming, vol. 3. Sorting and Searching (second ed.), Addison-Wesley, pp. 80\\u2013105, ISBN 0-201-89685-0.\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Insertion Sort at the Wayback Machine (archived 8 March 2015) \\u2013 graphical demonstration\\nAdamovsky, John Paul, Binary Insertion Sort \\u2013 Scoreboard \\u2013 Complete Investigation and C Implementation, Pathcom.\\nInsertion Sort \\u2013 a comparison with other O(n2) sorting algorithms, UK: Core war.\\nInsertion sort (C) (wiki), LiteratePrograms \\u2013 implementations of insertion sort in C and several other programming languages\"}, {\"Library sort\": \"Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions. The name comes from an analogy:\\n\\nSuppose a librarian were to store their books alphabetically on a long shelf, starting with the As at the left end, and continuing to the right along the shelf with no spaces between the books until the end of the Zs. If the librarian acquired a new book that belongs to the B section, once they find the correct space in the B section, they will have to move every book over, from the middle of the Bs all the way down to the Zs in order to make room for the new book. This is an insertion sort. However, if they were to leave a space after every letter, as long as there was still space after B, they would only have to move a few books to make room for the new one. This is the basic principle of the Library Sort.\\nThe algorithm was proposed by Michael A. Bender, Mart\\u00edn Farach-Colton, and Miguel Mosteiro in 2004 and was published in 2006.Like the insertion sort it is based on, library sort is a comparison sort; however, it was shown to have a high probability of running in O(n log n) time (comparable to quicksort), rather than an insertion sort's O(n2). There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.\\nCompared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. The amount and distribution of that space would be implementation dependent. In the paper the size of the needed array is (1 + \\u03b5)n, but with no further recommendations on how to choose \\u03b5. Moreover, it is neither adaptive nor stable. In order to warrant the with-high-probability time bounds, it requires to randomly permute the input, what changes the relative order of equal elements and shuffles any presorted input. Also, the algorithm uses binary search to find the insertion point for each element, which does not take profit of presorted input.\\nAnother drawback is that it cannot be run as an online algorithm, because it is not possible to randomly shuffle the input. If used without this shuffling, it could easily degenerate into quadratic behaviour.\\nOne weakness of insertion sort is that it may require a high number of swap operations and be costly if memory write is expensive. Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, but is also adding an extra cost in the rebalancing step. In addition, locality of reference will be poor compared to mergesort as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets.\\n\\nImplementation\\nAlgorithm\\nLet us say we have an array of n elements. We choose the gap we intend to give. Then we would have a final array of size (1 + \\u03b5)n. The algorithm works in log n rounds. In each round we insert as many elements as there are in the final array already, before re-balancing the array. For finding the position of inserting, we apply Binary Search in the final array and then swap the following elements till we hit an empty space. Once the round is over, we re-balance the final array by inserting spaces between each element.\\nFollowing are three important steps of the algorithm:\\n\\nBinary Search: Finding the position of insertion by applying binary search within the already inserted elements. This can be done by linearly moving towards left or right side of the array if you hit an empty space in the middle element.\\nInsertion: Inserting the element in the position found and swapping the following elements by 1 position till an empty space is hit. This is done in logarithmic time, with high probability.\\nRe-Balancing: Inserting spaces between each pair of elements in the array. The cost of rebalancing is linear in the number of elements already inserted. As these lengths increase with the powers of 2 for each round, the total cost of rebalancing is also linear.\\n\\nPseudocode\\nprocedure rebalance(A, begin, end) is\\n    r \\u2190 end\\n    w \\u2190 end \\u00f7 2\\n\\n    while r \\u2265 begin do\\n        A[w+1] \\u2190 gap\\n        A[w] \\u2190 A[r]\\n        r \\u2190 r \\u2212 1\\n        w \\u2190 w \\u2212 2\\n\\nprocedure sort(A) is\\n    n \\u2190 length(A)\\n    S \\u2190 new array of n gaps\\n\\n    for i \\u2190 1 to floor(log2(n) + 1) do\\n        for j \\u2190 2^i to 2^(i + 1) do\\n            ins \\u2190 binarysearch(A[j], S, 2^(i \\u2212 1))\\n            insert A[j] at S[ins]\\n\\nHere, binarysearch(el, A, k) performs binary search in the first k elements of A, skipping over gaps, to find a place where to locate element el. Insertion should favor gaps over filled-in elements.\\n\\nReferences\\nExternal links\\nGapped Insertion Sort\"}, {\"Partial sorting\": \"In computer science, partial sorting is a relaxed variant of the sorting problem. Total sorting is the problem of returning a list of items such that its elements all appear in order, while partial sorting is returning a list of the k smallest (or k largest) elements in order. The other elements (above the k smallest ones) may also be sorted, as in an in-place partial sort, or may be discarded, which is common in streaming partial sorts. A common practical example of partial sorting is computing the \\\"Top 100\\\" of some list.\\nIn terms of indices, in a partially sorted list, for every index i from 1 to k, the i-th element is in the same place as it would be in the fully sorted list: element i of the partially sorted list contains order statistic i of the input list.\\n\\nOffline problems\\nHeap-based solution\\nHeaps admit a simple single-pass partial sort when k is fixed: insert the first k elements of the input into a max-heap. Then make one pass over the remaining elements, add each to the heap in turn, and remove the largest element. Each insertion operation takes O(log k) time, resulting in O(n log k) time overall; this \\\"partial heapsort\\\" algorithm is practical for small values of k and in online settings. An \\\"online heapselect\\\" algorithm described below, based on a min-heap, takes O(n + k log n).\\n\\nSolution by partitioning selection\\nA further relaxation requiring only a list of the k smallest elements, but without requiring that these be ordered, makes the problem equivalent to partition-based selection; the original partial sorting problem can be solved by such a selection algorithm to obtain an array where the first k elements are the k smallest, and sorting these, at a total cost of O(n + k log k) operations. A popular choice to implement this algorithm scheme is to combine quickselect and quicksort; the result is sometimes called \\\"quickselsort\\\".Common in current (as of 2022) C++ STL implementations is a pass of heapselect for a list of k elements, followed by a heapsort for the final result.\\n\\nSpecialised sorting algorithms\\nMore efficient than the aforementioned are specialized partial sorting algorithms based on mergesort and quicksort. In the quicksort variant, there is no need to recursively sort partitions which only contain elements that would fall after the k'th place in the final sorted array (starting from the \\\"left\\\" boundary). Thus, if the pivot falls in position k or later, we recurse only on the left partition:\\nfunction partial_quicksort(A, i, j, k) is\\n    if i < j then\\n        p \\u2190 pivot(A, i, j)\\n        p \\u2190 partition(A, i, j, p)\\n        partial_quicksort(A, i, p-1, k)\\n        if p < k-1 then\\n            partial_quicksort(A, p+1, j, k)\\n\\nThe resulting algorithm is called partial quicksort and requires an expected time of only O(n + k log k), and is quite efficient in practice, especially if a selection sort is used as a base case when k becomes small relative to n. However, the worst-case time complexity is still very bad, in the case of a bad pivot selection. Pivot selection along the lines of the worst-case linear time selection algorithm (see Quicksort \\u00a7 Choice of pivot) could be used to get better worst-case performance. Partial quicksort, quickselect (including the multiple variant), and quicksort can all be generalized into what is known as a chunksort.\\n\\nIncremental sorting\\nIncremental sorting is a version of the partial sorting problem where the input is given up front but k is unknown: given a k-sorted array, it should be possible to extend the partially sorted part so that the array becomes (k+1)-sorted.Heaps lead to an O(n + k log n) \\\"online heapselect\\\" solution to incremental partial sorting: first \\\"heapify\\\", in linear time, the complete input array to produce a min-heap. Then extract the minimum of the heap k times.A different incremental sort can be obtained by modifying quickselect. The version due to Paredes and Navarro maintains a stack of pivots across calls, so that incremental sorting can be accomplished by repeatedly requesting the smallest item of an array A from the following algorithm:\\n\\nThe stack S is initialized to contain only the length n of A. k-sorting the array is done by calling IQS(A, i, S) for i = 0, 1, 2, ...; this sequence of calls has average-case complexity O(n + k log k), which is asymptotically equivalent to O(n + k log n). The worst-case time is quadratic, but this can be fixed by replacing the random pivot selection by the median of medians algorithm.\\n\\nLanguage/library support\\nThe C++ standard specifies a library function called std::partial_sort.\\nThe Python standard library includes functions nlargest and nsmallest in its heapq module.\\nThe Julia standard library includes a PartialQuickSort algorithm used in partialsort! and variants.\\n\\nSee also\\nSelection algorithm\\n\\nReferences\\nExternal links\\nJ.M. Chambers (1971). Partial sorting. CACM 14(5):357\\u2013358.\"}, {\"Polyphase merge sort\": \"A polyphase merge sort is a variation of a bottom-up merge sort that sorts a list using an initial uneven distribution of sub-lists (runs), primarily used for external sorting, and is more efficient than an ordinary merge sort when there are fewer than eight external working files (such as a tape drive or a file on a hard drive). A polyphase merge sort is not a stable sort.\\n\\nOrdinary merge sort\\nA merge sort splits the records of a dataset into sorted runs of records and then repeatedly merges sorted runs into larger sorted runs until only one run, the sorted dataset, remains.\\nAn ordinary merge sort using four working files organizes them as a pair of input files and a pair of output files. The dataset is distributed evenly between two of the working files, either as sorted runs or in the simplest case, single records, which can be considered to be sorted runs of size 1. Once all of the dataset is transferred to the two working files, those two working files become the input files for the first merge iteration. Each merge iteration merges runs from the two input working files, alternating the merged output between the two output files, again distributing the merged runs evenly between the two output files (until the final merge iteration). Once all of the runs from the two inputs files are merged and output, then the output files become the input files and vice versa for the next merge iteration. The number of runs decreases by a factor of 2 at each iteration, such as 64, 32, 16, 8, 4, 2, 1. For the final merge iteration, the two input files only have one sorted run (1/2 of the dataset) each, and the merged result is a single sorted run (the sorted dataset) on one of the output files. This is also described at Merge sort \\u00a7 Use with tape drives.\\nIf there are only three working files, then an ordinary merge sort merges sorted runs from two working files onto a single working file, then distributes the runs evenly between the two output files. The merge iteration reduces run count by a factor of 2, the redistribute iteration doesn't reduce run count (the factor is 1). Each iteration could be considered to reduce the run count by an average factor of \\u221a2 \\u2248 1.41. If there are 5 working files, then the pattern alternates between a 3 way merge and a 2 way merge, for an average factor of \\u221a6 \\u2248 2.45.\\nIn general, for an even number N of working files, each iteration of an ordinary merge sort reduces run count by a factor of N/2, while for an odd number N of working files, each iteration reduces the run count by an average factor of \\u221a(N2\\u22121)/4 = \\u221aN2\\u22121/2.\\n\\nPolyphase merge\\nFor N < 8 working files, a polyphase merge sort achieves a higher effective run count reduction factor by unevenly distributing sorted runs between N\\u22121 working files (explained in next section). Each iteration merges runs from N\\u22121 working files onto a single output working file. When the end of one of the N\\u22121 working files is reached, then it becomes the new output file and what was the output file becomes one of the N\\u22121 working input files, starting a new iteration of polyphase merge sort. Each iteration merges only a fraction of the dataset (about 1/2 to 3/4), except for the last iteration which merges all of the dataset into a single sorted run. The initial distribution is set up so that only one input working file is emptied at a time, except for the final merge iteration which merges N\\u22121 single runs (of varying size, this is explained next) from the N\\u22121 input working files to the single output file, resulting in a single sorted run, the sorted dataset.\\nFor each polyphase iteration, the total number of runs follows a pattern similar to a reversed Fibonacci numbers of higher order sequence. With 4 files, and a dataset consisting of 57 runs, the total run count on each iteration would be 57, 31, 17, 9, 5, 3, 1. Note that except for the last iteration, the run count reduction factor is a bit less than 2, 57/31, 31/17, 17/9, 9/5, 5/3, 3/1, about 1.84 for a 4 file case, but each iteration except the last reduced the run count while processing about 65% of the dataset, so the run count reduction factor per dataset processed during the intermediate iterations is about 1.84 / 0.65 = 2.83. For a dataset consisting of 57 runs of 1 record each, then after the initial distribution, polyphase merge sort moves 232 records during the 6 iterations it takes to sort the dataset, for an overall reduction factor of 2.70 (this is explained in more detail later).\\nAfter the first polyphase iteration, what was the output file now contains the results of merging N\\u22121 original runs, but the remaining N\\u22122 input working files still contain the remaining original runs, so the second merge iteration produces runs of size (N\\u22121) + (N\\u22122) = (2N \\u2212 3) original runs. The third iteration produces runs of size (4N \\u2212 7) original runs. With 4 files, the first iteration creates runs of size 3 original runs, the second iteration 5 original runs, the third iteration 9 original runs and so on, following the Fibonacci like pattern, 1, 3, 5, 9, 17, 31, 57, ... , so the increase in run size follows the same pattern as the decrease in run count in reverse. In the example case of 4 files and 57 runs of 1 record each, the last iteration merges 3 runs of size 31, 17, 9, resulting in a single sorted run of size 31+17+9 = 57 records, the sorted dataset. An example of the run counts and run sizes for 4 files, 31 records can be found in table 4.3 of.\\n\\nPerfect 3 file polyphase merge sort\\nIt is easiest to look at the polyphase merge starting from its ending conditions and working backwards. At the start of each iteration, there will be two input files and one output file. At the end of the iteration, one input file will have been completely consumed and will become the output file for the next iteration. The current output file will become an input file for the next iteration. The remaining files (just one in the 3 file case) have only been partially consumed and their remaining runs will be input for the next iteration.\\nFile 1 just emptied and became the new output file. One run is left on each input tape, and merging those runs together will make the sorted file.\\n\\nFile 1 (out):                                           <1 run> *        (the sorted file)\\nFile 2 (in ): ... | <1 run> *               -->     ... <1 run> | *          (consumed)\\nFile 3 (in ):     | <1 run> *                           <1 run> | *          (consumed)\\n\\n...  possible runs that have already been read\\n|    marks the read pointer of the file\\n*    marks end of file\\n\\nStepping back to the previous iteration, we were reading from 1 and 2. One run is merged from 1 and 2 before file 1 goes empty.  Notice that file 2 is not completely consumed\\u2014it has one run left to match the final merge (above).\\n\\nFile 1 (in ): ... | <1 run> *                      ... <1 run> | *\\nFile 2 (in ):     | <2 run> *           -->            <1 run> | <1 run> *\\nFile 3 (out):                                          <1 run> *\\n\\nStepping back another iteration, 2 runs are merged from 1 and 3 before file 3 goes empty.\\n\\nFile 1 (in ):     | <3 run>                        ... <2 run> | <1 run> *\\nFile 2 (out):                               -->        <2 run> *\\nFile 3 (in ): ... | <2 run> *                          <2 run> | *\\n\\nStepping back another iteration, 3 runs are merged from 2 and 3 before file 2 goes empty.\\n\\nFile 1 (out):                                          <3 run> *\\nFile 2 (in ): ... | <3 run> *               -->    ... <3 run> | *\\nFile 3 (in ):     | <5 run> *                          <3 run> | <2 run> *\\n\\nStepping back another iteration, 5 runs are merged from 1 and 2 before file 1 goes empty.\\n\\nFile 1 (in ): ... | <5 run> *                      ... <5 run> | *\\nFile 2 (in ):     | <8 run> *               -->        <5 run> | <3 run> *\\nFile 3 (out):                                          <5 run> *\\n\\nDistribution for polyphase merge sort\\nLooking at the perfect 3 file case, the number of runs for merged working backwards: 1, 1, 2, 3, 5, ... reveals a Fibonacci sequence. The sequence for more than 3 files is a bit more complicated; for 4 files, starting at the final state and working backwards, the run count pattern is {1,0,0,0}, {0,1,1,1}, {1,0,2,2}, {3,2,0,4}, {7,6,4,0}, {0,13,11,7}, {13,0,24,20}, ... .\\nFor everything to work out optimally, the last merge phase should have exactly one run on each input file.  If any input file has more than one run, then another phase would be required. Consequently, the polyphase merge sort needs to be clever about the initial distribution of the input data's runs to the initial output files.  For example, an input file with 13 runs would write 5 runs to file 1 and 8 runs to file 2.\\nIn practice, the input file will not have the exact number of runs needed for a perfect distribution. One way to deal with this is by padding the actual distribution with imaginary \\\"dummy runs\\\" to simulate an ideal run distribution. A dummy run behaves like a run with no records in it. Merging one or more dummy runs with one or more real runs just merges the real runs, and merging one or more dummy runs with no real runs results in a single dummy run. Another approach is to emulate dummy runs as needed during the merge operations.\\\"Optimal\\\" distribution algorithms require knowing the number of runs in advance. Otherwise, in the more common case where the number of runs is not known in advance, \\\"near optimal\\\" distribution algorithms are used. Some distribution algorithms include rearranging runs. If the number of runs is known in advance, only a partial distribution is needed before starting the merge phases. For example, consider the 3 file case, starting with n runs in File_1. Define Fi = Fi\\u22121 + Fi\\u22122 as the ith Fibonacci number.  If n = Fi, then move Fi\\u22122 runs to File_2, leaving Fi\\u22121 runs remaining on File_1, a perfect run distribution. If Fi < n < Fi+1, move n\\u2212Fi runs to File_2 and Fi+1\\u2212n runs to File_3. The first merge iteration merges n\\u2212Fi runs from File_1 and File_2, appending the n\\u2212Fi merged runs to the Fi+1\\u2212n runs already moved to File_3. File_1 ends up with Fi\\u22122 runs remaining, File_2 is emptied, and File_3 ends up with Fi\\u22121 runs, again a perfect run distribution. For 4 or more files, the math is more complicated, but the concept is the same.\\n\\nComparison versus ordinary merge sort\\nAfter the initial distribution, an ordinary merge sort using 4 files will sort 16 single record runs in 4 iterations of the entire dataset, moving a total of 64 records in order to sort the dataset after the initial distribution. A polyphase merge sort using 4 files will sort 17 single record runs in 4 iterations, but since each iteration but the last iteration only moves a fraction of the dataset, it only moves a total of 48 records in order to sort the dataset after the initial distribution. In this case, ordinary merge sort factor is 2.0, while polyphase overall factor is \\u22482.73.\\nTo explain how the reduction factor is related to sort performance, the reduction factor equations are:\\n\\nreduction_factor = exp(number_of_runs*log(number_of_runs)/run_move_count)\\nrun_move_count = number_of_runs * log(number_of_runs)/log(reduction_factor)\\nrun_move_count = number_of_runs * log_reduction_factor(number_of_runs)\\n\\nUsing the run move count equation for the above examples: \\n\\nordinary merge sort \\u2192 \\n  \\n    \\n      \\n        16\\n        \\u00d7\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        16\\n        )\\n        =\\n        64\\n      \\n    \\n    {\\\\displaystyle 16\\\\times \\\\log _{2}(16)=64}\\n  ,\\npolyphase merge sort \\u2192 \\n  \\n    \\n      \\n        17\\n        \\u00d7\\n        \\n          log\\n          \\n            2.73\\n          \\n        \\n        \\u2061\\n        (\\n        17\\n        )\\n        =\\n        48\\n      \\n    \\n    {\\\\displaystyle 17\\\\times \\\\log _{2.73}(17)=48}\\n  .Here is a table of effective reduction factors for polyphase and ordinary merge sort listed by number of files, based on actual sorts of a few million records. This table roughly corresponds to the reduction factor per dataset moved tables shown in fig 3 and fig 4 of polyphase merge sort.pdf\\n\\n# files\\n|     average fraction of data per iteration\\n|     |     polyphase reduction factor on ideal sized data\\n|     |     |     ordinary reduction factor on ideal sized data\\n|     |     |     |\\n3     .73   1.94  1.41  (sqrt  2)\\n4     .63   2.68  2.00\\n5     .58   3.20  2.45  (sqrt  6)\\n6     .56   3.56  3.00\\n7     .55   3.80  3.46  (sqrt 12)\\n8     .54   3.95  4.00\\n9     .53   4.07  4.47  (sqrt 20)\\n10    .53   4.15  5.00\\n11    .53   4.22  5.48  (sqrt 30)\\n12    .53   4.28  6.00\\n32    .53   4.87 16.00\\n\\nIn general, polyphase merge sort is better than ordinary merge sort when there are fewer than 8 files, while ordinary merge sort starts to become better at around 8 or more files.\\n\\nReferences\\nFurther reading\\nBradley, James (1982), File and Data Base Techniques, Holt, Rinehart and Winston, ISBN 0-03-058673-9\\nReynolds, Samuel W. (August 1961), \\\"A generalized polyphase merge algorithm\\\", Communications of the ACM, New York, NY: ACM, 4 (8): 347\\u2013349, doi:10.1145/366678.366689, S2CID 28416100\\nSedgewick, Robert (1983), Algorithms, Addison-Wesley, pp. 163\\u2013165, ISBN 0-201-06672-6\\n\\n\\n== External links ==\"}, {\"Tree sort\": \"A tree sort is a sort algorithm that builds a binary search tree from the elements to be sorted, and then traverses the tree (in-order) so that the elements come out in sorted order. Its typical use is sorting elements online: after each insertion, the set of elements seen so far is available in sorted order.\\nTree sort can be used as a one-time sort, but it is equivalent to quicksort as both recursively partition the elements based on a pivot, and since quicksort is in-place and has lower overhead, tree sort has few advantages over quicksort. It has better worst case complexity when a self-balancing tree is used, but even more overhead.\\n\\nEfficiency\\nAdding one item to a binary search tree is on average an O(log n) process (in big O notation). Adding n items is an O(n log n) process, making tree sorting a 'fast sort' process. Adding an item to an unbalanced binary tree requires O(n) time in the worst-case: When the tree resembles a linked list (degenerate tree). This results in a worst case of O(n\\u00b2) time for this sorting algorithm.\\nThis worst case occurs when the algorithm operates on an already sorted set, or one that is nearly sorted, reversed or nearly reversed. Expected O(n log n) time can however be achieved by shuffling the array, but this does not help for equal items.\\nThe worst-case behaviour can be improved by using a self-balancing binary search tree. Using such a tree, the algorithm has an O(n log n) worst-case performance, thus being degree-optimal for a comparison sort. However, tree sort algorithms require separate memory to be allocated for the tree, as opposed to in-place algorithms such as quicksort or heapsort. On most common platforms, this means that heap memory has to be used, which is a significant performance hit when compared to quicksort and heapsort. When using a splay tree as the binary search tree, the resulting algorithm (called splaysort) has the additional property that it is an adaptive sort, meaning that its running time is faster than O(n log n) for inputs that are nearly sorted.\\n\\nExample\\nThe following tree sort algorithm in pseudocode accepts a collection of comparable items and outputs the items in ascending order:\\n\\nIn a simple functional programming form, the algorithm (in Haskell) would look something like this:\\n\\nIn the above implementation, both the insertion algorithm and the retrieval algorithm have O(n\\u00b2) worst-case scenarios.\\n\\nExternal links\\n\\nBinary Tree Java Applet and Explanation at the Wayback Machine (archived 29 November 2016)\\nTree Sort of a Linked List\\nTree Sort in C++\\n\\n\\n== References ==\"}, {\"Category:Selection algorithms\": \"\"}, {\"Selection algorithm\": \"In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n)-time (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.\\nThe simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, keeping track of the running minimum \\u2013 the minimum so far \\u2013 (or maximum) and can be seen as related to the selection sort. Conversely, the hardest case of a selection algorithm is finding the median. In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in median of medians. The best-known selection algorithm is Quickselect, which is related to Quicksort; like Quicksort, it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well.\\n\\nSelection by sorting\\nBy sorting the list or array then selecting the desired element, selection can be reduced to sorting. This method is inefficient for selecting a single element, but is efficient when many selections need to be made from an array, in which case only one initial, expensive sort is needed, followed by many cheap selection operations \\u2013 O(1) for an array, though selection is O(n) in a linked list, even if sorted, due to lack of random access. In general, sorting requires O(n log n) time, where n is the length of the list, although a lower bound is possible with non-comparative sorting algorithms like radix sort and counting sort.\\nRather than sorting the whole list or array, one can instead use partial sorting to select the k smallest or k largest elements. The kth smallest (resp., kth largest element) is then the largest (resp., smallest element) of the partially sorted list \\u2013 this then takes O(1) to access in an array and O(k) to access in a list.\\n\\nUnordered partial sorting\\nIf partial sorting is relaxed so that the k smallest elements are returned, but not in order, the factor of O(k log k) can be eliminated. An additional maximum selection (taking O(k) time) is required, but since \\n  \\n    \\n      \\n        k\\n        \\u2264\\n        n\\n      \\n    \\n    {\\\\displaystyle k\\\\leq n}\\n  , this still yields asymptotic complexity of O(n). In fact, partition-based selection algorithms yield both the kth smallest element itself and the k smallest elements (with other elements not in order). This can be done in O(n) time \\u2013 average complexity of Quickselect, and worst-case complexity of refined partition-based selection algorithms.\\nConversely, given a selection algorithm, one can easily get an unordered partial sort (k smallest elements, not in order) in O(n) time by iterating through the list and recording all elements less than the kth element. If this results in fewer than k \\u2212 1 elements, any remaining elements equal the kth element. Care must be taken, due to the possibility of equality of elements: one must not include all elements less than or equal to the kth element, as elements greater than the kth element may also be equal to it.\\nThus unordered partial sorting (lowest k elements, but not ordered) and selection of the kth element are very similar problems. Not only do they have the same asymptotic complexity, O(n), but a solution to either one can be converted into a solution to the other by a straightforward algorithm (finding a max of k elements, or filtering elements of a list below a cutoff of the value of the kth element).\\n\\nPartial selection sort\\nA simple example of selection by partial sorting is to use the partial selection sort.\\nThe obvious linear time algorithm to find the minimum (resp. maximum) \\u2013 iterating over the list and keeping track of the minimum (resp. maximum) element so far \\u2013 can be seen as a partial selection sort that selects the 1 smallest element. However, many other partial sorts also reduce to this algorithm for the case k =1, such as a partial heap sort.\\nMore generally, a partial selection sort yields a simple selection algorithm which takes O(kn) time. This is asymptotically inefficient, but can be sufficiently efficient if k is small, and is easy to implement. Concretely, we simply find the minimum value and move it to the beginning, repeating on the remaining list until we have accumulated k elements, and then return the kth element. Here is partial selection sort-based algorithm:\\n\\nfunction select(list[1..n], k)\\n    for i from 1 to k\\n        minIndex = i\\n        minValue = list[i]\\n        for j from i+1 to n do\\n            if list[j] < minValue then\\n                minIndex = j\\n                minValue = list[j]\\n        swap list[i] and list[minIndex]\\n    return list[k]\\n\\nPartition-based selection\\nLinear performance can be achieved by a partition-based selection algorithm, most basically Quickselect. Quickselect is a variant of Quicksort \\u2013 in both one chooses a pivot and then partitions the data by it, but while Quicksort recurses on both sides of the partition, Quickselect only recurses on one side, namely the side on which the desired kth element is. As with Quicksort, this has optimal average performance, in this case linear, but poor worst-case performance, in this case quadratic. This occurs for instance by taking the first element as the pivot and searching for the maximum element, if the data is already sorted. In practice this can be avoided by choosing a random element as pivot, which yields almost certain linear performance. Alternatively, a more careful deterministic pivot strategy can be used, such as median of medians. These are combined in the hybrid introselect algorithm (analogous to introsort), which starts with Quickselect but falls back to median of medians if progress is slow, resulting in both fast average performance and optimal worst-case performance of O(n).\\nThe partition-based algorithms are generally done in place, which thus results in partially sorting the data. They can be done out of place, not changing the original data, at the cost of O(n) additional space.\\n\\nMedian selection as pivot strategy\\nA median-selection algorithm can be used to yield a general selection algorithm or sorting algorithm, by applying it as the pivot strategy in Quickselect or Quicksort; if the median-selection algorithm is asymptotically optimal (linear-time), the resulting selection or sorting algorithm is as well. In fact, an exact median is not necessary \\u2013 an approximate median is sufficient. In the median of medians selection algorithm, the pivot strategy computes an approximate median and uses this as pivot, recursing on a smaller set to compute this pivot. In practice the overhead of pivot computation is significant, so these algorithms are generally not used, but this technique is of theoretical interest in relating selection and sorting algorithms.\\nIn detail, given a median-selection algorithm, one can use it as a pivot strategy in Quickselect, obtaining a selection algorithm. If the median-selection algorithm is optimal, meaning O(n), then the resulting general selection algorithm is also optimal, again meaning linear. This is because Quickselect is a divide and conquer algorithm, and using the median at each pivot means that at each step the search set decreases by half in size, so the overall complexity is a geometric series times the complexity of each step, and thus simply a constant times the complexity of a single step, in fact \\n  \\n    \\n      \\n        2\\n        =\\n        1\\n        \\n          /\\n        \\n        (\\n        1\\n        \\u2212\\n        (\\n        1\\n        \\n          /\\n        \\n        2\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle 2=1/(1-(1/2))}\\n   times (summing the series).\\nSimilarly, given a median-selection algorithm or general selection algorithm applied to find the median, one can use it as a pivot strategy in Quicksort, obtaining a sorting algorithm. If the selection algorithm is optimal, meaning O(n), then the resulting sorting algorithm is optimal, meaning O(n log n). The median is the best pivot for sorting, as it evenly divides the data, and thus guarantees optimal sorting, assuming the selection algorithm is optimal. A sorting analog to median of medians exists, using the pivot strategy (approximate median) in Quicksort, and similarly yields an optimal Quicksort.\\n\\nIncremental sorting by selection\\nConverse to selection by sorting, one can incrementally sort by repeated selection. Abstractly, selection only yields a single element, the kth element. However, practical selection algorithms frequently involve partial sorting, or can be modified to do so. Selecting by partial sorting naturally does so, sorting the elements up to k, and selecting by partitioning also sorts some elements: the pivots are sorted to the correct positions, with the kth element being the final pivot, and the elements between the pivots have values between the pivot values. The difference between partition-based selection and partition-based sorting, as in Quickselect versus Quicksort, is that in selection one recurses on only one side of each pivot, sorting only the pivots (an average of log(n) pivots are used), rather than recursing on both sides of the pivot.\\nThis can be used to speed up subsequent selections on the same data; in the extreme, a fully sorted array allows O(1) selection. Further, compared with first doing a full sort, incrementally sorting by repeated selection amortizes the sorting cost over multiple selections.\\nFor partially sorted data (up to k), so long as the partially sorted data and the index k up to which the data is sorted are recorded, subsequent selections of j less than or equal to k can simply select the jth element, as it is already sorted, while selections of j greater than k only need to sort the elements above the kth position.\\nFor partitioned data, if the list of pivots is stored (for example, in a sorted list of the indices), then subsequent selections only need to select in the interval between two pivots (the nearest pivots below and above). The biggest gain is from the top-level pivots, which eliminate costly large partitions: a single pivot near the middle of the data cuts the time for future selections in half. The pivot list will grow over subsequent selections, as the data becomes more sorted, and can even be passed to a partition-based sort as the basis of a full sort.\\n\\nUsing data structures to select in sublinear time\\nGiven an unorganized list of data, linear time (\\u03a9(n)) is required to find the minimum element, because we have to examine every element (otherwise, we might miss it). If we organize the list, for example by keeping it sorted at all times, then selecting the kth largest element is trivial, but then insertion requires linear time, as do other operations such as combining two lists.\\nThe strategy to find an order statistic in sublinear time is to store the data in an organized fashion using suitable data structures that facilitate the selection. Two such data structures are tree-based structures and frequency tables.\\nWhen only the minimum (or maximum) is needed, a good approach is to use a heap, which is able to find the minimum (or maximum) element in constant time, while all other operations, including insertion, are O(log n) or better. More generally, a self-balancing binary search tree can easily be augmented to make it possible to both insert an element and find the kth largest element in O(log n) time; this is called an order statistic tree. We simply store in each node a count of how many descendants it has, and use this to determine which path to follow. The information can be updated efficiently since adding a node only affects the counts of its O(log n) ancestors, and tree rotations only affect the counts of the nodes involved in the rotation.\\nAnother simple strategy is based on some of the same concepts as the hash table. When we know the range of values beforehand, we can divide that range into h subintervals and assign these to h buckets. When we insert an element, we add it to the bucket corresponding to the interval it falls in. To find the minimum or maximum element, we scan from the beginning or end for the first nonempty bucket and find the minimum or maximum element in that bucket. In general, to find the kth element, we maintain a count of the number of elements in each bucket, then scan the buckets from left to right adding up counts until we find the bucket containing the desired element, then use the expected linear-time algorithm to find the correct element in that bucket.\\nIf we choose h of size roughly sqrt(n), and the input is close to uniformly distributed, this scheme can perform selections in expected O(sqrt(n)) time. Unfortunately, this strategy is also sensitive to clustering of elements in a narrow interval, which may result in buckets with large numbers of elements (clustering can be eliminated through a good hash function, but finding the element with the kth largest hash value isn't very useful). Additionally, like hash tables this structure requires table resizings to maintain efficiency as elements are added and n becomes much larger than h2. A useful case of this is finding an order statistic or extremum in a finite range of data. Using above table with bucket interval 1 and maintaining counts in each bucket is much superior to other methods. Such hash tables are like frequency tables used to classify the data in descriptive statistics.\\n\\nLower bounds\\nIn The Art of Computer Programming, Donald E. Knuth discussed a number of lower bounds for the number of comparisons required to locate the t smallest entries of an unorganized list of n items (using only comparisons). There is a trivial lower bound of n \\u2212 1 for the minimum or maximum entry. To see this, consider a tournament where each game represents one comparison. Since every player except the winner of the tournament must lose a game before we know the winner, we have a lower bound of n \\u2212 1 comparisons.\\nThe story becomes more complex for other indexes. We define \\n  \\n    \\n      \\n        \\n          W\\n          \\n            t\\n          \\n        \\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle W_{t}(n)}\\n   as the minimum number of comparisons required to find the t smallest values. Knuth references a paper published by S. S. Kislitsyn, which shows an upper bound on this value:\\n\\n  \\n    \\n      \\n        \\n          W\\n          \\n            t\\n          \\n        \\n        (\\n        n\\n        )\\n        \\u2264\\n        n\\n        \\u2212\\n        t\\n        +\\n        \\n          \\u2211\\n          \\n            n\\n            +\\n            1\\n            \\u2212\\n            t\\n            <\\n            j\\n            \\u2264\\n            n\\n          \\n        \\n        \\u2308\\n        \\n          \\n            log\\n            \\n              2\\n            \\n          \\n          \\n          j\\n        \\n        \\u2309\\n        \\n        \\n          for\\n        \\n        \\n        n\\n        \\u2265\\n        t\\n      \\n    \\n    {\\\\displaystyle W_{t}(n)\\\\leq n-t+\\\\sum _{n+1-t<j\\\\leq n}\\\\lceil {\\\\log _{2}\\\\,j}\\\\rceil \\\\quad {\\\\text{for}}\\\\,n\\\\geq t}\\n  This bound is achievable for t=2 but better, more complex bounds are known for larger t.\\n\\nSpace complexity\\nThe required space complexity of selection is O(1) additional storage, in addition to storing the array in which selection is being performed. Such space complexity can be achieved while preserving optimal O(n) time complexity.\\n\\nOnline selection algorithm\\nOnline selection may refer narrowly to computing the kth smallest element of a stream, in which case partial sorting algorithms \\u2014 with k + O(1) space for the k smallest elements so far \\u2014 can be used, but partition-based algorithms cannot be.\\nAlternatively, selection itself may be required to be online, that is, an element can only be selected from a sequential input at the instance of observation and each selection, respectively refusal, is irrevocable. The problem is to select, under these constraints, a specific element of the input sequence (as for example the largest or the smallest value)\\twith largest probability. This problem can be tackled by the Odds algorithm, which yields the optimal under an independence condition; it is also optimal itself as an algorithm with the number of computations being linear in the length of input.\\nThe simplest example is the secretary problem of choosing the maximum with high probability, in which case optimal strategy (on random data) is to track the running maximum of the first n/e elements and reject them, and then select the first element that is higher than this maximum.\\n\\nRelated problems\\nOne may generalize the selection problem to apply to ranges within a list, yielding the problem of range queries. The question of range median queries (computing the medians of multiple ranges) has been analyzed.\\n\\nLanguage support\\nVery few languages have built-in support for general selection, although many provide facilities for finding the smallest or largest element of a list. A notable exception is C++, which provides a templated nth_element method with a guarantee of expected linear time, and also partitions the data, requiring that the nth element be sorted into its correct place, elements before the nth element are less than it, and elements after the nth element are greater than it. It is implied but not required that it is based on Hoare's algorithm (or some variant) by its requirement of expected linear time and partitioning of data.For Perl, the module Sort::Key::Top, available from CPAN, provides a set of functions to select the top n elements from a list using several orderings and custom key extraction procedures. Furthermore, the Statistics::CaseResampling module provides a function to calculate quantiles using Quickselect.\\nPython's standard library (since 2.4) includes heapq.nsmallest() and nlargest(), returning sorted lists, in O(n log k) time. Numpy has the partition() function.\\nMatlab includes maxk() and mink() functions, which return the maximal (minimal) k values in a vector as well as their indices.\\nBecause language support for sorting is more ubiquitous, the simplistic approach of sorting followed by indexing is preferred in many environments despite its disadvantage in speed. Indeed, for lazy languages, this simplistic approach can even achieve the best complexity possible for the k smallest/greatest sorted (with maximum/minimum as a special case) if the sort is lazy enough.\\n\\nSee also\\nOrdinal optimization\\nSearch algorithm\\n\\nReferences\\nExternal links\\n\\\"Lecture notes for January 25, 1996: Selection and order statistics\\\", ICS 161: Design and Analysis of Algorithms, David Eppstein\"}, {\"BFPRT\": \"In computer science, the median of medians is an approximate median selection algorithm, frequently used to supply a good pivot for an exact selection algorithm, most commonly quickselect, that selects the kth smallest element of an initially unsorted array. Median of medians finds an approximate median in linear time. Using this approximate median as an improved pivot, the worst-case complexity of quickselect reduces from quadratic to linear, which is also the asymptotically optimal worst-case complexity of any selection algorithm. In other words, the median of medians is an approximate median-selection algorithm that helps building an asymptotically optimal, exact general selection algorithm (especially in the sense of worst-case complexity), by producing good pivot elements.\\nMedian of medians can also be used as a pivot strategy in quicksort, yielding an optimal algorithm, with worst-case complexity \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n  . Although this approach optimizes the asymptotic worst-case complexity quite well, it is typically outperformed in practice by instead choosing random pivots for its average \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   complexity for selection and average \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   complexity for sorting, without any overhead of computing the pivot.\\nSimilarly, Median of medians is used in the hybrid introselect algorithm as a fallback for pivot selection at each iteration until kth smallest is found. This again ensures a worst-case linear performance, in addition to average-case linear performance: introselect starts with quickselect (with random pivot, default), to obtain good average performance, and then falls back to modified quickselect with pivot obtained from median of medians if the progress is too slow. Even though asymptotically similar, such a hybrid algorithm will have a lower complexity than a straightforward introselect up to a constant factor (both in average-case and worst-case), at any finite length.\\nThe algorithm was published in Blum et al. (1973), and thus is sometimes called BFPRT after the last names of the authors. In the original paper the algorithm was referred to as PICK, referring to quickselect as \\\"FIND\\\".\\n\\nMotivation\\nQuickselect is linear-time on average, but it can require quadratic time with poor pivot choices. This is because quickselect is a divide and conquer algorithm, with each step taking \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time in the size of the remaining search set. If the search set decreases exponentially quickly in size (by a fixed proportion), this yields a geometric series times the \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   factor of a single step, and thus linear overall time. However, if the search set decreases slowly in size, such as linearly (by a fixed number of elements, in the worst case only reducing by one element each time), then a linear sum of linear steps yields quadratic overall time (formally, triangular numbers grow quadratically). For example, the worst-case occurs when pivoting on the smallest element at each step, such as applying quickselect for the maximum element to already sorted data and taking the first element as pivot each time.\\nIf one instead consistently chooses \\\"good\\\" pivots, this is avoided and one always gets linear performance even in the worst case. A \\\"good\\\" pivot is one for which we can establish that a constant proportion of elements fall both below and above it, as then the search set decreases at least by a constant proportion at each step, hence exponentially quickly, and the overall time remains linear. The median is a good pivot \\u2013 the best for sorting, and the best overall choice for selection \\u2013 decreasing the search set by half at each step. Thus if one can compute the median in linear time, this only adds linear time to each step, and thus the overall complexity of the algorithm remains linear.\\nThe median-of-medians algorithm computes an approximate median, namely a point that is guaranteed to be between the 30th and 70th percentiles (in the middle 4 deciles). Thus the search set decreases by at least 30%. The problem is reduced to 70% of the original size, which is a fixed proportion smaller. Applying the same algorithm on the now smaller set recursively until only one or two elements remain results in a cost of \\n  \\n    \\n      \\n        \\n          \\n            n\\n            \\n              1\\n              \\u2212\\n              0.7\\n            \\n          \\n        \\n        \\u2248\\n        3.33\\n        \\n          n\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{1-0.7}}\\\\approx 3.33{n}}\\n\\nAlgorithm\\nAs stated before, median-of-medians is used as a pivot selection strategy in the quickselect algorithm, which in pseudocode looks as follows. Be careful to handle left, right and n when implementing. The following pseudocode assumes that left, right, and the list use one-based numbering and that select is initially called with 1 as the argument to left and the length of the list as the argument to right. Note that this returns the index of the n'th smallest number after rearranging the list, rather than the actual value of the n'th smallest number.\\n\\nfunction select(list, left, right, n)\\n    loop\\n        if left = right then\\n            return left\\n        pivotIndex := pivot(list, left, right)\\n        pivotIndex := partition(list, left, right, pivotIndex, n)\\n        if n = pivotIndex then\\n            return n\\n        else if n < pivotIndex then\\n            right := pivotIndex - 1\\n        else\\n            left := pivotIndex + 1\\n\\nSubroutine pivot is the actual median-of-medians algorithm. It divides its input (a list of length n) into groups of at most five elements, computes the median of each of those groups using some subroutine, then recursively computes the true median of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n   medians found in the previous step:. Note that pivot calls select; this is an instance of mutual recursion.\\n\\nfunction pivot(list, left, right)\\n    // for 5 or less elements just get median\\n    if right \\u2212 left < 5 then\\n        return partition5(list, left, right)\\n    // otherwise move the medians of five-element subgroups to the first n/5 positions\\n    for i from left to right in steps of 5\\n        // get the median position of the i'th five-element subgroup\\n        subRight := i + 4\\n        if subRight > right then\\n            subRight := right\\n        median5 := partition5(list, i, subRight)\\n        swap list[median5] and list[left + floor((i \\u2212 left)/5)]\\n\\n    // compute the median of the n/5 medians-of-five\\n    mid := (right \\u2212 left) / 10 + left + 1\\n    return select(list, left, left + floor((right \\u2212 left) / 5), mid)\\n\\nPartition helper functions\\nThere is a subroutine called partition that can, in linear time, group a list (ranging from indices left to right) into three parts, those less than a certain element, those equal to it, and those greater than the element (a three-way partition). The grouping into three parts ensures that the median-of-medians maintains linear execution time in a case of many or all coincident elements. Here is pseudocode that performs a partition about the element list[pivotIndex]:\\n\\nfunction partition(list, left, right, pivotIndex, n)\\n    pivotValue := list[pivotIndex]\\n    swap list[pivotIndex] and list[right]  // Move pivot to end\\n    storeIndex := left\\n    // Move all elements smaller than the pivot to the left of the pivot\\n    for i from left to right \\u2212 1 do\\n        if list[i] < pivotValue then\\n            swap list[storeIndex] and list[i]\\n            increment storeIndex\\n    // Move all elements equal to the pivot right after\\n    // the smaller elements\\n    storeIndexEq = storeIndex\\n    for i from storeIndex to right \\u2212 1 do\\n        if list[i] = pivotValue then\\n            swap list[storeIndexEq] and list[i]\\n            increment storeIndexEq\\n    swap list[right] and list[storeIndexEq]  // Move pivot to its final place\\n    // Return location of pivot considering the desired location n\\n    if n < storeIndex then\\n        return storeIndex  // n is in the group of smaller elements\\n    if n \\u2264 storeIndexEq then\\n        return n  // n is in the group equal to pivot\\n    return storeIndexEq // n is in the group of larger elements\\n\\nThe partition5 subroutine selects the median of a group of at most five elements; an easy way to implement this is insertion sort, as shown below. It can also be implemented as a decision tree.\\n\\nfunction partition5( list, left, right)\\n    i := left + 1\\n    while i \\u2264 right\\n        j := i\\n        while j > left and list[j\\u22121] > list[j] do\\n            swap list[j\\u22121] and list[j]\\n            j := j \\u2212 1\\n        i :=  i + 2\\n            \\n    return floor((left + right) / 2)\\n\\nProperties of pivot\\nOf the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n   groups, half the number of groups (\\n  \\n    \\n      \\n        \\n          \\n            1\\n            2\\n          \\n        \\n        \\u00d7\\n        \\n          \\n            n\\n            5\\n          \\n        \\n        =\\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {1}{2}}\\\\times {\\\\frac {n}{5}}={\\\\frac {n}{10}}}\\n  ) have their median less than the pivot (Median of Medians). Also, another half the number of groups (again, \\n  \\n    \\n      \\n        \\n          \\n            1\\n            2\\n          \\n        \\n        \\u00d7\\n        \\n          \\n            n\\n            5\\n          \\n        \\n        =\\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {1}{2}}\\\\times {\\\\frac {n}{5}}={\\\\frac {n}{10}}}\\n  ) have their median greater than the pivot. In each of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{10}}}\\n   groups with median less than the pivot, there are two elements that are smaller than their respective medians, which are smaller than the pivot. Thus, each of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{10}}}\\n   groups have at least 3 elements that are smaller than the pivot. Similarly, in each of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{10}}}\\n   groups with median greater than the pivot, there are two elements that are greater than their respective medians, which are greater than the pivot. Thus, each of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{10}}}\\n   groups have at least 3 elements that are greater than the pivot. Hence, the pivot is less than \\n  \\n    \\n      \\n        3\\n        \\u00d7\\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 3\\\\times {\\\\frac {n}{10}}}\\n   elements and greater than another \\n  \\n    \\n      \\n        3\\n        \\u00d7\\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 3\\\\times {\\\\frac {n}{10}}}\\n   elements. Thus the chosen median splits the ordered elements somewhere between 30%/70% and 70%/30%, which assures worst-case linear behavior of the algorithm. To visualize:\\n\\n(red = \\\"(one of the two possible) median of medians\\\", gray = \\\"number < red\\\", white = \\\"number > red\\\")\\n5-tuples are shown here sorted by median, for clarity.  Sorting the tuples is not necessary because we only need the median for use as pivot element.\\nNote that all elements above/left of the red (30% of the 100 elements) are less, and all elements below/right of the red (another 30% of the 100 elements) are greater.\\n\\nProof of O(n) running time\\nThe median-calculating recursive call does not exceed worst-case linear behavior because the list of medians has size \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n  , while the other recursive call recurses on at most 70% of the list. Let \\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle T(n)}\\n   be the time it takes to run a median-of-medians Quickselect algorithm on an array of size \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  . Then we know this time is:\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        \\u2264\\n        T\\n        (\\n        n\\n        \\n          /\\n        \\n        5\\n        )\\n        +\\n        T\\n        (\\n        n\\n        \\u22c5\\n        7\\n        \\n          /\\n        \\n        10\\n        )\\n        +\\n        c\\n        \\u22c5\\n        n\\n        ,\\n      \\n    \\n    {\\\\displaystyle T(n)\\\\leq T(n/5)+T(n\\\\cdot 7/10)+c\\\\cdot n,}\\n  where\\n\\nthe \\n  \\n    \\n      \\n        T\\n        \\n          (\\n          \\n            \\n              n\\n              5\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle T\\\\left({\\\\frac {n}{5}}\\\\right)}\\n   part is for finding the true median of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n   medians, by running an (independent) Quickselect on them (since finding the median is just a special case of selecting a k-smallest element)\\nthe \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   term \\n  \\n    \\n      \\n        c\\n        \\u22c5\\n        n\\n      \\n    \\n    {\\\\displaystyle c\\\\cdot n}\\n   is for the partitioning work to create the two sides, one of which our Quickselect will recurse (we visited each element a constant number of times in order to form them into n/5 groups and take each median in \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   time).\\nthe \\n  \\n    \\n      \\n        T\\n        \\n          (\\n          \\n            \\n              \\n                7\\n                n\\n              \\n              10\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle T\\\\left({\\\\frac {7n}{10}}\\\\right)}\\n   part is for the actual Quickselect recursion (for the worst case, in which the k-th element is in the bigger partition that can be of size \\n  \\n    \\n      \\n        \\n          \\n            \\n              7\\n              n\\n            \\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {7n}{10}}}\\n   maximally)By induction:\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        \\u2264\\n        10\\n        \\u22c5\\n        c\\n        \\u22c5\\n        n\\n        \\u2208\\n        O\\n        (\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle T(n)\\\\leq 10\\\\cdot c\\\\cdot n\\\\in O(n).}\\n\\nAnalysis\\nThe key step is reducing the problem to selecting in two lists whose total length is shorter than the original list, plus a linear factor for the reduction step. This allows a simple induction to show that the overall running time is linear.\\nThe specific choice of groups of five elements is explained as follows. Firstly, computing median of an odd list is faster and simpler; while one could use an even list, this requires taking the average of the two middle elements, which is slower than simply selecting the single exact middle element. Secondly, five is the smallest odd number such that median of medians works. With groups of only three elements, the resulting list of medians to search in is length \\n  \\n    \\n      \\n        \\n          \\n            n\\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{3}}}\\n  , and reduces the list to recurse into length \\n  \\n    \\n      \\n        \\n          \\n            2\\n            3\\n          \\n        \\n        n\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {2}{3}}n}\\n  , since it is greater than 1/2 \\u00d7 2/3 = 1/3 of the elements and less than 1/2 \\u00d7 2/3 = 1/3 of the elements. Thus this still leaves \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements to search in, not reducing the problem sufficiently. The individual lists are shorter, however, and one can bound the resulting complexity to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   by the Akra\\u2013Bazzi method, but it does not prove linearity.\\nConversely, one may instead group by \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   = seven, nine, or more elements, and this does work. This reduces the size of the list of medians to \\n  \\n    \\n      \\n        \\n          \\n            n\\n            g\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{g}}}\\n  , and the size of the list to recurse into asymptotes at 3n/4 (75%), as the quadrants in the above table approximate 25%, as the size of the overlapping lines decreases proportionally. This reduces the scaling factor from 10 asymptotically to 4, but accordingly raises the \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   term for the partitioning work. Finding the median of a larger group takes longer, but is a constant factor (depending only on \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  ), and thus does not change the overall performance as n grows. In fact, considering the number of comparisons in the worst case, the constant factor is \\n  \\n    \\n      \\n        \\n          \\n            \\n              2\\n              g\\n              (\\n              g\\n              \\u2212\\n              1\\n              )\\n            \\n            \\n              g\\n              \\u2212\\n              3\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {2g(g-1)}{g-3}}}\\n  .\\nIf one instead groups the other way, say dividing the \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   element list into 5 lists, computing the median of each, and then computing the median of these \\u2013 i.e., grouping by a constant fraction, not a constant number \\u2013 one does not as clearly reduce the problem, since it requires computing 5 medians, each in a list of \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n   elements, and then recursing on a list of length at most \\n  \\n    \\n      \\n        \\n          \\n            7\\n            10\\n          \\n        \\n        n\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {7}{10}}n}\\n  . As with grouping by 3, the individual lists are shorter, but the overall length is no shorter \\u2013 in fact longer \\u2013 and thus one can only prove superlinear bounds. Grouping into a square of \\n  \\n    \\n      \\n        \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {n}}}\\n   lists of length \\n  \\n    \\n      \\n        \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {n}}}\\n   is similarly complicated.\\n\\nReferences\\nExternal links\\n\\\"Lecture notes for January 30, 1996: Deterministic selection\\\", ICS 161: Design and Analysis of Algorithms, David Eppstein\"}, {\"Floyd\\u2013Rivest algorithm\": \"In computer science, the Floyd-Rivest algorithm is a selection algorithm developed by Robert W. Floyd and Ronald L. Rivest that has an optimal expected number of comparisons within lower-order terms. It is functionally equivalent to quickselect, but runs faster in practice on average. It has an expected running time of O(n) and an expected number of comparisons of n + min(k, n \\u2212 k) + O(n1/2 log1/2 n).\\nThe algorithm was originally presented in a Stanford University technical report containing two papers, where it was referred to as SELECT and paired with PICK, or median of medians. It was subsequently published in Communications of the ACM, Volume 18: Issue 3.\\n\\nAlgorithm\\nThe Floyd-Rivest algorithm is a divide and conquer algorithm, sharing many similarities with quickselect. It uses sampling to help partition the list into three sets. It then recursively selects the kth smallest element from the appropriate set.\\nThe general steps are:\\n\\nSelect a small random sample S from the list L.\\nFrom S, recursively select two elements, u and v, such that u < v. These two elements will be the pivots for the partition and are expected to contain the kth smallest element of the entire list between them (in a sorted list).\\nUsing u and v, partition S into three sets: A, B, and C. A will contain the elements with values less than u, B will contain the elements with values between u and v, and C will contain the elements with values greater than v.\\nPartition the remaining elements in L (that is, the elements in L - S) by comparing them to u or v and placing them into the appropriate set. If k is smaller than half the number of the elements in L rounded up, then the remaining elements should be compared to v first and then only to u if they are smaller than v. Otherwise, the remaining elements should be compared to u first and only to v if they are greater than u.\\nBased on the value of k, apply the algorithm recursively to the appropriate set to select the kth smallest element in L.By using |S| = \\u0398(n2/3 log1/3 n), we can get n + min(k, n \\u2212 k) + O(n2/3 log1/3 n) expected comparisons.  We can get n + min(k, n \\u2212 k) + O(n1/2 log1/2 n) expected comparisons by starting with a small S and repeatedly updating u and v to keep the size of B small enough (O(n1/2 log1/2 n) at \\u0398(n) processed elements) without unacceptable risk of the desired element being outside of B.\\n\\nPseudocode version\\nThe following pseudocode rearranges the elements between left and right, such that for some value k, where left \\u2264 k \\u2264 right, the kth element in the list will contain the (k \\u2212 left + 1)th smallest value, with the ith element being less than or equal to the kth for all left \\u2264 i \\u2264 k and the jth element being larger or equal to for k \\u2264 j \\u2264 right:\\n\\n// left is the left index for the interval\\n// right is the right index for the interval\\n// k is the desired index value, where array[k] is the (k+1)th smallest element when left = 0\\nfunction select(array, left, right, k) is\\n    while right > left do\\n        // Use select recursively to sample a smaller set of size s\\n        // the arbitrary constants 600 and 0.5 are used in the original\\n        // version to minimize execution time.\\n        if right \\u2212 left > 600 then\\n            n := right \\u2212 left + 1\\n            i := k \\u2212 left + 1\\n            z := ln(n)\\n            s := 0.5 \\u00d7 exp(2 \\u00d7 z/3)\\n            sd := 0.5 \\u00d7 sqrt(z \\u00d7 s \\u00d7 (n \\u2212 s)/n) \\u00d7 sign(i \\u2212 n/2)\\n            newLeft := max(left, k \\u2212 i \\u00d7 s/n + sd)\\n            newRight := min(right, k + (n \\u2212 i) \\u00d7 s/n + sd)\\n            select(array, newLeft, newRight, k)\\n        // partition the elements between left and right around t\\n        t := array[k] \\n        i := left\\n        j := right\\n        swap array[left] and array[k]\\n        if array[right] > t then\\n            swap array[right] and array[left]\\n        while i < j do\\n            swap array[i] and array[j]\\n            i := i + 1\\n            j := j \\u2212 1\\n            while array[i] < t do\\n                i := i + 1\\n            while array[j] > t do\\n                j := j \\u2212 1\\n        if array[left] = t then\\n            swap array[left] and array[j]\\n        else\\n            j := j + 1\\n            swap array[j] and array[right]\\n        // Adjust left and right towards the boundaries of the subset\\n        // containing the (k \\u2212 left + 1)th smallest element.\\n        if j \\u2264 k then\\n            left := j + 1\\n        if k \\u2264 j then\\n            right := j \\u2212 1\\n\\nSee also\\nQuickselect\\nIntroselect\\nMedian of medians\\n\\n\\n== References ==\"}, {\"Introselect\": \"In computer science, introselect (short for \\\"introspective selection\\\") is a selection algorithm that is a hybrid of quickselect and median of medians which has fast average performance and optimal worst-case performance. Introselect is related to the introsort sorting algorithm: these are analogous refinements of the basic quickselect and quicksort algorithms, in that they both start with the quick algorithm, which has good average performance and low overhead, but fall back to an optimal worst-case algorithm (with higher overhead) if the quick algorithm does not progress rapidly enough. Both algorithms were introduced by David Musser in (Musser 1997), with the purpose of providing generic algorithms for the C++ Standard Library that have both fast average performance and optimal worst-case performance, thus allowing the performance requirements to be tightened.However, in most C++ Standard Library implementations, a different \\\"introselect\\\" algorithm is used, which combines quickselect and heapselect, and has a worst-case running time of O(n log n). The C++ draft standard, as of 2022, does not have requirements on the worst-case performance, therefore allowing such choice.\\n\\nAlgorithms\\nIntrosort achieves practical performance comparable to quicksort while preserving O(n log n) worst-case behavior by creating a hybrid of quicksort and heapsort. Introsort starts with quicksort, so it achieves performance similar to quicksort if quicksort works, and falls back to heapsort (which has optimal worst-case performance) if quicksort does not progress quickly enough. Similarly, introselect combines quickselect with median of medians to achieve worst-case linear selection with performance similar to quickselect.\\nIntroselect works by optimistically starting out with quickselect and only switching to a worst-case linear-time selection algorithm (the Blum-Floyd-Pratt-Rivest-Tarjan median of medians algorithm) if it recurses too many times without making sufficient progress. The switching strategy is the main technical content of the algorithm. Simply limiting the recursion to constant depth is not good enough, since this would make the algorithm switch on all sufficiently large lists. Musser discusses a couple of simple approaches:\\n\\nKeep track of the list of sizes of the subpartitions processed so far. If at any point k recursive calls have been made without halving the list size, for some small positive k, switch to the worst-case linear algorithm.\\nSum the size of all partitions generated so far. If this exceeds the list size times some small positive constant k, switch to the worst-case linear algorithm. This sum is easy to track in a single scalar variable.Both approaches limit the recursion depth to k \\u2308log n\\u2309 = O(log n) and the total running time to O(n).\\nThe paper suggested that more research on introselect was forthcoming, but the author retired in 2007 without having published any such further research.\\n\\nSee also\\nFloyd\\u2013Rivest algorithm\\n\\n\\n== References ==\"}, {\"Median of medians\": \"In computer science, the median of medians is an approximate median selection algorithm, frequently used to supply a good pivot for an exact selection algorithm, most commonly quickselect, that selects the kth smallest element of an initially unsorted array. Median of medians finds an approximate median in linear time. Using this approximate median as an improved pivot, the worst-case complexity of quickselect reduces from quadratic to linear, which is also the asymptotically optimal worst-case complexity of any selection algorithm. In other words, the median of medians is an approximate median-selection algorithm that helps building an asymptotically optimal, exact general selection algorithm (especially in the sense of worst-case complexity), by producing good pivot elements.\\nMedian of medians can also be used as a pivot strategy in quicksort, yielding an optimal algorithm, with worst-case complexity \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n  . Although this approach optimizes the asymptotic worst-case complexity quite well, it is typically outperformed in practice by instead choosing random pivots for its average \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   complexity for selection and average \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   complexity for sorting, without any overhead of computing the pivot.\\nSimilarly, Median of medians is used in the hybrid introselect algorithm as a fallback for pivot selection at each iteration until kth smallest is found. This again ensures a worst-case linear performance, in addition to average-case linear performance: introselect starts with quickselect (with random pivot, default), to obtain good average performance, and then falls back to modified quickselect with pivot obtained from median of medians if the progress is too slow. Even though asymptotically similar, such a hybrid algorithm will have a lower complexity than a straightforward introselect up to a constant factor (both in average-case and worst-case), at any finite length.\\nThe algorithm was published in Blum et al. (1973), and thus is sometimes called BFPRT after the last names of the authors. In the original paper the algorithm was referred to as PICK, referring to quickselect as \\\"FIND\\\".\\n\\nMotivation\\nQuickselect is linear-time on average, but it can require quadratic time with poor pivot choices. This is because quickselect is a divide and conquer algorithm, with each step taking \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time in the size of the remaining search set. If the search set decreases exponentially quickly in size (by a fixed proportion), this yields a geometric series times the \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   factor of a single step, and thus linear overall time. However, if the search set decreases slowly in size, such as linearly (by a fixed number of elements, in the worst case only reducing by one element each time), then a linear sum of linear steps yields quadratic overall time (formally, triangular numbers grow quadratically). For example, the worst-case occurs when pivoting on the smallest element at each step, such as applying quickselect for the maximum element to already sorted data and taking the first element as pivot each time.\\nIf one instead consistently chooses \\\"good\\\" pivots, this is avoided and one always gets linear performance even in the worst case. A \\\"good\\\" pivot is one for which we can establish that a constant proportion of elements fall both below and above it, as then the search set decreases at least by a constant proportion at each step, hence exponentially quickly, and the overall time remains linear. The median is a good pivot \\u2013 the best for sorting, and the best overall choice for selection \\u2013 decreasing the search set by half at each step. Thus if one can compute the median in linear time, this only adds linear time to each step, and thus the overall complexity of the algorithm remains linear.\\nThe median-of-medians algorithm computes an approximate median, namely a point that is guaranteed to be between the 30th and 70th percentiles (in the middle 4 deciles). Thus the search set decreases by at least 30%. The problem is reduced to 70% of the original size, which is a fixed proportion smaller. Applying the same algorithm on the now smaller set recursively until only one or two elements remain results in a cost of \\n  \\n    \\n      \\n        \\n          \\n            n\\n            \\n              1\\n              \\u2212\\n              0.7\\n            \\n          \\n        \\n        \\u2248\\n        3.33\\n        \\n          n\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{1-0.7}}\\\\approx 3.33{n}}\\n\\nAlgorithm\\nAs stated before, median-of-medians is used as a pivot selection strategy in the quickselect algorithm, which in pseudocode looks as follows. Be careful to handle left, right and n when implementing. The following pseudocode assumes that left, right, and the list use one-based numbering and that select is initially called with 1 as the argument to left and the length of the list as the argument to right. Note that this returns the index of the n'th smallest number after rearranging the list, rather than the actual value of the n'th smallest number.\\n\\nfunction select(list, left, right, n)\\n    loop\\n        if left = right then\\n            return left\\n        pivotIndex := pivot(list, left, right)\\n        pivotIndex := partition(list, left, right, pivotIndex, n)\\n        if n = pivotIndex then\\n            return n\\n        else if n < pivotIndex then\\n            right := pivotIndex - 1\\n        else\\n            left := pivotIndex + 1\\n\\nSubroutine pivot is the actual median-of-medians algorithm. It divides its input (a list of length n) into groups of at most five elements, computes the median of each of those groups using some subroutine, then recursively computes the true median of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n   medians found in the previous step:. Note that pivot calls select; this is an instance of mutual recursion.\\n\\nfunction pivot(list, left, right)\\n    // for 5 or less elements just get median\\n    if right \\u2212 left < 5 then\\n        return partition5(list, left, right)\\n    // otherwise move the medians of five-element subgroups to the first n/5 positions\\n    for i from left to right in steps of 5\\n        // get the median position of the i'th five-element subgroup\\n        subRight := i + 4\\n        if subRight > right then\\n            subRight := right\\n        median5 := partition5(list, i, subRight)\\n        swap list[median5] and list[left + floor((i \\u2212 left)/5)]\\n\\n    // compute the median of the n/5 medians-of-five\\n    mid := (right \\u2212 left) / 10 + left + 1\\n    return select(list, left, left + floor((right \\u2212 left) / 5), mid)\\n\\nPartition helper functions\\nThere is a subroutine called partition that can, in linear time, group a list (ranging from indices left to right) into three parts, those less than a certain element, those equal to it, and those greater than the element (a three-way partition). The grouping into three parts ensures that the median-of-medians maintains linear execution time in a case of many or all coincident elements. Here is pseudocode that performs a partition about the element list[pivotIndex]:\\n\\nfunction partition(list, left, right, pivotIndex, n)\\n    pivotValue := list[pivotIndex]\\n    swap list[pivotIndex] and list[right]  // Move pivot to end\\n    storeIndex := left\\n    // Move all elements smaller than the pivot to the left of the pivot\\n    for i from left to right \\u2212 1 do\\n        if list[i] < pivotValue then\\n            swap list[storeIndex] and list[i]\\n            increment storeIndex\\n    // Move all elements equal to the pivot right after\\n    // the smaller elements\\n    storeIndexEq = storeIndex\\n    for i from storeIndex to right \\u2212 1 do\\n        if list[i] = pivotValue then\\n            swap list[storeIndexEq] and list[i]\\n            increment storeIndexEq\\n    swap list[right] and list[storeIndexEq]  // Move pivot to its final place\\n    // Return location of pivot considering the desired location n\\n    if n < storeIndex then\\n        return storeIndex  // n is in the group of smaller elements\\n    if n \\u2264 storeIndexEq then\\n        return n  // n is in the group equal to pivot\\n    return storeIndexEq // n is in the group of larger elements\\n\\nThe partition5 subroutine selects the median of a group of at most five elements; an easy way to implement this is insertion sort, as shown below. It can also be implemented as a decision tree.\\n\\nfunction partition5( list, left, right)\\n    i := left + 1\\n    while i \\u2264 right\\n        j := i\\n        while j > left and list[j\\u22121] > list[j] do\\n            swap list[j\\u22121] and list[j]\\n            j := j \\u2212 1\\n        i :=  i + 2\\n            \\n    return floor((left + right) / 2)\\n\\nProperties of pivot\\nOf the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n   groups, half the number of groups (\\n  \\n    \\n      \\n        \\n          \\n            1\\n            2\\n          \\n        \\n        \\u00d7\\n        \\n          \\n            n\\n            5\\n          \\n        \\n        =\\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {1}{2}}\\\\times {\\\\frac {n}{5}}={\\\\frac {n}{10}}}\\n  ) have their median less than the pivot (Median of Medians). Also, another half the number of groups (again, \\n  \\n    \\n      \\n        \\n          \\n            1\\n            2\\n          \\n        \\n        \\u00d7\\n        \\n          \\n            n\\n            5\\n          \\n        \\n        =\\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {1}{2}}\\\\times {\\\\frac {n}{5}}={\\\\frac {n}{10}}}\\n  ) have their median greater than the pivot. In each of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{10}}}\\n   groups with median less than the pivot, there are two elements that are smaller than their respective medians, which are smaller than the pivot. Thus, each of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{10}}}\\n   groups have at least 3 elements that are smaller than the pivot. Similarly, in each of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{10}}}\\n   groups with median greater than the pivot, there are two elements that are greater than their respective medians, which are greater than the pivot. Thus, each of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{10}}}\\n   groups have at least 3 elements that are greater than the pivot. Hence, the pivot is less than \\n  \\n    \\n      \\n        3\\n        \\u00d7\\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 3\\\\times {\\\\frac {n}{10}}}\\n   elements and greater than another \\n  \\n    \\n      \\n        3\\n        \\u00d7\\n        \\n          \\n            n\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 3\\\\times {\\\\frac {n}{10}}}\\n   elements. Thus the chosen median splits the ordered elements somewhere between 30%/70% and 70%/30%, which assures worst-case linear behavior of the algorithm. To visualize:\\n\\n(red = \\\"(one of the two possible) median of medians\\\", gray = \\\"number < red\\\", white = \\\"number > red\\\")\\n5-tuples are shown here sorted by median, for clarity.  Sorting the tuples is not necessary because we only need the median for use as pivot element.\\nNote that all elements above/left of the red (30% of the 100 elements) are less, and all elements below/right of the red (another 30% of the 100 elements) are greater.\\n\\nProof of O(n) running time\\nThe median-calculating recursive call does not exceed worst-case linear behavior because the list of medians has size \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n  , while the other recursive call recurses on at most 70% of the list. Let \\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle T(n)}\\n   be the time it takes to run a median-of-medians Quickselect algorithm on an array of size \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  . Then we know this time is:\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        \\u2264\\n        T\\n        (\\n        n\\n        \\n          /\\n        \\n        5\\n        )\\n        +\\n        T\\n        (\\n        n\\n        \\u22c5\\n        7\\n        \\n          /\\n        \\n        10\\n        )\\n        +\\n        c\\n        \\u22c5\\n        n\\n        ,\\n      \\n    \\n    {\\\\displaystyle T(n)\\\\leq T(n/5)+T(n\\\\cdot 7/10)+c\\\\cdot n,}\\n  where\\n\\nthe \\n  \\n    \\n      \\n        T\\n        \\n          (\\n          \\n            \\n              n\\n              5\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle T\\\\left({\\\\frac {n}{5}}\\\\right)}\\n   part is for finding the true median of the \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n   medians, by running an (independent) Quickselect on them (since finding the median is just a special case of selecting a k-smallest element)\\nthe \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   term \\n  \\n    \\n      \\n        c\\n        \\u22c5\\n        n\\n      \\n    \\n    {\\\\displaystyle c\\\\cdot n}\\n   is for the partitioning work to create the two sides, one of which our Quickselect will recurse (we visited each element a constant number of times in order to form them into n/5 groups and take each median in \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   time).\\nthe \\n  \\n    \\n      \\n        T\\n        \\n          (\\n          \\n            \\n              \\n                7\\n                n\\n              \\n              10\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle T\\\\left({\\\\frac {7n}{10}}\\\\right)}\\n   part is for the actual Quickselect recursion (for the worst case, in which the k-th element is in the bigger partition that can be of size \\n  \\n    \\n      \\n        \\n          \\n            \\n              7\\n              n\\n            \\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {7n}{10}}}\\n   maximally)By induction:\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        \\u2264\\n        10\\n        \\u22c5\\n        c\\n        \\u22c5\\n        n\\n        \\u2208\\n        O\\n        (\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle T(n)\\\\leq 10\\\\cdot c\\\\cdot n\\\\in O(n).}\\n\\nAnalysis\\nThe key step is reducing the problem to selecting in two lists whose total length is shorter than the original list, plus a linear factor for the reduction step. This allows a simple induction to show that the overall running time is linear.\\nThe specific choice of groups of five elements is explained as follows. Firstly, computing median of an odd list is faster and simpler; while one could use an even list, this requires taking the average of the two middle elements, which is slower than simply selecting the single exact middle element. Secondly, five is the smallest odd number such that median of medians works. With groups of only three elements, the resulting list of medians to search in is length \\n  \\n    \\n      \\n        \\n          \\n            n\\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{3}}}\\n  , and reduces the list to recurse into length \\n  \\n    \\n      \\n        \\n          \\n            2\\n            3\\n          \\n        \\n        n\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {2}{3}}n}\\n  , since it is greater than 1/2 \\u00d7 2/3 = 1/3 of the elements and less than 1/2 \\u00d7 2/3 = 1/3 of the elements. Thus this still leaves \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements to search in, not reducing the problem sufficiently. The individual lists are shorter, however, and one can bound the resulting complexity to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   by the Akra\\u2013Bazzi method, but it does not prove linearity.\\nConversely, one may instead group by \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   = seven, nine, or more elements, and this does work. This reduces the size of the list of medians to \\n  \\n    \\n      \\n        \\n          \\n            n\\n            g\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{g}}}\\n  , and the size of the list to recurse into asymptotes at 3n/4 (75%), as the quadrants in the above table approximate 25%, as the size of the overlapping lines decreases proportionally. This reduces the scaling factor from 10 asymptotically to 4, but accordingly raises the \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   term for the partitioning work. Finding the median of a larger group takes longer, but is a constant factor (depending only on \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  ), and thus does not change the overall performance as n grows. In fact, considering the number of comparisons in the worst case, the constant factor is \\n  \\n    \\n      \\n        \\n          \\n            \\n              2\\n              g\\n              (\\n              g\\n              \\u2212\\n              1\\n              )\\n            \\n            \\n              g\\n              \\u2212\\n              3\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {2g(g-1)}{g-3}}}\\n  .\\nIf one instead groups the other way, say dividing the \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   element list into 5 lists, computing the median of each, and then computing the median of these \\u2013 i.e., grouping by a constant fraction, not a constant number \\u2013 one does not as clearly reduce the problem, since it requires computing 5 medians, each in a list of \\n  \\n    \\n      \\n        \\n          \\n            n\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{5}}}\\n   elements, and then recursing on a list of length at most \\n  \\n    \\n      \\n        \\n          \\n            7\\n            10\\n          \\n        \\n        n\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {7}{10}}n}\\n  . As with grouping by 3, the individual lists are shorter, but the overall length is no shorter \\u2013 in fact longer \\u2013 and thus one can only prove superlinear bounds. Grouping into a square of \\n  \\n    \\n      \\n        \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {n}}}\\n   lists of length \\n  \\n    \\n      \\n        \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {n}}}\\n   is similarly complicated.\\n\\nReferences\\nExternal links\\n\\\"Lecture notes for January 30, 1996: Deterministic selection\\\", ICS 161: Design and Analysis of Algorithms, David Eppstein\"}, {\"Order statistic tree\": \"In computer science, an order statistic tree is a variant of the binary search tree (or more generally, a B-tree) that supports two additional operations beyond insertion, lookup and deletion:\\n\\nSelect(i) \\u2013 find the i'th smallest element stored in the tree\\nRank(x) \\u2013 find the rank of element x in the tree, i.e. its index in the sorted list of elements of the treeBoth operations can be performed in O(log n) worst case time when a self-balancing tree is used as the base data structure.\\n\\nAugmented search tree implementation\\nTo turn a regular search tree into an order statistic tree, the nodes of the tree need to store one additional value, which is the size of the subtree rooted at that node (i.e., the number of nodes below it). All operations that modify the tree must adjust this information to preserve the invariant that\\n\\nsize[x] = size[left[x]] + size[right[x]] + 1\\n\\nwhere size[nil] = 0 by definition. Select can then be implemented as:\\u200a342\\u200a\\nfunction Select(t, i)\\n    // Returns the i'th element (one-indexed) of the elements in t\\n    p \\u2190 size[left[t]]+1\\n    if i = p\\n        return t\\n    else if i < p\\n        return Select(left[t], i)\\n    else\\n        return Select(right[t], i - p)\\n\\nRank can be implemented, using the parent-function p[x], as:\\u200a342\\u200a\\nfunction Rank(T, x)\\n    // Returns the position of x (one-indexed) in the linear sorted list of elements of the tree T\\n    r \\u2190 size[left[x]] + 1\\n    y \\u2190 x\\n    while y \\u2260 T.root\\n        if y = right[p[y]]\\n            r \\u2190 r + size[left[p[y]]] + 1\\n        y \\u2190 p[y]\\n    return r\\n\\nOrder-statistic trees can be further amended with bookkeeping information to maintain balance (e.g., tree height can be added to get an order statistic AVL tree, or a color bit to get a red\\u2013black order statistic tree). Alternatively, the size field can be used in conjunction with a weight-balancing scheme at no additional storage cost.\\n\\nReferences\\nExternal links\\nOrder statistic tree on PineWiki, Yale University.\\nThe Python package blist uses order statistic B-trees to implement lists with fast insertion at arbitrary positions.\"}, {\"Quickselect\": \"In computer science, quickselect is a selection algorithm to find the kth smallest element in an unordered list, also known as the kth order statistic. Like the related quicksort sorting algorithm, it was developed by Tony Hoare, and thus is also known as Hoare's selection algorithm. Like quicksort, it is efficient in practice and has good average-case performance, but has poor worst-case performance. Quickselect and its variants are the selection algorithms most often used in efficient real-world implementations. \\nQuickselect uses the same overall approach as quicksort, choosing one element as a pivot and partitioning the data in two based on the pivot, accordingly as less than or greater than the pivot. However, instead of recursing into both sides, as in quicksort, quickselect only recurses into one side \\u2013 the side with the element it is searching for. This reduces the average complexity from \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  , with a worst case of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  .\\nAs with quicksort, quickselect is generally implemented as an in-place algorithm, and beyond selecting the kth element, it also partially sorts the data. See selection algorithm for further discussion of the connection with sorting.\\n\\nAlgorithm\\nIn quicksort, there is a subprocedure called partition that can, in linear time, group a list (ranging from indices left to right) into two parts: those less than a certain element, and those greater than or equal to the element. Here is pseudocode that performs a partition about the element list[pivotIndex]:\\n\\nfunction partition(list, left, right, pivotIndex) is\\n    pivotValue := list[pivotIndex]\\n    swap list[pivotIndex] and list[right]  // Move pivot to end\\n    storeIndex := left\\n    for i from left to right \\u2212 1 do\\n        if list[i] < pivotValue then\\n            swap list[storeIndex] and list[i]\\n            increment storeIndex\\n    swap list[right] and list[storeIndex]  // Move pivot to its final place\\n    return storeIndex\\n\\nThis is known as the Lomuto partition scheme, which is simpler but less efficient than Hoare's original partition scheme.\\nIn quicksort, we recursively sort both branches, leading to best-case \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   time. However, when doing selection, we already know which partition our desired element lies in, since the pivot is in its final sorted position, with all those preceding it in an unsorted order and all those following it in an unsorted order. Therefore, a single recursive call locates the desired element in the correct partition, and we build upon this for quickselect:\\n\\n// Returns the k-th smallest element of list within left..right inclusive\\n// (i.e. left <= k <= right).\\nfunction select(list, left, right, k) is\\n    if left = right then   // If the list contains only one element,\\n        return list[left]  // return that element\\n    pivotIndex  := ...     // select a pivotIndex between left and right,\\n                           // e.g., left + floor(rand() % (right \\u2212 left + 1))\\n    pivotIndex  := partition(list, left, right, pivotIndex)\\n    // The pivot is in its final sorted position\\n    if k = pivotIndex then\\n        return list[k]\\n    else if k < pivotIndex then\\n        return select(list, left, pivotIndex \\u2212 1, k)\\n    else\\n        return select(list, pivotIndex + 1, right, k) \\n\\nNote the resemblance to quicksort: just as the minimum-based selection algorithm is a partial selection sort, this is a partial quicksort, generating and partitioning only \\n  \\n    \\n      \\n        O\\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(\\\\log n)}\\n   of its \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   partitions. This simple procedure has expected linear performance, and, like quicksort, has quite good performance in practice. It is also an in-place algorithm, requiring only constant memory overhead if tail call optimization is available, or if eliminating the tail recursion with a loop:\\nfunction select(list, left, right, k) is\\n    loop\\n        if left = right then\\n            return list[left]\\n        pivotIndex := ...     // select pivotIndex between left and right\\n        pivotIndex := partition(list, left, right, pivotIndex)\\n        if k = pivotIndex then\\n            return list[k]\\n        else if k < pivotIndex then\\n            right := pivotIndex \\u2212 1\\n        else\\n            left := pivotIndex + 1\\n\\nTime complexity\\nLike quicksort, quickselect has good average performance, but is sensitive to the pivot that is chosen. If good pivots are chosen, meaning ones that consistently decrease the search set by a given fraction, then the search set decreases in size exponentially and by induction (or summing the geometric series) one sees that performance is linear, as each step is linear and the overall time is a constant times this (depending on how quickly the search set reduces). However, if bad pivots are consistently chosen, such as decreasing by only a single element each time, then worst-case performance is quadratic: \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle O(n^{2}).}\\n   This occurs for example in searching for the maximum element of a set, using the first element as the pivot, and having sorted data. The probability of the worst-case occurring decreases exponentially with \\n  \\n    \\n      \\n        n\\n        ,\\n      \\n    \\n    {\\\\displaystyle n,}\\n   so quickselect has almost certain \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time complexity.\\n\\nVariants\\nThe easiest solution is to choose a random pivot, which yields almost certain linear time. Deterministically, one can use median-of-3 pivot strategy (as in the quicksort), which yields linear performance on partially sorted data, as is common in the real world. However, contrived sequences can still cause worst-case complexity; David Musser describes a \\\"median-of-3 killer\\\" sequence that allows an attack against that strategy, which was one motivation for his introselect algorithm.\\nOne can assure linear performance even in the worst case by using a more sophisticated pivot strategy; this is done in the median of medians algorithm. However, the overhead of computing the pivot is high, and thus this is generally not used in practice. One can combine basic quickselect with median of medians as fallback to get both fast average case performance and linear worst-case performance; this is done in introselect.\\nFiner computations of the average time complexity yield a worst case of \\n  \\n    \\n      \\n        n\\n        (\\n        2\\n        +\\n        2\\n        log\\n        \\u2061\\n        2\\n        +\\n        o\\n        (\\n        1\\n        )\\n        )\\n        \\u2264\\n        3.4\\n        n\\n        +\\n        o\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle n(2+2\\\\log 2+o(1))\\\\leq 3.4n+o(n)}\\n   for random pivots (in the case of the median; other k are faster). The constant can be improved to 3/2 by a more complicated pivot strategy, yielding the Floyd\\u2013Rivest algorithm, which has average complexity of \\n  \\n    \\n      \\n        1.5\\n        n\\n        +\\n        O\\n        (\\n        \\n          n\\n          \\n            1\\n            \\n              /\\n            \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle 1.5n+O(n^{1/2})}\\n   for median, with other k being faster.\\n\\nSee also\\nFloyd\\u2013Rivest algorithm\\nIntroselect\\nMedian of medians\\n\\nReferences\\nExternal links\\n\\\"qselect\\\", Quickselect algorithm in Matlab,  Manolis Lourakis\"}, {\"Category:Stable sorts\": \"Stable sorting algorithms maintain the relative order of records with equal keys (i.e. values). That is, a sorting algorithm is stable if whenever there are two records R and S with the same key and with R appearing before S in the original list, R will appear before S in the sorted list.  See here for a more complete description.\"}, {\"Block sort\": \"Block sort, or block merge sort, is a sorting algorithm combining at least two merge operations with an insertion sort to arrive at O(n log n) in-place stable sorting. It gets its name from the observation that merging two sorted lists, A and B, is equivalent to breaking A into evenly sized blocks, inserting each A block into B under special rules, and merging AB pairs.\\nOne practical algorithm for O(log n) in place merging was proposed by Pok-Son Kim and Arne Kutzner in 2008.\\n\\nOverview\\nThe outer loop of block sort is identical to a bottom-up merge sort, where each level of the sort merges pairs of subarrays, A and B, in sizes of 1, then 2, then 4, 8, 16, and so on, until both subarrays combined are the array itself.\\nRather than merging A and B directly as with traditional methods, a block-based merge algorithm divides A into discrete blocks of size \\u221aA (resulting in \\u221aA number of blocks as well), inserts each A block into B such that the first value of each A block is less than or equal (\\u2264) to the B value immediately after it, then locally merges each A block with any B values between it and the next A block.\\nAs merges still require a separate buffer large enough to hold the A block to be merged, two areas within the array are reserved for this purpose (known as internal buffers). The first two A blocks are thus modified to contain the first instance of each value within A, with the original contents of those blocks shifted over if necessary. The remaining A blocks are then inserted into B and merged using one of the two buffers as swap space. This process causes the values in that buffer to be rearranged.\\nOnce every A and B block of every A and B subarray have been merged for that level of the merge sort, the values in that buffer must be sorted to restore their original order, so an insertion sort must be applied. The values in the buffers are then redistributed to their first sorted position within the array. This process repeats for each level of the outer bottom-up merge sort, at which point the array will have been stably sorted.\\n\\nAlgorithm\\nThe following operators are used in the code examples:\\n\\nAdditionally, block sort relies on the following operations as part of its overall algorithm:\\n\\nSwap: exchange the positions of two values in an array.\\nBlock swap: exchange a range of values within an array with values in a different range of the array.\\nBinary search: assuming the array is sorted, check the middle value of the current search range, then if the value is lesser check the lower range, and if the value is greater check the upper range. Block sort uses two variants: one which finds the first position to insert a value in the sorted array, and one which finds the last position.\\nLinear search: find a particular value in an array by checking every single element in order, until it is found.\\nInsertion sort: for each item in the array, loop backward and find where it needs to be inserted, then insert it at that position.\\nArray rotation: move the items in an array to the left or right by some number of spaces, with values on the edges wrapping around to the other side. Rotations can be implemented as three reversals.Rotate(array, amount, range)\\n    Reverse(array, range)\\n    Reverse(array, [range.start, range.start + amount))\\n    Reverse(array, [range.start + amount, range.end))\\n\\nFloor power of two: floor a value to the next power of two. Thus 63 becomes 32, 64 stays 64, and so forth.FloorPowerOfTwo(x)\\n    x = x | (x >> 1)\\n    x = x | (x >> 2)\\n    x = x | (x >> 4)\\n    x = x | (x >> 8)\\n    x = x | (x >> 16)\\n    if (this is a 64-bit system)\\n        x = x | (x >> 32)\\n    return x - (x >> 1)\\n\\nOuter loop\\nAs previously stated, the outer loop of a block sort is identical to a bottom-up merge sort. However, it benefits from the variant that ensures each A and B subarray are the same size to within one item:\\n\\n   BlockSort(array)\\n       power_of_two = FloorPowerOfTwo(array.size)\\n       scale = array.size/power_of_two // 1.0 \\u2264 scale < 2.0\\n      \\n       // insertion sort 16\\u201331 items at a time\\n       for (merge = 0; merge < power_of_two; merge += 16)\\n           start = merge * scale\\n           end = start + 16 * scale\\n           InsertionSort(array, [start, end))\\n      \\n       for (length = 16; length < power_of_two; length += length)\\n           for (merge = 0; merge < power_of_two; merge += length * 2)\\n               start = merge * scale\\n               mid = (merge + length) * scale\\n               end = (merge + length * 2) * scale\\n              \\n               if (array[end \\u2212 1] < array[start])\\n                   // the two ranges are in reverse order, so a rotation is enough to merge them\\n                   Rotate(array, mid \\u2212 start, [start, end))\\n               else if (array[mid \\u2212 1] > array[mid])\\n                   Merge(array, A = [start, mid), B = [mid, end))\\n               // else the ranges are already correctly ordered\\n\\nFixed-point math may also be used, by representing the scale factor as a fraction integer_part + numerator/denominator:\\n\\n   power_of_two = FloorPowerOfTwo(array.size)\\n   denominator = power_of_two/16\\n   numerator_step = array.size % denominator\\n   integer_step = floor(array.size/denominator)\\n  \\n   // insertion sort 16\\u201331 items at a time\\n  \\n   while (integer_step < array.size)\\n       integer_part = numerator = 0\\n       while (integer_part < array.size)\\n           // get the ranges for A and B\\n           start = integer_part\\n          \\n           integer_part += integer_step\\n           numerator += numerator_step\\n           if (numerator \\u2265 denominator)\\n               numerator \\u2212= denominator\\n               integer_part++\\n          \\n           mid = integer_part\\n          \\n           integer_part += integer_step\\n           numerator += numerator_step\\n           if (numerator \\u2265 denominator)\\n               numerator \\u2212= denominator\\n               integer_part++\\n          \\n           end = integer_part\\n          \\n           if (array[end \\u2212 1] < array[start])\\n               Rotate(array, mid \\u2212 start, [start, end))\\n           else if (array[mid \\u2212 1] > array[mid])\\n               Merge(array, A = [start, mid), B = [mid, end))\\n      \\n       integer_step += integer_step\\n       numerator_step += numerator_step\\n       if (numerator_step \\u2265 denominator)\\n           numerator_step \\u2212= denominator\\n           integer_step++\\n\\nExtract buffers\\nThe two internal buffers needed for each level of the merge step are created by moving the first 2\\u221aA first instances of their values within an A subarray to the start of A. First it iterates over the elements in A and counts off the unique values it needs, then it applies array rotations to move those unique values to the start. If A did not contain enough unique values to fill the two buffers (of size \\u221aA each), B can be used just as well. In this case it moves the last instance of each value to the end of B, with that part of B not being included during the merges.\\n\\nwhile (integer_step < array.size)\\n    block_size = \\u221ainteger_step\\n    buffer_size = integer_step/block_size + 1\\n    [extract two buffers of size 'buffer_size' each]\\n\\nIf B does not contain enough unique values either, it pulls out the largest number of unique values it could find, then adjusts the size of the A and B blocks such that the number of resulting A blocks is less than or equal to the number of unique items pulled out for the buffer. Only one buffer will be used in this case \\u2013 the second buffer won't exist.\\n\\nbuffer_size = [number of unique values found]\\nblock_size = integer_step/buffer_size + 1\\n  \\ninteger_part = numerator = 0\\nwhile (integer_part < array.size)\\n    [get the ranges for A and B]\\n    [adjust A and B to not include the ranges used by the buffers]\\n\\nTag A blocks\\nOnce the one or two internal buffers have been created, it begins merging each A and B subarray for this level of the merge sort. To do so, it divides each A and B subarray into evenly sized blocks of the size calculated in the previous step, where the first A block and last B block are unevenly sized if needed. It then loops over each of the evenly sized A blocks and swaps the second value with a corresponding value from the first of the two internal buffers. This is known as tagging the blocks.\\n\\n// blockA is the range of the remaining A blocks,\\n// and firstA is the unevenly sized first A block\\nblockA = [A.start, A.end)\\nfirstA = [A.start, A.start + |blockA| % block_size)\\n  \\n// swap the second value of each A block with the value in buffer1\\nfor (index = 0, indexA = firstA.end + 1; indexA < blockA.end; indexA += block_size)\\n    Swap(array[buffer1.start + index], array[indexA])\\n    index++\\n  \\nlastA = firstA\\nblockB = [B.start, B.start + minimum(block_size, |B|))\\nblockA.start += |firstA|\\n\\nRoll and drop\\nAfter defining and tagging the A blocks in this manner, the A blocks are rolled through the B blocks by block swapping the first evenly sized A block with the next B block. This process repeats until the first value of the A block with the smallest tag value is less than or equal to the last value of the B block that was just swapped with an A block.\\nAt that point, the minimum A block (the A block with the smallest tag value) is swapped to the start of the rolling A blocks and the tagged value is restored with its original value from the first buffer. This is known as dropping a block behind, as it will no longer be rolled along with the remaining A blocks. That A block is then inserted into the previous B block, first by using a binary search on B to find the index where the first value of A is less than or equal to the value at that index of B, and then by rotating A into B at that index.\\n\\n   minA = blockA.start\\n   indexA = 0\\n  \\n   while (true)\\n       // if there's a previous B block and the first value of the minimum A block is \\u2264\\n       // the last value of the previous B block, then drop that minimum A block behind.\\n       // or if there are no B blocks left then keep dropping the remaining A blocks.\\n       if ((|lastB| > 0 and array[lastB.end - 1] \\u2265 array[minA]) or |blockB| = 0)\\n           // figure out where to split the previous B block, and rotate it at the split\\n           B_split = BinaryFirst(array, array[minA], lastB)\\n           B_remaining = lastB.end - B_split\\n          \\n           // swap the minimum A block to the beginning of the rolling A blocks\\n           BlockSwap(array, blockA.start, minA, block_size)\\n          \\n           // restore the second value for the A block\\n           Swap(array[blockA.start + 1], array[buffer1.start + indexA])\\n           indexA++\\n          \\n           // rotate the A block into the previous B block\\n           Rotate(array, blockA.start - B_split, [B_split, blockA.start + block_size))\\n          \\n           // locally merge the previous A block with the B values that follow it,\\n           // using the second internal buffer as swap space (if it exists)\\n           if (|buffer2| > 0)\\n               MergeInternal(array, lastA, [lastA.end, B_split), buffer2)\\n           else\\n               MergeInPlace(array, lastA, [lastA.end, B_split))\\n          \\n           // update the range for the remaining A blocks,\\n           // and the range remaining from the B block after it was split\\n           lastA = [blockA.start - B_remaining, blockA.start - B_remaining + block_size)\\n           lastB = [lastA.end, lastA.end + B_remaining)\\n          \\n           // if there are no more A blocks remaining, this step is finished\\n           blockA.start = blockA.start + block_size\\n           if (|blockA| = 0)\\n               break\\n          \\n           minA = [new minimum A block] (see below)\\n       else if (|blockB| < block_size)\\n           // move the last B block, which is unevenly sized,\\n           // to before the remaining A blocks, by using a rotation\\n           Rotate(array, blockB.start - blockA.start, [blockA.start, blockB.end))\\n          \\n           lastB = [blockA.start, blockA.start + |blockB|)\\n           blockA.start += |blockB|\\n           blockA.end += |blockB|\\n           minA += |blockB|\\n           blockB.end = blockB.start\\n       else\\n           // roll the leftmost A block to the end by swapping it with the next B block\\n           BlockSwap(array, blockA.start, blockB.start, block_size)\\n           lastB = [blockA.start, blockA.start + block_size)\\n           if (minA = blockA.start)\\n               minA = blockA.end\\n          \\n           blockA.start += block_size\\n           blockA.end += block_size\\n           blockB.start += block_size\\n          \\n           // this is equivalent to minimum(blockB.end + block_size, B.end),\\n           // but that has the potential to overflow\\n           if (blockB.end > B.end - block_size)\\n               blockB.end = B.end\\n           else\\n               blockB.end += block_size\\n  \\n   // merge the last A block with the remaining B values\\n   if (|buffer2| > 0)\\n       MergeInternal(array, lastA, [lastA.end, B.end), buffer2)\\n   else\\n       MergeInPlace(array, lastA, [lastA.end, B.end))\\n\\nOne optimization that can be applied during this step is the floating-hole technique. When the minimum A block is dropped behind and needs to be rotated into the previous B block, after which its contents are swapped into the second internal buffer for the local merges, it would be faster to swap the A block to the buffer beforehand, and to take advantage of the fact that the contents of that buffer do not need to retain any order. So rather than rotating the second buffer (which used to be the A block before the block swap) into the previous B block at position index, the values in the B block after index can simply be block swapped with the last items of the buffer.\\nThe floating hole in this case refers to the contents of the second internal buffer floating around the array, and acting as a hole in the sense that the items do not need to retain their order.\\n\\nLocal merges\\nOnce the A block has been rotated into the B block, the previous A block is then merged with the B values that follow it, using the second buffer as swap space. When the first A block is dropped behind this refers to the unevenly sized A block at the start, when the second A block is dropped behind it means the first A block, and so forth.\\n\\nMergeInternal(array, A, B, buffer)\\n    // block swap the values in A with those in 'buffer'\\n    BlockSwap(array, A.start, buffer.start, |A|)\\n\\n    A_count = 0, B_count = 0, insert = 0\\n    while (A_count < |A| and B_count < |B|)\\n        if (array[buffer.start + A_count] \\u2264 array[B.start + B_count])\\n            Swap(array[A.start + insert], array[buffer.start + A_count])\\n            A_count++\\n        else\\n            Swap(array[A.start + insert], array[B.start + B_count])\\n            B_count++\\n        insert++\\n\\n    // block swap the remaining part of the buffer with the remaining part of the array\\n    BlockSwap(array, buffer.start + A_count, A.start + insert, |A| - A_count)\\n\\nIf the second buffer does not exist, a strictly in-place merge operation must be performed, such as a rotation-based version of the Hwang and Lin algorithm, the Dudzinski and Dydek algorithm, or a repeated binary search and rotate.\\n\\nMergeInPlace(array, A, B)\\n    while (|A| > 0 and |B| > 0)\\n        // find the first place in B where the first item in A needs to be inserted\\n        mid = BinaryFirst(array, array[A.start], B)\\n\\n        // rotate A into place\\n        amount = mid - A.end\\n        Rotate(array, amount, [A.start, mid))\\n\\n        // calculate the new A and B ranges\\n        B = [mid, B.end)\\n        A = [A.start + amount, mid)\\n        A.start = BinaryLast(array, array[A.start], A)\\n\\nAfter dropping the minimum A block and merging the previous A block with the B values that follow it, the new minimum A block must be found within the blocks that are still being rolled through the array. This is handled by running a linear search through those A blocks and comparing the tag values to find the smallest one.\\n\\nminA = blockA.start\\nfor (findA = minA + block_size; findA < blockA.end - 1; findA += block_size)\\n    if (array[findA + 1] < array[minA + 1])\\n        minA = findA\\n\\nThese remaining A blocks then continue rolling through the array and being dropped and inserted where they belong. This process repeats until all of the A blocks have been dropped and rotated into the previous B block.\\nOnce the last remaining A block has been dropped behind and inserted into B where it belongs, it should be merged with the remaining B values that follow it. This completes the merge process for that particular pair of A and B subarrays. However, it must then repeat the process for the remaining A and B subarrays for the current level of the merge sort.\\nNote that the internal buffers can be reused for every set of A and B subarrays for this level of the merge sort, and do not need to be re-extracted or modified in any way.\\n\\nRedistribute\\nAfter all of the A and B subarrays have been merged, the one or two internal buffers are still left over. The first internal buffer was used for tagging the A blocks, and its contents are still in the same order as before, but the second internal buffer may have had its contents rearranged when it was used as swap space for the merges. This means the contents of the second buffer will need to be sorted using a different algorithm, such as insertion sort. The two buffers must then be redistributed back into the array using the opposite process that was used to create them.\\nAfter repeating these steps for every level of the bottom-up merge sort, the block sort is completed.\\n\\nVariants\\nBlock sort works by extracting two internal buffers, breaking A and B subarrays into evenly sized blocks, rolling and dropping the A blocks into B (using the first buffer to track the order of the A blocks), locally merging using the second buffer as swap space, sorting the second buffer, and redistributing both buffers. While the steps do not change, these subsystems can vary in their actual implementation.\\nOne variant of block sort allows it to use any amount of additional memory provided to it, by using this external buffer for merging an A subarray or A block with B whenever A fits into it. In this situation it would be identical to a merge sort.\\nGood choices for the buffer size include:\\n\\nRather than tagging the A blocks using the contents of one of the internal buffers, an indirect movement-imitation buffer can be used instead. This is an internal buffer defined as s1 t s2, where s1 and s2 are each as large as the number of A and B blocks, and t contains any values immediately following s1 that are equal to the last value of s1 (thus ensuring that no value in s2 appears in s1). A second internal buffer containing \\u221aA unique values is still used. The first \\u221aA values of s1 and s2 are then swapped with each other to encode information into the buffer about which blocks are A blocks and which are B blocks. When an A block at index i is swapped with a B block at index j (where the first evenly sized A block is initially at index 0), s1[i] and s1[j] are swapped with s2[i] and s2[j], respectively. This imitates the movements of the A blocks through B. The unique values in the second buffer are used to determine the original order of the A blocks as they are rolled through the B blocks. Once all of the A blocks have been dropped, the movement-imitation buffer is used to decode whether a given block in the array is an A block or a B block, each A block is rotated into B, and the second internal buffer is used as swap space for the local merges.\\nThe second value of each A block doesn't necessarily need to be tagged \\u2013 the first, last, or any other element could be used instead. However, if the first value is tagged, the values will need to be read from the first internal buffer (where they were swapped) when deciding where to drop the minimum A block.\\nMany sorting algorithms can be used to sort the contents of the second internal buffer, including unstable sorts like quicksort, since the contents of the buffer are guaranteed to unique. Insertion sort is still recommended, though, for its situational performance and lack of recursion.\\n\\nAnalysis\\nBlock sort is a well-defined and testable class of algorithms, with working implementations available as a merge and as a sort. This allows its characteristics to be measured and considered.\\n\\nComplexity\\nBlock sort begins by performing insertion sort on groups of 16\\u201331 items in the array. Insertion sort is an \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   operation, so this leads to anywhere from \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          16\\n          \\n            2\\n          \\n        \\n        \\u00d7\\n        n\\n        \\n          /\\n        \\n        16\\n        )\\n      \\n    \\n    {\\\\displaystyle O(16^{2}\\\\times n/16)}\\n   to \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          31\\n          \\n            2\\n          \\n        \\n        \\u00d7\\n        n\\n        \\n          /\\n        \\n        31\\n        )\\n      \\n    \\n    {\\\\displaystyle O(31^{2}\\\\times n/31)}\\n  , which is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   once the constant factors are omitted. It must also apply an insertion sort on the second internal buffer after each level of merging is completed. However, as this buffer was limited to \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   in size, the \\n  \\n    \\n      \\n        O\\n        \\n          \\n            \\n              n\\n            \\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle O{\\\\sqrt {n}}^{2}}\\n   operation also ends up being \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\nNext it must extract two internal buffers for each level of the merge sort. It does so by iterating over the items in the A and B subarrays and incrementing a counter whenever the value changes, and upon finding enough values it rotates them to the start of A or the end of B. In the worst case this will end up searching the entire array before finding \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   non-contiguous unique values, which requires \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   comparisons and \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   rotations for \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   values. This resolves to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        +\\n        \\n          \\n            n\\n          \\n        \\n        \\u00d7\\n        \\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n+{\\\\sqrt {n}}\\\\times {\\\\sqrt {n}})}\\n  , or \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\nWhen none of the A or B subarrays contained \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   unique values to create the internal buffers, a normally suboptimal in-place merge operation is performed where it repeatedly binary searches and rotates A into B. However, the known lack of unique values within any of the subarrays places a hard limit on the number of binary searches and rotations that will be performed during this step, which is again \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   items rotated up to \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times, or \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  . The size of each block is also adjusted to be smaller in the case where it found \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   unique values but not 2\\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n  , which further limits the number of unique values contained within any A or B block.\\nTagging the A blocks is performed \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times for each A subarray, then the A blocks are rolled through and inserted into the B blocks up to \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times. The local merges retain the same \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   complexity of a standard merge, albeit with more assignments since the values must be swapped rather than copied. The linear search for finding the new minimum A block iterates over \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   blocks \\n  \\n    \\n      \\n        \\n          \\n            A\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {A}}}\\n   times. And the buffer redistribution process is identical to the buffer extraction but in reverse, and therefore has the same \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   complexity.\\nAfter omitting all but the highest complexity and considering that there are \\n  \\n    \\n      \\n        log\\n        \\u2061\\n        \\n          n\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\log {n}}\\n   levels in the outer merge loop, this leads to a final asymptotic complexity of \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        \\n          n\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log {n})}\\n   for the worst and average cases. For the best case, where the data is already in order, the merge step performs  \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        16\\n      \\n    \\n    {\\\\displaystyle n/16}\\n   comparisons for the first level, then \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        32\\n      \\n    \\n    {\\\\displaystyle n/32}\\n  , \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        64\\n      \\n    \\n    {\\\\displaystyle n/64}\\n  , \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        128\\n      \\n    \\n    {\\\\displaystyle n/128}\\n  , etc. This is a well-known mathematical series which resolves to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\n\\nMemory\\nAs block sort is non-recursive and does not require the use of dynamic allocations, this leads to constant stack and heap space. It uses O(1) auxiliary memory in a transdichotomous model, which accepts that the O(log n) bits needed to keep track of the ranges for A and B cannot be any greater than 32 or 64 on 32-bit or 64-bit computing systems, respectively, and therefore simplifies to O(1) space for any array that can feasibly be allocated.\\n\\nStability\\nAlthough items in the array are moved out of order during a block sort, each operation is fully reversible and will have restored the original order of equivalent items by its completion.\\nStability requires the first instance of each value in an array before sorting to still be the first instance of that value after sorting. Block sort moves these first instances to the start of the array to create the two internal buffers, but when all of the merges are completed for the current level of the block sort, those values are distributed back to the first sorted position within the array. This maintains stability.\\nBefore rolling the A blocks through the B blocks, each A block has its second value swapped with a value from the first buffer. At that point the A blocks are moved out of order to roll through the B blocks. However, once it finds where it should insert the smallest A block into the previous B block, that smallest A block is moved back to the start of the A blocks and its second value is restored. By the time all of the A blocks have been inserted, the A blocks will be in order again and the first buffer will contain its original values in the original order.\\nUsing the second buffer as swap space when merging an A block with some B values causes the contents of that buffer to be rearranged. However, as the algorithm already ensured the buffer only contains unique values, sorting the contents of the buffer is sufficient to restore their original stable order.\\n\\nAdaptivity\\nBlock sort is an adaptive sort on two levels: first, it skips merging A and B subarrays that are already in order. Next, when A and B need to be merged and are broken into evenly sized blocks, the A blocks are only rolled through B as far as is necessary, and each block is only merged with the B values immediately following it. The more ordered the data originally was, the fewer B values there will be that need to be merged into A.\\n\\nAdvantages\\nBlock sort is a stable sort that does not require additional memory, which is useful in cases where there is not enough free memory to allocate the O(n) buffer. When using the external buffer variant of block sort, it can scale from using O(n) memory to progressively smaller buffers as needed, and will still work efficiently within those constraints.\\n\\nDisadvantages\\nBlock sort does not exploit sorted ranges of data on as fine a level as some other algorithms, such as Timsort. It only checks for these sorted ranges at the two predefined levels: the A and B subarrays, and the A and B blocks. It is also harder to implement and parallelize compared to a merge sort.\\n\\n\\n== References ==\"}, {\"Bubble sort\": \"Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the input list element by element, comparing the current element with the one after it, swapping their values if needed. These passes through the list are repeated until no swaps had to be performed during a pass, meaning that the list has become fully sorted. The algorithm, which is a comparison sort, is named for the way the larger elements \\\"bubble\\\" up to the top of the list. \\nThis simple algorithm performs poorly in real world use and is used primarily as an educational tool. More efficient algorithms such as quicksort, timsort, or merge sort are used by the sorting libraries built into popular programming languages such as Python and Java.\\n\\nAnalysis\\nPerformance\\nBubble sort has a worst-case and average complexity of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  , where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of items being sorted. Most practical sorting algorithms have substantially better worst-case or average complexity, often \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n  . Even other \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   sorting algorithms, such as insertion sort, generally run faster than bubble sort, and are no more complex. For this reason, bubble sort is rarely used in practice.\\nLike insertion sort, bubble sort is adaptive, giving it an advantage over algorithms like quicksort. This means that it may outperform those algorithms in cases where the list is already mostly sorted (having a small number of inversions), despite the fact that it has worse average-case time complexity. For example, bubble sort is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   on a list that is already sorted, while quicksort would still perform its entire \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   sorting process.\\nWhile any sorting algorithm can be made \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   on a presorted list simply by checking the list before the algorithm runs, improved performance on almost-sorted lists is harder to replicate.\\n\\nRabbits and turtles\\nThe distance and direction that elements must move during the sort determine bubble sort's performance because elements move in different directions at different speeds. An element that must move toward the end of the list can move quickly because it can take part in successive swaps. For example, the largest element in the list will win every swap, so it moves to its sorted position on the first pass even if it starts near the beginning. On the other hand, an element that must move toward the beginning of the list cannot move faster than one step per pass, so elements move toward the beginning very slowly. If the smallest element is at the end of the list, it will take \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   passes to move it to the beginning. This has led to these types of elements being named rabbits and turtles, respectively, after the characters in Aesop's fable of The Tortoise and the Hare.\\nVarious efforts have been made to eliminate turtles to improve upon the speed of bubble sort. Cocktail sort is a bi-directional bubble sort that goes from beginning to end, and then reverses itself, going end to beginning. It can move turtles fairly well, but it retains \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   worst-case complexity. Comb sort compares elements separated by large gaps, and can move turtles extremely quickly before proceeding to smaller and smaller gaps to smooth out the list. Its average speed is comparable to faster algorithms like quicksort.\\n\\nStep-by-step example\\nTake an array of numbers \\\"5 1 4 2 8\\\", and sort the array from lowest number to greatest number using bubble sort. In each step, elements written in bold are being compared. Three passes will be required;\\n\\nFirst Pass\\n( 5 1 4 2 8 ) \\u2192 ( 1 5 4 2 8 ), Here, algorithm compares the first two elements, and swaps since 5 > 1.\\n( 1 5 4 2 8 ) \\u2192 ( 1 4 5 2 8 ), Swap since 5 > 4\\n( 1 4 5 2 8 ) \\u2192 ( 1 4 2 5 8 ), Swap since 5 > 2\\n( 1 4 2 5 8 ) \\u2192 ( 1 4 2 5 8 ), Now, since these elements are already in order (8 > 5), algorithm does not swap them.\\nSecond Pass\\n( 1 4 2 5 8 ) \\u2192 ( 1 4 2 5 8 )\\n( 1 4 2 5 8 ) \\u2192 ( 1 2 4 5 8 ), Swap since 4 > 2\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )Now, the array is already sorted, but the algorithm does not know if it is completed. The algorithm needs one additional whole pass without any swap to know it is sorted.\\n\\nThird Pass\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n( 1 2 4 5 8 ) \\u2192 ( 1 2 4 5 8 )\\n\\nImplementation\\nPseudocode implementation\\nIn pseudocode the algorithm can be expressed as (0-based array):\\n\\nOptimizing bubble sort\\nThe bubble sort algorithm can be optimized by observing that the n-th pass finds the n-th largest element and puts it into its final place. So, the inner loop can avoid looking at the last n \\u2212 1 items when running for the n-th time:\\n\\nMore generally, it can happen that more than one element is placed in their final position on a single pass. In particular, after every pass, all elements after the last swap are sorted, and do not need to be checked again. This allows to skip over many elements, resulting in about a worst case 50% improvement in comparison count (though no improvement in swap counts), and adds very little complexity because the new code subsumes the \\\"swapped\\\" variable:\\nTo accomplish this in pseudocode, the following can be written:\\n\\nAlternate modifications, such as the cocktail shaker sort attempt to improve on the bubble sort performance while keeping the same idea of repeatedly comparing and swapping adjacent items.\\n\\nUse\\nAlthough bubble sort is one of the simplest sorting algorithms to understand and implement, its O(n2) complexity means that its efficiency decreases dramatically on lists of more than a small number of elements. Even among simple O(n2) sorting algorithms, algorithms like insertion sort are usually considerably more efficient.\\nDue to its simplicity, bubble sort is often used to introduce the concept of an algorithm, or a sorting algorithm, to introductory computer science students. However, some researchers such as Owen Astrachan have gone to great lengths to disparage bubble sort and its continued popularity in computer science education, recommending that it no longer even be taught.The Jargon File, which famously calls bogosort \\\"the archetypical [sic] perversely awful algorithm\\\", also calls bubble sort \\\"the generic bad algorithm\\\". Donald Knuth, in The Art of Computer Programming, concluded that \\\"the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems\\\", some of which he then discusses.Bubble sort is asymptotically equivalent in running time to insertion sort in the worst case, but the two algorithms differ greatly in the number of swaps necessary. Experimental results such as those of Astrachan have also shown that insertion sort performs considerably better even on random lists. For these reasons many modern algorithm textbooks avoid using the bubble sort algorithm in favor of insertion sort.\\nBubble sort also interacts poorly with modern CPU hardware. It produces at least twice as many writes as insertion sort, twice as many cache misses, and asymptotically more branch mispredictions. Experiments by Astrachan sorting strings in Java show bubble sort to be roughly one-fifth as fast as an insertion sort and 70% as fast as a selection sort.In computer graphics bubble sort is popular for its capability to detect a very small error (like swap of just two elements) in almost-sorted arrays and fix it with just linear complexity (2n). For example, it is used in a polygon filling algorithm, where bounding lines are sorted by their x coordinate at a specific scan line (a line parallel to the x axis) and with incrementing y their order changes (two elements are swapped) only at intersections of two lines. Bubble sort is a stable sort algorithm, like insertion sort.\\n\\nVariations\\nOdd\\u2013even sort is a parallel version of bubble sort, for message passing systems.\\nPasses can be from right to left, rather than left to right. This is more efficient for lists with unsorted items added to the end.\\nCocktail shaker sort alternates leftwards and rightwards passes.\\nI can't believe it can sort is a sorting algorithm that appears to be an incorrect version of bubble sort, but can be formally proven to work in a way more akin to insertion sort.\\n\\nDebate over name\\nBubble sort has been occasionally referred to as a \\\"sinking sort\\\".For example, Donald Knuth describes the insertion of values at or towards their desired location as letting \\\"[the value] settle to its proper level\\\", and that \\\"this method of sorting has sometimes been called the sifting or sinking technique.This debate is perpetuated by the ease with which one may consider this algorithm from two different but equally valid perspectives:\\n\\nThe larger values might be regarded as heavier and therefore be seen to progressively sink to the bottom of the list\\nThe smaller values might be regarded as lighter and therefore be seen to progressively bubble up to the top of the list.\\n\\nIn popular culture\\nIn 2007, former Google CEO Eric Schmidt asked then-presidential candidate Barack Obama during an interview about the best way to sort one million integers; Obama paused for a moment and replied: \\\"I think the bubble sort would be the wrong way to go.\\\"\\n\\nNotes\\nReferences\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Problem 2-2, pg.40.\\nSorting in the Presence of Branch Prediction and Caches\\nFundamentals of Data Structures by Ellis Horowitz, Sartaj Sahni and Susan Anderson-Freed ISBN 81-7371-605-6\\nOwen Astrachan. Bubble Sort: An Archaeological Algorithmic Analysis\\nComputer Integrated Manufacturing by Spasic PhD, Srdic MSc, Open Source, 1987.[1]\\n\\nExternal links\\n\\nMartin, David R. (2007). \\\"Animated Sorting Algorithms: Bubble Sort\\\". Archived from the original on 2015-03-03. \\u2013 graphical demonstration\\n\\\"Lafore's Bubble Sort\\\". (Java applet animation)\\nOEIS sequence A008302 (Table (statistics) of the number of permutations of [n] that need k pair-swaps during the sorting)\"}, {\"Bucket sort\": \"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort,  a generalization of pigeonhole sort that allows multiple keys per bucket, and is a cousin of radix sort in the most-to-least significant digit flavor. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity depends on the algorithm used to sort each bucket, the number of buckets to use, and whether the input is uniformly distributed.\\nBucket sort works as follows:\\n\\nSet up an array of initially empty \\\"buckets\\\".\\nScatter: Go over the original array, putting each object in its bucket.\\nSort each non-empty bucket.\\nGather: Visit the buckets in order and put all elements back into the original array.\\n\\nPseudocode\\nfunction bucketSort(array, k) is\\n    buckets \\u2190 new array of k empty lists\\n    M \\u2190 1 + the maximum key value in the array\\n    for i = 0 to length(array) do\\n        insert array[i] into buckets[floor(k \\u00d7 array[i] / M)]\\n    for i = 0 to k do \\n        nextSort(buckets[i])\\n    return the concatenation of buckets[0], ...., buckets[k]\\n\\nLet array denote the array to be sorted and k denote the number of buckets to use. One can compute the maximum key value in linear time by iterating over all the keys once. The floor function must be used to convert a floating number to an integer ( and possibly casting of datatypes too ). The function nextSort is a sorting function used to sort each bucket. Conventionally, insertion sort is used, but other algorithms could be used as well, such as selection sort or merge sort. Using bucketSort itself as nextSort produces a relative of radix sort; in particular, the case n = 2 corresponds to quicksort (although potentially with poor pivot choices).\\n\\nAnalysis\\nWorst-case analysis\\nWhen the input contains several keys that are close to each other (clustering), those elements are likely to be placed in the same bucket, which results in some buckets containing more elements than average. The worst-case scenario occurs when all the elements are placed in a single bucket. The overall performance would then be dominated by the algorithm used to sort each bucket, for example \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   insertion sort or \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        (\\n        n\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log(n))}\\n   comparison sort algorithms, such as merge sort.\\n\\nAverage-case analysis\\nConsider the case that the input is uniformly distributed. The first step, which is initialize the buckets and find the maximum key value in the array, can be done in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time. If division and multiplication can be done in constant time, then scattering each element to its bucket also costs \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  . Assume insertion sort is used to sort each bucket, then the third step costs \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          \\n            \\u2211\\n            \\n              i\\n              =\\n              1\\n            \\n            \\n              k\\n            \\n          \\n          \\n            \\n              n\\n              \\n                i\\n              \\n              \\n                2\\n              \\n            \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle O(\\\\textstyle \\\\sum _{i=1}^{k}\\\\displaystyle n_{i}^{2})}\\n  , where \\n  \\n    \\n      \\n        \\n          n\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n_{i}}\\n   is the length of the bucket indexed \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  . Since we are concerning the average time, the expectation \\n  \\n    \\n      \\n        E\\n        (\\n        \\n          n\\n          \\n            i\\n          \\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle E(n_{i}^{2})}\\n   has to be evaluated instead. Let \\n  \\n    \\n      \\n        \\n          X\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X_{ij}}\\n   be the random variable that is \\n  \\n    \\n      \\n        1\\n      \\n    \\n    {\\\\displaystyle 1}\\n   if element \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n   is placed in bucket \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , and \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n   otherwise. We have \\n  \\n    \\n      \\n        \\n          n\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          \\u2211\\n          \\n            j\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          X\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n_{i}=\\\\sum _{j=1}^{n}X_{ij}}\\n  . Therefore,\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                E\\n                (\\n                \\n                  n\\n                  \\n                    i\\n                  \\n                  \\n                    2\\n                  \\n                \\n                )\\n              \\n              \\n                \\n                =\\n                E\\n                \\n                  (\\n                  \\n                    \\n                      \\u2211\\n                      \\n                        j\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        j\\n                      \\n                    \\n                    \\n                      \\u2211\\n                      \\n                        k\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        k\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                E\\n                \\n                  (\\n                  \\n                    \\n                      \\u2211\\n                      \\n                        j\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      \\u2211\\n                      \\n                        k\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        j\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        k\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                E\\n                \\n                  (\\n                  \\n                    \\n                      \\u2211\\n                      \\n                        j\\n                        =\\n                        1\\n                      \\n                      \\n                        n\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        j\\n                      \\n                      \\n                        2\\n                      \\n                    \\n                  \\n                  )\\n                \\n                +\\n                E\\n                \\n                  (\\n                  \\n                    \\n                      \\u2211\\n                      \\n                        1\\n                        \\u2264\\n                        j\\n                        ,\\n                        k\\n                        \\u2264\\n                        n\\n                      \\n                    \\n                    \\n                      \\u2211\\n                      \\n                        j\\n                        \\u2260\\n                        k\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        j\\n                      \\n                    \\n                    \\n                      X\\n                      \\n                        i\\n                        k\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}E(n_{i}^{2})&=E\\\\left(\\\\sum _{j=1}^{n}X_{ij}\\\\sum _{k=1}^{n}X_{ik}\\\\right)\\\\\\\\&=E\\\\left(\\\\sum _{j=1}^{n}\\\\sum _{k=1}^{n}X_{ij}X_{ik}\\\\right)\\\\\\\\&=E\\\\left(\\\\sum _{j=1}^{n}X_{ij}^{2}\\\\right)+E\\\\left(\\\\sum _{1\\\\leq j,k\\\\leq n}\\\\sum _{j\\\\neq k}X_{ij}X_{ik}\\\\right)\\\\end{aligned}}}\\n  The last line separates the summation into the case \\n  \\n    \\n      \\n        j\\n        =\\n        k\\n      \\n    \\n    {\\\\displaystyle j=k}\\n   and the case \\n  \\n    \\n      \\n        j\\n        \\u2260\\n        k\\n      \\n    \\n    {\\\\displaystyle j\\\\neq k}\\n  . Since the chance of an object distributed to bucket \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   is \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        k\\n      \\n    \\n    {\\\\displaystyle 1/k}\\n  , \\n  \\n    \\n      \\n        \\n          X\\n          \\n            i\\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X_{ij}}\\n   is 1 with probability \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        k\\n      \\n    \\n    {\\\\displaystyle 1/k}\\n   and 0 otherwise. \\n\\n  \\n    \\n      \\n        E\\n        (\\n        \\n          X\\n          \\n            i\\n            j\\n          \\n          \\n            2\\n          \\n        \\n        )\\n        =\\n        \\n          1\\n          \\n            2\\n          \\n        \\n        \\u22c5\\n        \\n          (\\n          \\n            \\n              1\\n              k\\n            \\n          \\n          )\\n        \\n        +\\n        \\n          0\\n          \\n            2\\n          \\n        \\n        \\u22c5\\n        \\n          (\\n          \\n            1\\n            \\u2212\\n            \\n              \\n                1\\n                k\\n              \\n            \\n          \\n          )\\n        \\n        =\\n        \\n          \\n            1\\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E(X_{ij}^{2})=1^{2}\\\\cdot \\\\left({\\\\frac {1}{k}}\\\\right)+0^{2}\\\\cdot \\\\left(1-{\\\\frac {1}{k}}\\\\right)={\\\\frac {1}{k}}}\\n  \\n\\n  \\n    \\n      \\n        E\\n        (\\n        \\n          X\\n          \\n            i\\n            j\\n          \\n        \\n        \\n          X\\n          \\n            i\\n            k\\n          \\n        \\n        )\\n        =\\n        1\\n        \\u22c5\\n        \\n          (\\n          \\n            \\n              1\\n              k\\n            \\n          \\n          )\\n        \\n        \\n          (\\n          \\n            \\n              1\\n              k\\n            \\n          \\n          )\\n        \\n        =\\n        \\n          \\n            1\\n            \\n              k\\n              \\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E(X_{ij}X_{ik})=1\\\\cdot \\\\left({\\\\frac {1}{k}}\\\\right)\\\\left({\\\\frac {1}{k}}\\\\right)={\\\\frac {1}{k^{2}}}}\\n  With the summation, it would be\\n\\n  \\n    \\n      \\n        E\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                j\\n                =\\n                1\\n              \\n              \\n                n\\n              \\n            \\n            \\n              X\\n              \\n                i\\n                j\\n              \\n              \\n                2\\n              \\n            \\n          \\n          )\\n        \\n        +\\n        E\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                1\\n                \\u2264\\n                j\\n                ,\\n                k\\n                \\u2264\\n                n\\n              \\n            \\n            \\n              \\u2211\\n              \\n                j\\n                \\u2260\\n                k\\n              \\n            \\n            \\n              X\\n              \\n                i\\n                j\\n              \\n            \\n            \\n              X\\n              \\n                i\\n                k\\n              \\n            \\n          \\n          )\\n        \\n        =\\n        n\\n        \\u22c5\\n        \\n          \\n            1\\n            k\\n          \\n        \\n        +\\n        n\\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\u22c5\\n        \\n          \\n            1\\n            \\n              k\\n              \\n                2\\n              \\n            \\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                n\\n                \\n                  2\\n                \\n              \\n              +\\n              n\\n              k\\n              \\u2212\\n              n\\n            \\n            \\n              k\\n              \\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle E\\\\left(\\\\sum _{j=1}^{n}X_{ij}^{2}\\\\right)+E\\\\left(\\\\sum _{1\\\\leq j,k\\\\leq n}\\\\sum _{j\\\\neq k}X_{ij}X_{ik}\\\\right)=n\\\\cdot {\\\\frac {1}{k}}+n(n-1)\\\\cdot {\\\\frac {1}{k^{2}}}={\\\\frac {n^{2}+nk-n}{k^{2}}}}\\n  Finally, the complexity would be \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                1\\n              \\n              \\n                k\\n              \\n            \\n            E\\n            (\\n            \\n              n\\n              \\n                i\\n              \\n              \\n                2\\n              \\n            \\n            )\\n          \\n          )\\n        \\n        =\\n        O\\n        \\n          (\\n          \\n            \\n              \\u2211\\n              \\n                i\\n                =\\n                1\\n              \\n              \\n                k\\n              \\n            \\n            \\n              \\n                \\n                  \\n                    n\\n                    \\n                      2\\n                    \\n                  \\n                  +\\n                  n\\n                  k\\n                  \\u2212\\n                  n\\n                \\n                \\n                  k\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n          \\n          )\\n        \\n        =\\n        O\\n        \\n          (\\n          \\n            \\n              \\n                \\n                  n\\n                  \\n                    2\\n                  \\n                \\n                k\\n              \\n            \\n            +\\n            n\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left(\\\\sum _{i=1}^{k}E(n_{i}^{2})\\\\right)=O\\\\left(\\\\sum _{i=1}^{k}{\\\\frac {n^{2}+nk-n}{k^{2}}}\\\\right)=O\\\\left({\\\\frac {n^{2}}{k}}+n\\\\right)}\\n  .\\nThe last step of bucket sort, which is concatenating all the sorted objects in each buckets, requires \\n  \\n    \\n      \\n        O\\n        (\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(k)}\\n   time. Therefore, the total complexity is \\n  \\n    \\n      \\n        O\\n        \\n          (\\n          \\n            n\\n            +\\n            \\n              \\n                \\n                  n\\n                  \\n                    2\\n                  \\n                \\n                k\\n              \\n            \\n            +\\n            k\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle O\\\\left(n+{\\\\frac {n^{2}}{k}}+k\\\\right)}\\n  . Note that if k is chosen to be \\n  \\n    \\n      \\n        k\\n        =\\n        \\u0398\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle k=\\\\Theta (n)}\\n  , then bucket sort runs in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   average time, given a uniformly distributed input.\\n\\nOptimizations\\nA common optimization is to put the unsorted elements of the buckets back in the original array first, then run insertion sort over the complete array; because insertion sort's runtime is based on how far each element is from its final position, the number of comparisons remains relatively small, and the memory hierarchy is better exploited by storing the list contiguously in memory.If the input distribution is known or can be estimated, buckets can often be chosen which contain constant density (rather than merely having constant size). This allows \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   average time complexity even without uniformly distributed input.\\n\\nVariants\\nGeneric bucket sort\\nThe most common variant of bucket sort operates on a list of n numeric inputs between zero and some maximum value M and divides the value range into n buckets each of size M/n. If each bucket is sorted using insertion sort, the sort can be shown to run in expected linear time (where the average is taken over all possible inputs). However, the performance of this sort degrades with clustering; if many values occur close together, they will all fall into a single bucket and be sorted slowly.  This performance degradation is avoided in the original bucket sort algorithm by assuming that the input is generated by a random process that distributes elements uniformly over the interval [0,1).\\n\\nProxmapSort\\nSimilar to generic bucket sort as described above, ProxmapSort works by dividing an array of keys into subarrays via the use of a \\\"map key\\\" function that preserves a partial ordering on the keys; as each key is added to its subarray, insertion sort is used to keep that subarray sorted, resulting in the entire array being in sorted order when ProxmapSort completes. ProxmapSort differs from bucket sorts in its use of the map key to place the data approximately where it belongs in sorted order, producing a \\\"proxmap\\\" \\u2014 a proximity mapping \\u2014 of the keys.\\n\\nHistogram sort\\nAnother variant of bucket sort known as histogram sort or counting sort adds an initial pass that counts the number of elements that will fall into each bucket using a count array. Using this information, the array values can be arranged into a sequence of buckets in-place by a sequence of exchanges, leaving no space overhead for bucket storage.\\n\\nPostman's sort\\nThe Postman's sort is a variant of bucket sort that takes advantage of a hierarchical structure of elements, typically described by a set of attributes. This is the algorithm used by letter-sorting machines in post offices: mail is sorted first between domestic and international; then by state, province or territory; then by destination post office; then by routes, etc. Since keys are not compared against each other, sorting time is O(cn), where c depends on the size of the key and number of buckets. This is similar to a radix sort that works \\\"top down,\\\" or \\\"most significant digit first.\\\"\\n\\nShuffle sort\\nThe shuffle sort is a variant of bucket sort that begins by removing the first 1/8 of the n items to be sorted, sorts them recursively, and puts them in an array. This creates n/8 \\\"buckets\\\" to which the remaining 7/8 of the items are distributed. Each \\\"bucket\\\" is then sorted, and the \\\"buckets\\\" are concatenated into a sorted array.\\n\\nComparison with other sorting algorithms\\nBucket sort can be seen as a generalization of counting sort; in fact, if each bucket has size 1 then bucket sort degenerates to counting sort. The variable bucket size of bucket sort allows it to use O(n) memory instead of O(M) memory, where M is the number of distinct values; in exchange, it gives up counting sort's O(n + M) worst-case behavior.\\nBucket sort with two buckets is effectively a version of quicksort where the pivot value is always selected to be the middle value of the value range. While this choice is effective for uniformly distributed inputs, other means of choosing the pivot in quicksort such as randomly selected pivots make it more resistant to clustering in the input distribution.\\nThe n-way mergesort algorithm also begins by distributing the list into n sublists and sorting each one; however, the sublists created by mergesort have overlapping value ranges and so cannot be recombined by simple concatenation as in bucket sort. Instead, they must be interleaved by a merge algorithm. However, this added expense is counterbalanced by the simpler scatter phase and the ability to ensure that each sublist is the same size, providing a good worst-case time bound.\\nTop-down radix sort can be seen as a special case of bucket sort where both the range of values and the number of buckets is constrained to be a power of two. Consequently, each bucket's size is also a power of two, and the procedure can be applied recursively. This approach can accelerate the scatter phase, since we only need to examine a prefix of the bit representation of each element to determine its bucket.\\n\\nReferences\\nPaul E. Black \\\"Postman's Sort\\\" from Dictionary of Algorithms and Data Structures at NIST.\\nRobert Ramey '\\\"The Postman's Sort\\\" C Users Journal Aug. 1992\\nNIST's Dictionary of Algorithms and Data Structures: bucket sort\\n\\nExternal links\\nBucket Sort Code for Ansi C\\nVariant of Bucket Sort with Demo\"}, {\"Cascade merge sort\": \"Cascade merge sort is similar to the polyphase merge sort but uses a simpler distribution.  The merge is slower than a polyphase merge when there are fewer than six files, but faster when there are more than six.\\n\\nReferences\\nBradley, James (1982), File and Data Base Techniques, Holt, Rinehart and Winston, ISBN 0-03-058673-9\\n\\nExternal links\\nhttp://www.minkhollow.ca/Courses/461/Notes/Cosequential/Cascade.html\"}, {\"Cocktail shaker sort\": \"Cocktail shaker sort, also known as bidirectional bubble sort, cocktail sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is an extension of bubble sort.  The algorithm extends bubble sort by operating in two directions.  While it improves on bubble sort by more quickly moving items to the beginning of the list, it provides only marginal performance improvements. \\nLike most variants of bubble sort, cocktail shaker sort is used primarily as an educational tool. More performant algorithms such as quicksort, merge sort, or timsort are used by the sorting libraries built into popular programming languages such as Python and Java.\\n\\nPseudocode\\nThe simplest form goes through the whole list each time:\\n\\nprocedure cocktailShakerSort(A : list of sortable items) is\\n    do\\n        swapped := false\\n        for each i in 0 to length(A) \\u2212 1 do:\\n            if A[i] > A[i + 1] then // test whether the two elements are in the wrong order\\n                swap(A[i], A[i + 1]) // let the two elements change places\\n                swapped := true\\n            end if\\n        end for\\n        if not swapped then\\n            // we can exit the outer loop here if no swaps occurred.\\n            break do-while loop\\n        end if\\n        swapped := false\\n        for each i in length(A) \\u2212 1 to 0 do:\\n            if A[i] > A[i + 1] then\\n                swap(A[i], A[i + 1])\\n                swapped := true\\n            end if\\n        end for\\n    while swapped // if no elements have been swapped, then the list is sorted\\nend procedure\\n\\nThe first rightward pass will shift the largest element to its correct place at the end, and the following leftward pass will shift the smallest element to its correct place at the beginning. The second complete pass will shift the second largest and second smallest elements to their correct places, and so on. After i passes, the first i and the last i elements in the list are in their correct positions, and do not need to be checked. By shortening the part of the list that is sorted each time, the number of operations can be halved (see bubble sort).\\nThis is an example of the algorithm in MATLAB/OCTAVE with the optimization of remembering the last swap index and updating the bounds.\\n\\nDifferences from bubble sort\\nCocktail shaker sort is a slight variation of bubble sort. It differs in that instead of repeatedly passing through the list from bottom to top, it passes alternately from bottom to top and then from top to bottom. It can achieve slightly better performance than a standard bubble sort. The reason for this is that bubble sort only passes through the list in one direction and therefore can only move items backward one step each iteration.\\nAn example of a list that proves this point is the list (2,3,4,5,1), which would only need to go through one pass of cocktail sort to become sorted, but if using an ascending bubble sort would take four passes. However one cocktail sort pass should be counted as two bubble sort passes. Typically cocktail sort is less than two times faster than bubble sort.\\nAnother optimization can be that the algorithm remembers where the last actual swap has been done. In the next iteration, there will be no swaps beyond this limit and the algorithm has shorter passes. As the cocktail shaker sort goes bidirectionally, the range of possible swaps, which is the range to be tested, will reduce per pass, thus reducing the overall running time slightly.\\n\\nComplexity\\nThe complexity of the cocktail shaker sort in big O notation is \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   for both the worst case and the average case, but it becomes closer to \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   if the list is mostly ordered before applying the sorting algorithm. For example, if every element is at a position that differs by at most k (k \\u2265 1) from the position it is going to end up in, the complexity of cocktail shaker sort becomes \\n  \\n    \\n      \\n        O\\n        (\\n        k\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle O(kn).}\\n  \\nThe cocktail shaker sort is also briefly discussed in the book The Art of Computer Programming, along with similar refinements of bubble sort. In conclusion, Knuth states about bubble sort and its improvements:\\n\\nBut none of these refinements leads to an algorithm better than straight insertion [that is, insertion sort]; and we already know that straight insertion isn't suitable for large N. [...] In short, the bubble sort seems to have nothing to recommend it, except a catchy name and the fact that it leads to some interesting theoretical problems.\\n\\nReferences\\nSources\\nHartenstein, R. (July 2010). \\\"A new World Model of Computing\\\" (PDF). The Grand Challenge to Reinvent Computing. Belo Horizonte, Brazil: CSBC. Archived from the original (PDF) on 2013-08-07. Retrieved 2011-01-14.\\n\\nExternal links\\n\\nInteractive demo of cocktail sort\\nJava source code and an animated demo of cocktail sort (called bi-directional bubble sort) and several other algorithms\\n\\\".NET Implementation of cocktail sort and several other algorithms\\\". Archived from the original on 2012-02-12.\"}, {\"Counting sort\": \"In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small positive integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that possess distinct key values, and applying prefix sum on those counts to determine the positions of each key value in the output sequence. Its running time is linear in the number of items and the difference between the maximum key value and the minimum key value, so it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items. It is often used as a subroutine in radix sort, another sorting algorithm, which can handle larger keys more efficiently.Counting sort is not a comparison sort; it uses key values as indexes into an array and the \\u03a9(n log n) lower bound for comparison sorting will not apply. Bucket sort may be used in lieu of counting sort, and entails a similar time analysis. However, compared to counting sort, bucket sort requires linked lists, dynamic arrays, or a large amount of pre-allocated memory to hold the sets of items within each bucket, whereas counting sort stores a single number (the count of items) per bucket.\\n\\nInput and output assumptions\\nIn the most general case, the input to counting sort consists of a collection of n items, each of which has a non-negative integer key whose maximum value is at most k.\\nIn some descriptions of counting sort, the input to be sorted is assumed to be more simply a sequence of integers itself, but this simplification does not accommodate many applications of counting sort. For instance, when used as a subroutine in radix sort, the keys for each call to counting sort are individual digits of larger item keys; it would not suffice to return only a sorted list of the key digits, separated from the items.\\nIn applications such as in radix sort, a bound on the maximum key value k will be known in advance, and can be assumed to be part of the input to the algorithm. However, if the value of k is not already known then it may be computed, as a first step, by an additional loop over the data to determine the maximum key value.\\nThe output is an array of the elements ordered by their keys. Because of its application to radix sorting, counting sort must be a stable sort; that is, if two elements share the same key, their relative order in the output array and their relative order in the input array should match.\\n\\nPseudocode\\nIn pseudocode, the algorithm may be expressed as:\\n\\nfunction CountingSort(input, k)\\n    \\n    count \\u2190 array of k + 1 zeros\\n    output \\u2190 array of same length as input\\n    \\n    for i = 0 to length(input) - 1 do\\n        j = key(input[i])\\n        count[j] = count[j] + 1\\n\\n    for i = 1 to k do\\n        count[i] = count[i] + count[i - 1]\\n\\n    for i = length(input) - 1 down to 0 do\\n        j = key(input[i])\\n        count[j] = count[j] - 1\\n        output[count[j]] = input[i]\\n\\n    return output\\n\\nHere input is the input array to be sorted, key returns the numeric key of each item in the input array, count is an auxiliary array used first to store the numbers of items with each key, and then (after the second loop) to store the positions where items with each key should be placed,\\nk is the maximum value of the non-negative key values and output is the sorted output array.\\nIn summary, the algorithm loops over the items in the first loop, computing a histogram of the number of times each key occurs within the input collection. After that in the second loop, it performs a prefix sum computation on count in order to determine, for each key, the position range where the items having that key should be placed; i.e. items of key \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   should be placed starting in position count[\\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  ]. Finally, in the third loop, it loops over the items of input again, but in reverse order, moving each item into its sorted position in the output array.The relative order of items with equal keys is preserved here; i.e., this is a stable sort.\\n\\nComplexity analysis\\nBecause the algorithm uses only simple for loops, without recursion or subroutine calls, it is straightforward to analyze. The initialization of the count array, and the second for loop which performs a prefix sum on the count array, each iterate at most k + 1 times and therefore take O(k) time. The other two for loops, and the initialization of the output array, each take O(n) time. Therefore, the time for the whole algorithm is the sum of the times for these steps, O(n + k).Because it uses arrays of length k + 1 and n, the total space usage of the algorithm is also O(n + k). For problem instances in which the maximum key value is significantly smaller than the number of items, counting sort can be highly space-efficient, as the only storage it uses other than its input and output arrays is the Count array which uses space O(k).\\n\\nVariant algorithms\\nIf each item to be sorted is itself an integer, and used as key as well, then the second and third loops of counting sort can be combined; in the second loop, instead of computing the position where items with key i should be placed in the output, simply append Count[i] copies of the number i to the output.\\nThis algorithm may also be used to eliminate duplicate keys, by replacing the Count array with a bit vector that stores a one for a key that is present in the input and a zero for a key that is not present. If additionally the items are the integer keys themselves, both second and third loops can be omitted entirely and the bit vector will itself serve as output, representing the values as offsets of the non-zero entries, added to the range's lowest value. Thus the keys are sorted and the duplicates are eliminated in this variant just by being placed into the bit array.\\nFor data in which the maximum key size is significantly smaller than the number of data items, counting sort may be parallelized by splitting the input into subarrays of approximately equal size, processing each subarray in parallel to generate a separate count array for each subarray, and then merging the count arrays. When used as part of a parallel radix sort algorithm, the key size (base of the radix representation) should be chosen to match the size of the split subarrays. The simplicity of the counting sort algorithm and its use of the easily parallelizable prefix sum primitive also make it usable in more fine-grained parallel algorithms.As described, counting sort is not an in-place algorithm; even disregarding the count array, it needs separate input and output arrays. It is possible to modify the algorithm so that it places the items into sorted order within the same array that was given to it as the input, using only the count array as auxiliary storage; however, the modified in-place version of counting sort is not stable.\\n\\nHistory\\nAlthough radix sorting itself dates back far longer,\\ncounting sort, and its application to radix sorting, were both invented by Harold H. Seward in 1954.\\n\\nReferences\\nExternal links\\n\\nCounting Sort html5 visualization\\nDemonstration applet from Cardiff University Archived 2013-06-02 at the Wayback Machine\\nKagel, Art S. (2 June 2006), \\\"counting sort\\\",  in Black, Paul E. (ed.), Dictionary of Algorithms and Data Structures, U.S. National Institute of Standards and Technology, retrieved 2011-04-21.\"}, {\"Cubesort\": \"Cubesort is a parallel sorting algorithm that builds a self-balancing multi-dimensional array from the keys to be sorted. As the axes are of similar length the structure resembles a cube. After each key is inserted the cube can be rapidly converted to an array.A cubesort implementation written in C was published in 2014.\\n\\nOperation\\nCubesort's algorithm uses a specialized binary search on each axis to find the location to insert an element. When an axis grows too large it is split. Locality of reference is optimal as only four binary searches are performed on small arrays for each insertion. By using many small dynamic arrays the high cost for insertion on single large arrays is avoided.\\n\\nReferences\\nExternal links\\nCubesort description and implementation in C\\nNiedermeier, Rolf (1996). \\\"Recursively divisible problems\\\". Algorithms and Computation. Lecture Notes in Computer Science. Vol. 1178. Springer Berlin Heidelberg. pp. 187\\u2013188. doi:10.1007/BFb0009494. eISSN 1611-3349. ISBN 978-3-540-62048-8. ISSN 0302-9743. (passing mention)\"}, {\"Gnome sort\": \"Gnome sort (nicknamed stupid sort) is a variation of the insertion sort sorting algorithm that does not use nested loops. Gnome sort was originally proposed by Iranian computer scientist Hamid Sarbazi-Azad (professor of Computer Science and Engineering at Sharif University of Technology) in 2000. The sort was first called stupid sort (not to be confused with bogosort), and then later described by Dick Grune and named gnome sort.Gnome sort performs at least as many comparisons as insertion sort and has the same asymptotic run time characteristics. Gnome sort works by building a sorted list one element at a time, getting each item to the proper place in a series of swaps. The average running time is O(n2) but tends towards O(n) if the list is initially almost sorted.Dick Grune described the sorting method with the following story:\\n\\nGnome Sort is based on the technique used by the standard Dutch Garden Gnome (Du.: tuinkabouter). \\nHere is how a garden gnome sorts a line of flower pots. \\nBasically, he looks at the flower pot next to him and the previous one; if they are in the right order he steps one pot forward, otherwise, he swaps them and steps one pot backward. \\nBoundary conditions: if there is no previous pot, he steps forwards; if there is no pot next to him, he is done.\\n\\nPseudocode\\nHere is pseudocode for the gnome sort using a zero-based array:\\n\\nExample\\nGiven an unsorted array, a = [5, 3, 2, 4], the gnome sort takes the following steps during the while loop. The current position is highlighted in bold and indicated as a value of the variable pos.\\n\\nNotes\\nReferences\\nExternal links\\n\\nGnome sort\"}, {\"Insertion sort\": \"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time by comparisons. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:\\n\\nSimple implementation: Jon Bentley shows a three-line C/C++ version that is five lines when optimized.\\nEfficient for (quite) small data sets, much like other quadratic (i.e., O(n2)) sorting algorithms\\nMore efficient in practice than most other simple quadratic algorithms such as selection sort or bubble sort\\nAdaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(kn) when each element in the input is no more than k places away from its sorted position\\nStable; i.e., does not change the relative order of elements with equal keys\\nIn-place; i.e., only requires a constant amount O(1) of additional memory space\\nOnline; i.e., can sort a list as it receives itWhen people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.\\n\\nAlgorithm\\nInsertion sort iterates, consuming one input element each repetition, and grows a sorted output list. At each iteration, insertion sort removes one element from the input data, finds the location it belongs within the sorted list, and inserts it there. It repeats until no input elements remain.\\nSorting is typically done in-place, by iterating up the array, growing the sorted list behind it. At each array-position, it checks the value there against the largest value in the sorted list (which happens to be next to it, in the previous array-position checked). If larger, it leaves the element in place and moves to the next. If smaller, it finds the correct position within the sorted list, shifts all the larger values up to make a space, and inserts into that correct position.\\nThe resulting array after k iterations has the property where the first k + 1 entries are sorted (\\\"+1\\\" because the first entry is skipped). In each iteration the first remaining entry of the input is removed, and inserted into the result at the correct position, thus extending the result:\\n\\nbecomes\\n\\nwith each element greater than x copied to the right as it is compared against x.\\nThe most common variant of insertion sort, which operates on arrays, can be described as follows:\\n\\nSuppose there exists a function called Insert designed to insert a value into a sorted sequence at the beginning of an array. It operates by beginning at the end of the sequence and shifting each element one place to the right until a suitable position is found for the new element. The function has the side effect of overwriting the value stored immediately after the sorted sequence in the array.\\nTo perform an insertion sort, begin at the left-most element of the array and invoke Insert to insert each element encountered into its correct position. The ordered sequence into which the element is inserted is stored at the beginning of the array in the set of indices already examined. Each insertion overwrites a single value: the value being inserted.Pseudocode of the complete algorithm follows, where the arrays are zero-based:\\ni \\u2190 1\\nwhile i < length(A)\\n    j \\u2190 i\\n    while j > 0 and A[j-1] > A[j]\\n        swap A[j] and A[j-1]\\n        j \\u2190 j - 1\\n    end while\\n    i \\u2190 i + 1\\nend while\\n\\nThe outer loop runs over all the elements except the first one, because the single-element prefix A[0:1] is trivially sorted, so the invariant that the first i entries are sorted is true from the start. The inner loop moves element A[i] to its correct place so that after the loop, the first i+1 elements are sorted. Note that the and-operator in the test must use short-circuit evaluation, otherwise the test might result in an array bounds error, when j=0 and it tries to evaluate A[j-1] > A[j] (i.e. accessing A[-1] fails).\\nAfter expanding the swap operation in-place as x \\u2190 A[j]; A[j] \\u2190 A[j-1]; A[j-1] \\u2190 x (where x is a temporary variable), a slightly faster version can be produced that moves A[i] to its position in one go and only performs one assignment in the inner loop body:\\ni \\u2190 1\\nwhile i < length(A)\\n    x \\u2190 A[i]\\n    j \\u2190 i - 1\\n    while j >= 0 and A[j] > x\\n        A[j+1] \\u2190 A[j]\\n        j \\u2190 j - 1\\n    end while\\n    A[j+1] \\u2190 x\\n    i \\u2190 i + 1\\nend while\\n\\nThe new inner loop shifts elements to the right to clear a spot for x = A[i].\\nThe algorithm can also be implemented in a recursive way. The recursion just replaces the outer loop, calling itself and storing successively smaller values of n on the stack until n equals 0, where the function then returns up the call chain to execute the code after each recursive call starting with n equal to 1, with n increasing by 1 as each instance of the function returns to the prior instance. The initial call would be insertionSortR(A, length(A)-1).\\n\\nfunction insertionSortR(array A, int n)\\n    if n > 0\\n        insertionSortR(A, n-1)\\n        x \\u2190 A[n]\\n        j \\u2190 n-1\\n        while j >= 0 and A[j] > x\\n            A[j+1] \\u2190 A[j]\\n            j \\u2190 j-1\\n        end while\\n        A[j+1] \\u2190 x\\n    end if\\nend function\\n\\nIt does not make the code any shorter, it also doesn't reduce the execution time, but it increases the additional memory consumption from O(1) to O(N) (at the deepest level of recursion the stack contains N references to the A array, each with accompanying value of variable n from N down to 1).\\n\\nBest, worst, and average cases\\nThe best case input is an array that is already sorted. In this case insertion sort has a linear running time (i.e., O(n)). During each iteration, the first remaining element of the input is only compared with the right-most element of the sorted subsection of the array.\\nThe simplest worst case input is an array sorted in reverse order. The set of all worst case inputs consists of all arrays where each element is the smallest or second-smallest of the elements before it. In these cases every iteration of the inner loop will scan and shift the entire sorted subsection of the array before inserting the next element. This gives insertion sort a quadratic running time (i.e., O(n2)).\\nThe average case is also quadratic, which makes insertion sort impractical for sorting large arrays. However, insertion sort is one of the fastest algorithms for sorting very small arrays, even faster than quicksort; indeed, good quicksort implementations use insertion sort for arrays smaller than a certain threshold, also when arising as subproblems; the exact threshold must be determined experimentally and depends on the machine, but is commonly around ten.\\nExample: The following table shows the steps for sorting the sequence {3, 7, 4, 9, 5, 2, 6, 1}.  In each step, the key under consideration is underlined. The key that was moved (or left in place because it was the biggest yet considered) in the previous step is marked with an asterisk.\\n\\n3  7  4  9  5  2  6  1\\n3* 7  4  9  5  2  6  1\\n3  7* 4  9  5  2  6  1\\n3  4* 7  9  5  2  6  1\\n3  4  7  9* 5  2  6  1\\n3  4  5* 7  9  2  6  1\\n2* 3  4  5  7  9  6  1\\n2  3  4  5  6* 7  9  1\\n1* 2  3  4  5  6  7  9\\n\\nRelation to other sorting algorithms\\nInsertion sort is very similar to selection sort. As in selection sort, after k passes through the array, the first k elements are in sorted order. However, the fundamental difference between the two algorithms is that insertion sort scans backwards from the current key, while selection sort scans forwards.  This results in selection sort making the first k elements the k smallest elements of the unsorted input, while in insertion sort they are simply the first k elements of the input.\\nThe primary advantage of insertion sort over selection sort is that selection sort must always scan all remaining elements to find the absolute smallest element in the unsorted portion of the list, while insertion sort requires only a single comparison when the (k + 1)-st element is greater than the k-th element; when this is frequently true (such as if the input array is already sorted or partially sorted), insertion sort is distinctly more efficient compared to selection sort. On average (assuming the rank of the (k + 1)-st element rank is random), insertion sort will require comparing and shifting half of the previous k elements, meaning that insertion sort will perform about half as many comparisons as selection sort on average.\\nIn the worst case for insertion sort (when the input array is reverse-sorted), insertion sort performs just as many comparisons as selection sort. However, a disadvantage of insertion sort over selection sort is that it requires more writes due to the fact that, on each iteration, inserting the (k + 1)-st element into the sorted portion of the array requires many element swaps to shift all of the following elements, while only a single swap is required for each iteration of selection sort. In general, insertion sort will write to the array O(n2) times, whereas selection sort will write only O(n) times. For this reason selection sort may be preferable in cases where writing to memory is significantly more expensive than reading, such as with EEPROM or flash memory.\\nWhile some divide-and-conquer algorithms such as quicksort and mergesort outperform insertion sort for larger arrays, non-recursive sorting algorithms such as insertion sort or selection sort are generally faster for very small arrays (the exact size varies by environment and implementation, but is typically between 7 and 50 elements). Therefore, a useful optimization in the implementation of those algorithms is a hybrid approach, using the simpler algorithm when the array has been divided to a small size.\\n\\nVariants\\nD.L. Shell made substantial improvements to the algorithm; the modified version is called Shell sort.  The sorting algorithm compares elements separated by a distance that decreases on each pass. Shell sort has distinctly improved running times in practical work, with two simple variants requiring O(n3/2) and O(n4/3) running time.If the cost of comparisons exceeds the cost of swaps, as is the case for example with string keys stored by reference or with human interaction (such as choosing one of a pair displayed side-by-side), then using binary insertion sort may yield better performance. Binary insertion sort employs a binary search to determine the correct location to insert new elements, and therefore performs \\u2308log2 n\\u2309 comparisons in the worst case. When each element in the array is searched for and inserted this is O(n log n). The algorithm as a whole still has a running time of O(n2) on average because of the series of swaps required for each insertion.The number of swaps can be reduced by calculating the position of multiple elements before moving them. For example, if the target position of two elements is calculated before they are moved into the proper position, the number of swaps can be reduced by about 25% for random data. In the extreme case, this variant works similar to merge sort.\\nA variant named binary merge sort uses a binary insertion sort to sort groups of 32 elements, followed by a final sort using merge sort. It combines the speed of insertion sort on small data sets with the speed of merge sort on large data sets.To avoid having to make a series of swaps for each insertion, the input could be stored in a linked list, which allows elements to be spliced into or out of the list in constant time when the position in the list is known.  However, searching a linked list requires sequentially following the links to the desired position: a linked list does not have random access, so it cannot use a faster method such as binary search.  Therefore, the running time required for searching is O(n), and the time for sorting is O(n2). If a more sophisticated data structure (e.g., heap or binary tree) is used, the time required for searching and insertion can be reduced significantly; this is the essence of heap sort and binary tree sort.\\nIn 2006 Bender, Martin Farach-Colton, and Mosteiro published a new variant of insertion sort called library sort or gapped insertion sort that leaves a small number of unused spaces (i.e., \\\"gaps\\\") spread throughout the array. The benefit is that insertions need only shift elements over until a gap is reached. The authors show that this sorting algorithm runs with high probability in O(n log n) time.If a skip list is used, the insertion time is brought down to O(log n), and swaps are not needed because the skip list is implemented on a linked list structure.  The final running time for insertion would be O(n log n).\\n\\nList insertion sort code in C\\nIf the items are stored in a linked list, then the list can be sorted with O(1) additional space.  The algorithm starts with an initially empty (and therefore trivially sorted) list. The input items are taken off the list one at a time, and then inserted in the proper place in the sorted list. When the input list is empty, the sorted list has the desired result.\\n\\nThe algorithm below uses a trailing pointer for the insertion into the sorted list. A simpler recursive method rebuilds the list each time (rather than splicing) and can use O(n) stack space.\\n\\nReferences\\nFurther reading\\nKnuth, Donald (1998), \\\"5.2.1: Sorting by Insertion\\\", The Art of Computer Programming, vol. 3. Sorting and Searching (second ed.), Addison-Wesley, pp. 80\\u2013105, ISBN 0-201-89685-0.\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Insertion Sort at the Wayback Machine (archived 8 March 2015) \\u2013 graphical demonstration\\nAdamovsky, John Paul, Binary Insertion Sort \\u2013 Scoreboard \\u2013 Complete Investigation and C Implementation, Pathcom.\\nInsertion Sort \\u2013 a comparison with other O(n2) sorting algorithms, UK: Core war.\\nInsertion sort (C) (wiki), LiteratePrograms \\u2013 implementations of insertion sort in C and several other programming languages\"}, {\"Interpolation sort\": \"Interpolation sort is a kind of bucket sort. It uses an interpolation formula to assign data to the bucket. A general interpolation formula is:\\nInterpolation = INT(((Array[i] - min) / (max - min)) * (ArraySize - 1))\\n\\nAlgorithm\\nInterpolation sort (or histogram sort).\\nIt is a sorting algorithm that uses the interpolation formula to disperse data divide and conquer. Interpolation sort is also a variant of bucket sort algorithm. \\nThe interpolation sort method uses an array of record bucket lengths corresponding to the original number column. By operating the maintenance length array, the recursive algorithm can be prevented from changing the space complexity to \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   due to memory stacking. The segmentation record of the length array can using secondary function dynamically declare and delete the memory space of the array. The space complexity required to control the recursive program is \\n  \\n    \\n      \\n        O\\n        (\\n        3\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(3n)}\\n  . Contains a two-dimensional array of dynamically allocated memories and an array of record lengths. However the execution complexity can still be maintained as an efficient sorting method of \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        +\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n+k)}\\n  .\\nArray of dynamically allocated memory can be implemented by linked list, stack, queue, associative array, tree structure, etc. An array object such as JavaScript is applicable. The difference in data structure is related to the speed of data access and thus the time required for sorting.When the values in the ordered array are uniformly distributed approximately the arithmetic progression, the linear time of interpolation sort ordering is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\n\\nInterpolation sort algorithm\\nSet a bucket length array to record the length of the unsorted bucket. Initialize into the original array length.\\n[Main Sort] If the bucket length array is cleared and sorted is completed. Execute [Divide function] if it is not cleared.\\n[Divide function] Execute Divide by pop a bucket length from the end of the bucket length array. Find the maximum and minimum values in the bucket. If the maximum value is equal to the minimum value, the sorting is completed to stop Divide.\\nSet up a two-dimensional array as all empty buckets. Divide into the bucket according to the interpolation number.\\nAfter dividing into the buckets, push the length of the buckets into the array of bucket length. And put the items back into the original array one by one from all the buckets that are not empty.\\nReturn to [Main Sort].\\n\\nHistogram sort algorithm\\nThe NIST definition: An efficient 3-pass refinement of a bucket sort algorithm.\\n\\nThe first pass counts the number of items for each bucket in an auxiliary array, and then makes a running total so each auxiliary entry is the number of preceding items.\\nThe second pass puts each item in its proper bucket according to the auxiliary entry for the key of that item.\\nThe last pass sorts each bucket.\\n\\nPractice\\nInterpolation sort implementation\\nJavaScript code:\\n\\nInterpolation sort recursive method\\nWorst-case space complexity : \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n\\nHistogram sort implementation\\nVariant\\nInterpolation tag sort\\nInterpolation Tag Sort is a variant of Interpolation Sort. Applying the bucket sorting and dividing method, the array data is distributed into a limited number of buckets by mathematical interpolation formula, and the bucket then recursively the original processing program until the sorting is completed.\\nInterpolation tag sort is a recursive sorting method for interpolation sorting. To avoid stacking overflow caused by recursion, the memory crashes. Instead, use a Boolean data type tag array to operate the recursive function to release the memory. The extra memory space required is close to \\n  \\n    \\n      \\n        2\\n        n\\n        +\\n        (\\n        n\\n        )\\n        b\\n        i\\n        t\\n        s\\n      \\n    \\n    {\\\\displaystyle 2n+(n)bits}\\n  . Contains a two-dimensional array of dynamically allocated memory and a Boolean data type tag array. Stack, queue, associative array, and tree structure can be implemented as buckets.\\nAs the JavaScript array object is suitable for this sorting method, the difference in data structure is related to the speed of data access and thus the time required for sorting. The linear time \\u0398(n) is used when the values in the array to be sorted are evenly distributed. The bucket sort algorithm does not limit the sorting to the lower limit of \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        l\\n        o\\n        g\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nlogn)}\\n  . Interpolation tag sort average performance complexity is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        +\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n+k)}\\n  .\\n\\nInterpolation tag sort algorithm\\nSet a tag array equal to the original array size and initialize to a false value.\\n[Main Sort] Determines whether all buckets of the original array have been sorted. If the sorting is not completed, the [Divide function] is executed.\\n[Divide function] Find the maximum and minimum values in the bucket. If the maximum value is equal to the minimum value, the sorting is completed and the division is stopped.\\nSet up a two-dimensional array as all the empty buckets. Divide into the bucket according to the interpolation number.\\nAfter dividing into the bucket, mark the starting position of the bucket as a true value in the tag array. And put the items back into the original array one by one from all the buckets that are not empty.\\nReturn to [Main Sort].\\n\\nPractice\\nJavaScript code:\\n\\nIn-place Interpolation Tag Sort\\nThe in-place interpolation tag sort is an in-place algorithm of interpolation sort. In-place Interpolation Tag Sort can achieve sorting by only N times of swapping by maintaining N bit tags; however, the array to be sorted must be a continuous integer sequence and not repeated, or the series is completely evenly distributed to approximate The number of arithmetical progression.\\nThe factor column data must not be repeated. For example, sorting 0~100 can be sorted in one step. The number of exchanges is: \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  , the calculation time complexity is: \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  , and the worst space complexity is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n        b\\n        i\\n        t\\n        s\\n      \\n    \\n    {\\\\displaystyle O(n)bits}\\n  . If the characteristics of the series meet the conditional requirements of this sorting method: \\\"The array is a continuous integer or an arithmetical progression that does not repeat\\\", the in-place interpolation tag sort will be an excellent sorting method that is extremely fast and saves memory space.\\n\\nIn-place Interpolation Tag Sort Algorithm\\nIn-place Interpolation Tag Sort sorts non-repeating consecutive integer series, only one Boolean data type tag array with the same length as the original array, the array calculates the interpolation of the data from the beginning, and the interpolation points to a new position of the array. Position, the position that has been swapped is marked as true in the corresponding position of the tag array, and is incremented until the end of the array is sorted.\\nAlgorithm process:\\n\\nSet an equal number of tag arrays to initialize to false values.\\nVisit the array when tag[i] is false, calculate the position corresponding to the interpolation=p.\\nSwap a[i] and a[p], let tag[p] = true.\\nThe tour array is completed and the sorting is completed.\\n\\nPractice\\nJavaScript code:\\n\\nThe origin of In-place sorting performed in\\nO\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time\\nIn \\\"Mathematical Analysis of Algorithms\\\", (Information Processing '71, North Holland Publ.'72) Donald Knuth remarked \\\"... that research on computional complexity is an interesting way to sharpen our tools for more routine problems we face from day to day.\\\" \\nThe famous American computer scientist Donald Knuth in the mathematical analysis of algorithms pointed out that:\\\"With respect to the sorting problem, Knuth points out, that time effective in-situ permutation is inherently connected with the problem of finding the cycle leaders, and in-situ permutations could easily be performed in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time if we would be allowed to manipulate n extra \\\"tag\\\" bits specifying how much of the permutation has been carried out at any time. Without such tag bits, he concludes \\\"it seems reasonable to conjecture that every algorithm will require for in-situ permutation at least \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        l\\n        o\\n        g\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nlogn)}\\n   steps on the average.\\\" \\nThe In-place Interpolation Tag Sort is one of the sorting algorithms that the Donald Knuth professor said: \\\"manipulate n extra \\\"tag\\\" bits...finding the cycle leaders, and in-situ permutations could easily be performed in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time\\\".\\n\\nSimilar sorting method\\nFlashsort\\nProxmap sort\\nAmerican flag sort\\n\\nBucket sort mixing other sorting methods and recursive algorithm\\nBucket sort can be mixed with other sorting methods to complete sorting. If it is sorted by bucket sort and insert sort, also is a fairly efficient sorting method. But when the series appears a large deviation from the value: For example, when the maximum value of the series is greater than N times the next largest value. After the series of columns are processed, the distribution is that all the elements except the maximum value fall into the same bucket. The second sorting method uses insert sort. May cause execution complexity to fall into \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  . This has lost the meaning and high-speed performance of using bucket sort.\\nInterpolation sort is a way of recursively using bucket sort. After performing recursion, still use bucket sort to disperse the series. This can avoid the above situation. If you want to make the recursive interpolation sort execution complexity fall into \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  , it is necessary to present a factorial amplification in the entire series. In fact, there is very little chance that a series of special distributions will occur.\\n\\nReferences\\nExternal links\\ninterpolationSort.html\\nhistogramSort.html\\nThe FlashSort Algorithm\\nMathematical Analysis of Algorithms\\nhttp://www.drdobbs.com/database/the-flashsort1-algorithm/184410496\\n\\u6876\\u6392\\u5e8f\\u905e\\u8ff4\\u65b9\\u5f0f\\u6f14\\u7b97\\u6cd5 Bucket sort Recursive method. Whale Chen 2012/09/16\\n\\u63d2\\u503c\\u6a19\\u7c3d\\u6392\\u5e8f\\u6f14\\u7b97\\u6cd5 Interpolation Tag Sort Algorithm. Whale Chen 2013/03/24\\ninterpolation sort (Pascal version available)\\nw3schools JavaScript Array Sort testing platform\"}, {\"Library sort\": \"Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions. The name comes from an analogy:\\n\\nSuppose a librarian were to store their books alphabetically on a long shelf, starting with the As at the left end, and continuing to the right along the shelf with no spaces between the books until the end of the Zs. If the librarian acquired a new book that belongs to the B section, once they find the correct space in the B section, they will have to move every book over, from the middle of the Bs all the way down to the Zs in order to make room for the new book. This is an insertion sort. However, if they were to leave a space after every letter, as long as there was still space after B, they would only have to move a few books to make room for the new one. This is the basic principle of the Library Sort.\\nThe algorithm was proposed by Michael A. Bender, Mart\\u00edn Farach-Colton, and Miguel Mosteiro in 2004 and was published in 2006.Like the insertion sort it is based on, library sort is a comparison sort; however, it was shown to have a high probability of running in O(n log n) time (comparable to quicksort), rather than an insertion sort's O(n2). There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.\\nCompared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. The amount and distribution of that space would be implementation dependent. In the paper the size of the needed array is (1 + \\u03b5)n, but with no further recommendations on how to choose \\u03b5. Moreover, it is neither adaptive nor stable. In order to warrant the with-high-probability time bounds, it requires to randomly permute the input, what changes the relative order of equal elements and shuffles any presorted input. Also, the algorithm uses binary search to find the insertion point for each element, which does not take profit of presorted input.\\nAnother drawback is that it cannot be run as an online algorithm, because it is not possible to randomly shuffle the input. If used without this shuffling, it could easily degenerate into quadratic behaviour.\\nOne weakness of insertion sort is that it may require a high number of swap operations and be costly if memory write is expensive. Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, but is also adding an extra cost in the rebalancing step. In addition, locality of reference will be poor compared to mergesort as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets.\\n\\nImplementation\\nAlgorithm\\nLet us say we have an array of n elements. We choose the gap we intend to give. Then we would have a final array of size (1 + \\u03b5)n. The algorithm works in log n rounds. In each round we insert as many elements as there are in the final array already, before re-balancing the array. For finding the position of inserting, we apply Binary Search in the final array and then swap the following elements till we hit an empty space. Once the round is over, we re-balance the final array by inserting spaces between each element.\\nFollowing are three important steps of the algorithm:\\n\\nBinary Search: Finding the position of insertion by applying binary search within the already inserted elements. This can be done by linearly moving towards left or right side of the array if you hit an empty space in the middle element.\\nInsertion: Inserting the element in the position found and swapping the following elements by 1 position till an empty space is hit. This is done in logarithmic time, with high probability.\\nRe-Balancing: Inserting spaces between each pair of elements in the array. The cost of rebalancing is linear in the number of elements already inserted. As these lengths increase with the powers of 2 for each round, the total cost of rebalancing is also linear.\\n\\nPseudocode\\nprocedure rebalance(A, begin, end) is\\n    r \\u2190 end\\n    w \\u2190 end \\u00f7 2\\n\\n    while r \\u2265 begin do\\n        A[w+1] \\u2190 gap\\n        A[w] \\u2190 A[r]\\n        r \\u2190 r \\u2212 1\\n        w \\u2190 w \\u2212 2\\n\\nprocedure sort(A) is\\n    n \\u2190 length(A)\\n    S \\u2190 new array of n gaps\\n\\n    for i \\u2190 1 to floor(log2(n) + 1) do\\n        for j \\u2190 2^i to 2^(i + 1) do\\n            ins \\u2190 binarysearch(A[j], S, 2^(i \\u2212 1))\\n            insert A[j] at S[ins]\\n\\nHere, binarysearch(el, A, k) performs binary search in the first k elements of A, skipping over gaps, to find a place where to locate element el. Insertion should favor gaps over filled-in elements.\\n\\nReferences\\nExternal links\\nGapped Insertion Sort\"}, {\"Merge sort\": \"In computer science, merge sort (also commonly spelled as mergesort) is an efficient, general-purpose, and comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. Merge sort is a divide-and-conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up merge sort appeared in a report by Goldstine and von Neumann as early as 1948.\\n\\nAlgorithm\\nConceptually, a merge sort works as follows:\\n\\nDivide the unsorted list into n sublists, each containing one element (a list of one element is considered sorted).\\nRepeatedly merge sublists to produce new sorted sublists until there is only one sublist remaining.  This will be the sorted list.\\n\\nTop-down implementation\\nExample C-like code using indices for top-down merge sort algorithm that recursively splits the list (called runs in this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step is avoided with alternating the direction of the merge with each level of recursion (except for an initial one-time copy, that can be avoided too). To help understand this, consider an array with two elements. The elements are copied to B[], then merged back to A[]. If there are four elements, when the bottom of the recursion level is reached, single element runs from A[] are merged to B[], and then at the next higher level of recursion, those two-element runs are merged to A[]. This pattern continues with each level of recursion.\\n\\nSorting the entire array is accomplished by TopDownMergeSort(A, B, length(A)).\\n\\nBottom-up implementation\\nExample C-like code using indices for bottom-up merge sort algorithm which treats the list as an array of n sublists (called runs in this example) of size 1, and iteratively merges sub-lists back and forth between two buffers:\\n\\nTop-down implementation using lists\\nPseudocode for top-down merge sort algorithm which recursively divides the input list into smaller sublists until the sublists are trivially sorted, and then merges the sublists while returning up the call chain.\\n\\nfunction merge_sort(list m) is\\n    // Base case. A list of zero or one elements is sorted, by definition.\\n    if length of m \\u2264 1 then\\n        return m\\n\\n    // Recursive case. First, divide the list into equal-sized sublists\\n    // consisting of the first half and second half of the list.\\n    // This assumes lists start at index 0.\\n    var left := empty list\\n    var right := empty list\\n    for each x with index i in m do\\n        if i < (length of m)/2 then\\n            add x to left\\n        else\\n            add x to right\\n\\n    // Recursively sort both sublists.\\n    left := merge_sort(left)\\n    right := merge_sort(right)\\n\\n    // Then merge the now-sorted sublists.\\n    return merge(left, right)\\n\\nIn this example, the merge function merges the left and right sublists.\\n\\nfunction merge(left, right) is\\n    var result := empty list\\n\\n    while left is not empty and right is not empty do\\n        if first(left) \\u2264 first(right) then\\n            append first(left) to result\\n            left := rest(left)\\n        else\\n            append first(right) to result\\n            right := rest(right)\\n\\n    // Either left or right may have elements left; consume them.\\n    // (Only one of the following loops will actually be entered.)\\n    while left is not empty do\\n        append first(left) to result\\n        left := rest(left)\\n    while right is not empty do\\n        append first(right) to result\\n        right := rest(right)\\n    return result\\n\\nBottom-up implementation using lists\\nPseudocode for bottom-up merge sort algorithm which uses a small fixed size array of references to nodes, where array[i] is either a reference to a list of size 2i or nil. node is a reference or pointer to a node. The merge() function would be similar to the one shown in the top-down merge lists example, it merges two already sorted lists, and handles empty lists. In this case, merge() would use node for its input parameters and return value.\\n\\nfunction merge_sort(node head) is\\n    // return if empty list\\n    if head = nil then\\n        return nil\\n    var node array[32]; initially all nil\\n    var node result\\n    var node next\\n    var int  i\\n    result := head\\n    // merge nodes into array\\n    while result \\u2260 nil do\\n        next := result.next;\\n        result.next := nil\\n        for (i = 0; (i < 32) && (array[i] \\u2260 nil); i += 1) do\\n            result := merge(array[i], result)\\n            array[i] := nil\\n        // do not go past end of array\\n        if i = 32 then\\n            i -= 1\\n        array[i] := result\\n        result := next\\n    // merge array into single list\\n    result := nil\\n    for (i = 0; i < 32; i += 1) do\\n        result := merge(array[i], result)\\n    return result\\n\\nAnalysis\\nIn sorting n objects, merge sort has an average and worst-case performance of O(n log n). If the running time of merge sort for a list of length n is T(n), then the recurrence relation T(n) = 2T(n/2) + n follows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the n steps taken to merge the resulting two lists). The closed form follows from the master theorem for divide-and-conquer recurrences.\\nThe number of comparisons made by merge sort in the worst case is given by the sorting numbers. These numbers are equal to or slightly smaller than (n \\u2308lg n\\u2309 \\u2212 2\\u2308lg n\\u2309 + 1), which is between (n lg n \\u2212 n + 1) and (n lg n + n + O(lg n)). Merge sort's best case takes about half as many iterations as its worst case.For large n and a randomly ordered input list, merge sort's expected (average) number of comparisons approaches \\u03b1\\u00b7n fewer than the worst case, where \\n  \\n    \\n      \\n        \\u03b1\\n        =\\n        \\u2212\\n        1\\n        +\\n        \\n          \\u2211\\n          \\n            k\\n            =\\n            0\\n          \\n          \\n            \\u221e\\n          \\n        \\n        \\n          \\n            1\\n            \\n              \\n                2\\n                \\n                  k\\n                \\n              \\n              +\\n              1\\n            \\n          \\n        \\n        \\u2248\\n        0.2645.\\n      \\n    \\n    {\\\\displaystyle \\\\alpha =-1+\\\\sum _{k=0}^{\\\\infty }{\\\\frac {1}{2^{k}+1}}\\\\approx 0.2645.}\\n  \\nIn the worst case, merge sort uses approximately 39% fewer comparisons than quicksort does in its average case, and in terms of moves, merge sort's worst case complexity is O(n log n) - the same complexity as quicksort's best case.Merge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially, and is thus popular in languages such as Lisp, where sequentially accessed data structures are very common. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort.\\nMerge sort's most common implementation does not sort in place; therefore, the memory size of the input must be allocated for the sorted output to be stored in (see below for variations that need only n/2 extra spaces).\\n\\nNatural merge sort\\nA natural merge sort is similar to a bottom-up merge sort except that any naturally occurring runs (sorted sequences) in the input are exploited. Both monotonic and bitonic (alternating up/down) runs may be exploited, with lists (or equivalently tapes or files) being convenient data structures (used as FIFO queues or LIFO stacks). In the bottom-up merge sort, the starting point assumes each run is one item long. In practice, random input data will have many short runs that just happen to be sorted. In the typical case, the natural merge sort may not need as many passes because there are fewer runs to merge. In the best case, the input is already sorted (i.e., is one run), so the natural merge sort need only make one pass through the data. In many practical cases, long natural runs are present, and for that reason natural merge sort is exploited as the key component of Timsort. Example:\\n\\nStart       :  3  4  2  1  7  5  8  9  0  6\\nSelect runs : (3  4)(2)(1  7)(5  8  9)(0  6)\\nMerge       : (2  3  4)(1  5  7  8  9)(0  6)\\nMerge       : (1  2  3  4  5  7  8  9)(0  6)\\nMerge       : (0  1  2  3  4  5  6  7  8  9)\\n\\nFormally, the natural merge sort is said to be Runs-optimal, where \\n  \\n    \\n      \\n        \\n          \\n            R\\n            u\\n            n\\n            s\\n          \\n        \\n        (\\n        L\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathtt {Runs}}(L)}\\n   is the number of runs in \\n  \\n    \\n      \\n        L\\n      \\n    \\n    {\\\\displaystyle L}\\n  , minus one.\\nTournament replacement selection sorts are used to gather the initial runs for external sorting algorithms.\\n\\nPing-pong merge sort\\nInstead of merging two blocks at a time, a ping-pong merge merges four blocks at a time. The four sorted blocks are merged simultaneously to auxiliary space into two sorted blocks, then the two sorted blocks are merged back to main memory. Doing so omits the copy operation and reduces the total number of moves by half. An early public domain implementation of a four-at-once merge was by WikiSort in 2014, the method was later that year described as an optimization for patience sorting and named a ping-pong merge. Quadsort implemented the method in 2020 and named it a quad merge.\\n\\nIn-place merge sort\\nOne drawback of merge sort, when implemented on arrays, is its O(n) working memory requirement. Several methods to reduce memory or make merge sort fully in-place have been suggested:\\n\\nKronrod (1969) suggested an alternative version of merge sort that uses constant additional space.\\nKatajainen et al. present an algorithm that requires a constant amount of working memory: enough storage space to hold one element of the input array, and additional space to hold O(1) pointers into the input array. They achieve an O(n log n) time bound with small constants, but their algorithm is not stable.\\nSeveral attempts have been made at producing an in-place merge algorithm that can be combined with a standard (top-down or bottom-up) merge sort to produce an in-place merge sort. In this case, the notion of \\\"in-place\\\" can be relaxed to mean \\\"taking logarithmic stack space\\\", because standard merge sort requires that amount of space for its own stack usage. It was shown by Geffert et al. that in-place, stable merging is possible in O(n log n) time using a constant amount of scratch space, but their algorithm is complicated and has high constant factors: merging arrays of length n and m can take 5n + 12m + o(m) moves. This high constant factor and complicated in-place algorithm was made simpler and easier to understand. Bing-Chao Huang and Michael A. Langston presented a straightforward linear time algorithm practical in-place merge to merge a sorted list using fixed amount of additional space. They both have used the work of Kronrod and others. It merges in linear time and constant extra space. The algorithm takes little more average time than standard merge sort algorithms, free to exploit O(n) temporary extra memory cells, by less than a factor of two. Though the algorithm is much faster in a practical way but it is unstable also for some lists. But using similar concepts, they have been able to solve this problem. Other in-place algorithms include SymMerge, which takes O((n + m) log (n + m)) time in total and is stable. Plugging such an algorithm into merge sort increases its complexity to the non-linearithmic, but still quasilinear, O(n (log n)2).\\nMany applications of external sorting use a form of merge sorting where the input get split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of pages fit into main memory.\\nA modern stable linear and in-place merge variant is block merge sort which creates a section of unique values to use as swap space.\\nThe space overhead can be reduced to sqrt(n) by using binary searches and rotations. This method is employed by the C++ STL library and quadsort.\\nAn alternative to reduce the copying into multiple lists is to associate a new field of information with each key (the elements in m are called keys). This field will be used to link the keys and any associated information together in a sorted list (a key and its related information is called a record). Then the merging of the sorted lists proceeds by changing the link values; no records need to be moved at all. A field which contains only a link will generally be smaller than an entire record so less space will also be used. This is a standard sorting technique, not restricted to merge sort.\\nA simple way to reduce the space overhead to n/2 is to maintain left and right as a combined structure, copy only the left part of m into temporary space, and to direct the merge routine to place the merged output into m. With this version it is better to allocate the temporary space outside the merge routine, so that only one allocation is needed. The excessive copying mentioned previously is also mitigated, since the last pair of lines before the return result statement (function  merge in the pseudo code above) become superfluous.\\n\\nUse with tape drives\\nAn external merge sort is practical to run using disk or tape drives when the data to be sorted is too large to fit into memory. External sorting explains how merge sort is implemented with disk drives. A typical tape drive sort uses four tape drives. All I/O is sequential (except for rewinds at the end of each pass). A minimal implementation can get by with just two record buffers and a few program variables.\\nNaming the four tape drives as A, B, C, D, with the original data on A, and using only two record buffers, the algorithm is similar to the bottom-up implementation, using pairs of tape drives instead of arrays in memory. The basic algorithm can be described as follows:\\n\\nMerge pairs of records from A; writing two-record sublists alternately to C and D.\\nMerge two-record sublists from C and D into four-record sublists; writing these alternately to A and B.\\nMerge four-record sublists from A and B into eight-record sublists; writing these alternately to C and D\\nRepeat until you have one list containing all the data, sorted\\u2014in log2(n) passes.Instead of starting with very short runs, usually a hybrid algorithm is used, where the initial pass will read many records into memory, do an internal sort to create a long run, and then distribute those long runs onto the output set. The step avoids many early passes. For example, an internal sort of 1024 records will save nine passes. The internal sort is often large because it has such a benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory. One of them, the Knuth's 'snowplow' (based on a binary min-heap), generates runs twice as long (on average) as a size of memory used.With some overhead, the above algorithm can be modified to use three tapes. O(n log n) running time can also be achieved using two queues, or a stack and a queue, or three stacks. In the other direction, using k > two tapes (and O(k) items in memory), we can reduce the number of tape operations in O(log k) times by using a k/2-way merge.\\nA more sophisticated merge sort that optimizes tape (and disk) drive usage is the polyphase merge sort.\\n\\nOptimizing merge sort\\nOn modern computers, locality of reference can be of paramount importance in software optimization, because multilevel memory hierarchies are used. Cache-aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the tiled merge sort algorithm stops partitioning subarrays when subarrays of size S are reached, where S is the number of data items fitting into a CPU's cache. Each of these subarrays is sorted with an in-place sorting algorithm such as insertion sort, to discourage memory swaps, and normal merge sort is then completed in the standard recursive fashion. This algorithm has demonstrated better performance on machines that benefit from cache optimization. (LaMarca & Ladner 1997)\\n\\nParallel merge Sort\\nMerge sort parallelizes well due to the use of the divide-and-conquer method. Several different parallel variants of the algorithm have been developed over the years. Some parallel merge sort algorithms are strongly related to the sequential top-down merge algorithm while others have a different general structure and use the K-way merge method.\\n\\nMerge sort with parallel recursion\\nThe sequential merge sort procedure can be described in two phases, the divide phase and the merge phase. The first consists of many recursive calls that repeatedly perform the same division process until the subsequences are trivially sorted (containing one or no element). An intuitive approach is the parallelization of those recursive calls. Following pseudocode describes the merge sort with parallel recursion using the fork and join keywords:\\n\\n// Sort elements lo through hi (exclusive) of array A.\\nalgorithm mergesort(A, lo, hi) is\\n    if lo+1 < hi then  // Two or more elements.\\n        mid := \\u230a(lo + hi) / 2\\u230b\\n        fork mergesort(A, lo, mid)\\n        mergesort(A, mid, hi)\\n        join\\n        merge(A, lo, mid, hi)\\n\\nThis algorithm is the trivial modification of the sequential version and does not parallelize well. Therefore, its speedup is not very impressive. It has a span of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n)}\\n  , which is only an improvement of \\n  \\n    \\n      \\n        \\u0398\\n        (\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (\\\\log n)}\\n   compared to the sequential version (see Introduction to Algorithms). This is mainly due to the sequential merge method, as it is the bottleneck of the parallel executions.\\n\\nMerge sort with parallel merging\\nBetter parallelism can be achieved by using a parallel merge algorithm. Cormen et al. present a binary variant that merges two sorted sub-sequences into one sorted output sequence.In one of the sequences (the longer one if unequal length), the element of the middle index is selected. Its position in the other sequence is determined in such a way that this sequence would remain sorted if this element were inserted at this position. Thus, one knows how many other elements from both sequences are smaller and the position of the selected element in the output sequence can be calculated. For the partial sequences of the smaller and larger elements created in this way, the merge algorithm is again executed in parallel until the base case of the recursion is reached.\\nThe following pseudocode shows the modified parallel merge sort method using the parallel merge algorithm (adopted from Cormen et al.).\\n\\n/**\\n * A: Input array\\n * B: Output array\\n * lo: lower bound\\n * hi: upper bound\\n * off: offset\\n */\\nalgorithm parallelMergesort(A, lo, hi, B, off) is\\n    len := hi - lo + 1\\n    if len == 1 then\\n        B[off] := A[lo]\\n    else let T[1..len] be a new array\\n        mid := \\u230a(lo + hi) / 2\\u230b \\n        mid' := mid - lo + 1\\n        fork parallelMergesort(A, lo, mid, T, 1)\\n        parallelMergesort(A, mid + 1, hi, T, mid' + 1) \\n        join \\n        parallelMerge(T, 1, mid', mid' + 1, len, B, off)\\n\\nIn order to analyze a recurrence relation for the worst case span, the recursive calls of parallelMergesort have to be incorporated only once due to their parallel execution, obtaining\\n\\nFor detailed information about the complexity of the parallel merge procedure, see Merge algorithm.\\nThe solution of this recurrence is given by\\n\\nThis parallel merge algorithm reaches a parallelism of \\n  \\n    \\n      \\n        \\u0398\\n        \\n          (\\n          \\n            \\n              n\\n              \\n                (\\n                log\\n                \\u2061\\n                n\\n                \\n                  )\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\textstyle \\\\Theta \\\\left({\\\\frac {n}{(\\\\log n)^{2}}}\\\\right)}\\n  , which is much higher than the parallelism of the previous algorithm. Such a sort can perform well in practice when combined with a fast stable sequential sort, such as insertion sort, and a fast sequential merge as a base case for merging small arrays.\\n\\nParallel multiway merge sort\\nIt seems arbitrary to restrict the merge sort algorithms to a binary merge method, since there are usually p > 2 processors available. A better approach may be to use a K-way merge method, a generalization of binary merge, in which \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   sorted sequences are merged. This merge variant is well suited to describe a sorting algorithm on a PRAM.\\n\\nBasic Idea\\nGiven an unsorted sequence of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   elements, the goal is to sort the sequence with \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   available processors. These elements are distributed equally among all processors and sorted locally using a sequential Sorting algorithm. Hence, the sequence consists of sorted sequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{1},...,S_{p}}\\n   of length \\n  \\n    \\n      \\n        \\u2308\\n        \\n          \\n            n\\n            p\\n          \\n        \\n        \\u2309\\n      \\n    \\n    {\\\\textstyle \\\\lceil {\\\\frac {n}{p}}\\\\rceil }\\n  . For simplification let \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   be a multiple of \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  , so that \\n  \\n    \\n      \\n        \\n          |\\n          \\n            S\\n            \\n              i\\n            \\n          \\n          |\\n        \\n        =\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle \\\\left\\\\vert S_{i}\\\\right\\\\vert ={\\\\frac {n}{p}}}\\n   for \\n  \\n    \\n      \\n        i\\n        =\\n        1\\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        p\\n      \\n    \\n    {\\\\displaystyle i=1,...,p}\\n  .\\nThese sequences will be used to perform a multisequence selection/splitter selection. For \\n  \\n    \\n      \\n        j\\n        =\\n        1\\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        p\\n      \\n    \\n    {\\\\displaystyle j=1,...,p}\\n  , the algorithm determines splitter elements \\n  \\n    \\n      \\n        \\n          v\\n          \\n            j\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{j}}\\n   with global rank \\n  \\n    \\n      \\n        k\\n        =\\n        j\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle k=j{\\\\frac {n}{p}}}\\n  . Then the corresponding positions of \\n  \\n    \\n      \\n        \\n          v\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          v\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{1},...,v_{p}}\\n   in each sequence \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   are determined with binary search and thus the \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   are further partitioned into \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   subsequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n            ,\\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            i\\n            ,\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i,1},...,S_{i,p}}\\n   with \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n            ,\\n            j\\n          \\n        \\n        :=\\n        {\\n        x\\n        \\u2208\\n        \\n          S\\n          \\n            i\\n          \\n        \\n        \\n          |\\n        \\n        r\\n        a\\n        n\\n        k\\n        (\\n        \\n          v\\n          \\n            j\\n            \\u2212\\n            1\\n          \\n        \\n        )\\n        <\\n        r\\n        a\\n        n\\n        k\\n        (\\n        x\\n        )\\n        \\u2264\\n        r\\n        a\\n        n\\n        k\\n        (\\n        \\n          v\\n          \\n            j\\n          \\n        \\n        )\\n        }\\n      \\n    \\n    {\\\\textstyle S_{i,j}:=\\\\{x\\\\in S_{i}|rank(v_{j-1})<rank(x)\\\\leq rank(v_{j})\\\\}}\\n  .\\nFurthermore, the elements of \\n  \\n    \\n      \\n        \\n          S\\n          \\n            1\\n            ,\\n            i\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            p\\n            ,\\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{1,i},...,S_{p,i}}\\n   are assigned to processor \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  , means all elements between rank \\n  \\n    \\n      \\n        (\\n        i\\n        \\u2212\\n        1\\n        )\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle (i-1){\\\\frac {n}{p}}}\\n   and rank \\n  \\n    \\n      \\n        i\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle i{\\\\frac {n}{p}}}\\n  , which are distributed over all \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n  . Thus, each processor receives a sequence of sorted sequences. The fact that the rank \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   of the splitter elements \\n  \\n    \\n      \\n        \\n          v\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle v_{i}}\\n   was chosen globally, provides two important properties: On the one hand, \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   was chosen so that each processor can still operate on \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\textstyle n/p}\\n   elements after assignment. The algorithm is perfectly load-balanced. On the other hand, all elements on processor \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   are less than or equal to all elements on processor \\n  \\n    \\n      \\n        i\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle i+1}\\n  . Hence, each processor performs the p-way merge locally and thus obtains a sorted sequence from its sub-sequences. Because of the second property, no further p-way-merge has to be performed, the results only have to be put together in the order of the processor number.\\n\\nMulti-sequence selection\\nIn its simplest form, given \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   sorted sequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          S\\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{1},...,S_{p}}\\n   distributed evenly on \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   processors and a rank \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  , the task is to find an element \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   with a global rank \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   in the union of the sequences. Hence, this can be used to divide each \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   in two parts at a splitter index \\n  \\n    \\n      \\n        \\n          l\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle l_{i}}\\n  , where the lower part contains only elements which are smaller than \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  , while the elements bigger than \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   are located in the upper part.\\nThe presented sequential algorithm returns the indices of the splits in each sequence, e.g. the indices \\n  \\n    \\n      \\n        \\n          l\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle l_{i}}\\n   in sequences \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle S_{i}}\\n   such that \\n  \\n    \\n      \\n        \\n          S\\n          \\n            i\\n          \\n        \\n        [\\n        \\n          l\\n          \\n            i\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle S_{i}[l_{i}]}\\n   has a global rank less than \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   and \\n  \\n    \\n      \\n        \\n          r\\n          a\\n          n\\n          k\\n        \\n        \\n          (\\n          \\n            \\n              S\\n              \\n                i\\n              \\n            \\n            [\\n            \\n              l\\n              \\n                i\\n              \\n            \\n            +\\n            1\\n            ]\\n          \\n          )\\n        \\n        \\u2265\\n        k\\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {rank} \\\\left(S_{i}[l_{i}+1]\\\\right)\\\\geq k}\\n  .\\nalgorithm msSelect(S : Array of sorted Sequences [S_1,..,S_p], k : int) is\\n    for i = 1 to p do \\n\\t(l_i, r_i) = (0, |S_i|-1)\\n\\t\\n    while there exists i: l_i < r_i do\\n\\t// pick Pivot Element in S_j[l_j], .., S_j[r_j], chose random j uniformly\\n\\tv := pickPivot(S, l, r)\\n\\tfor i = 1 to p do \\n\\t    m_i = binarySearch(v, S_i[l_i, r_i]) // sequentially\\n\\tif m_1 + ... + m_p >= k then // m_1+ ... + m_p is the global rank of v\\n\\t    r := m  // vector assignment\\n\\telse\\n\\t    l := m \\n\\t\\n    return l\\n\\nFor the complexity analysis the PRAM model is chosen. If the data is evenly distributed over all \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  , the p-fold execution of the binarySearch method has a running time of \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                n\\n                \\n                  /\\n                \\n                p\\n              \\n              )\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\log \\\\left(n/p\\\\right)\\\\right)}\\n  . The expected recursion depth is \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                \\n                  \\u2211\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n                \\n                  S\\n                  \\n                    i\\n                  \\n                \\n                \\n                  |\\n                \\n              \\n              )\\n            \\n          \\n          )\\n        \\n        =\\n        \\n          \\n            O\\n          \\n        \\n        (\\n        log\\n        \\u2061\\n        (\\n        n\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(\\\\log \\\\left(\\\\textstyle \\\\sum _{i}|S_{i}|\\\\right)\\\\right)={\\\\mathcal {O}}(\\\\log(n))}\\n   as in the ordinary Quickselect. Thus the overall expected running time is \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\log(n/p)\\\\log(n)\\\\right)}\\n  .\\nApplied on the parallel multiway merge sort, this algorithm has to be invoked in parallel such that all splitter elements of rank \\n  \\n    \\n      \\n        i\\n        \\n          \\n            n\\n            p\\n          \\n        \\n      \\n    \\n    {\\\\textstyle i{\\\\frac {n}{p}}}\\n   for \\n  \\n    \\n      \\n        i\\n        =\\n        1\\n        ,\\n        .\\n        .\\n        ,\\n        p\\n      \\n    \\n    {\\\\displaystyle i=1,..,p}\\n   are found simultaneously. These splitter elements can then be used to partition each sequence in \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   parts, with the same total running time of \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\,\\\\log(n/p)\\\\log(n)\\\\right)}\\n  .\\n\\nPseudocode\\nBelow, the complete pseudocode of the parallel multiway merge sort algorithm is given. We assume that there is a barrier synchronization before and after the multisequence selection such that every processor can determine the splitting elements and the sequence partition properly.\\n\\n/**\\n * d: Unsorted Array of Elements\\n * n: Number of Elements\\n * p: Number of Processors\\n * return Sorted Array\\n */\\nalgorithm parallelMultiwayMergesort(d : Array, n : int, p : int) is\\n    o := new Array[0, n]                         // the output array\\n    for i = 1 to p do in parallel                // each processor in parallel\\n        S_i := d[(i-1) * n/p, i * n/p] \\t         // Sequence of length n/p\\n\\tsort(S_i)                                // sort locally\\n        synch\\n\\tv_i := msSelect([S_1,...,S_p], i * n/p)          // element with global rank i * n/p\\n        synch\\n\\t(S_i,1, ..., S_i,p) := sequence_partitioning(si, v_1, ..., v_p) // split s_i into subsequences\\n\\t    \\n\\to[(i-1) * n/p, i * n/p] := kWayMerge(s_1,i, ..., s_p,i)  // merge and assign to output array\\n\\t\\n    return o\\n\\nAnalysis\\nFirstly, each processor sorts the assigned \\n  \\n    \\n      \\n        n\\n        \\n          /\\n        \\n        p\\n      \\n    \\n    {\\\\displaystyle n/p}\\n   elements locally using a sorting algorithm with complexity \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            n\\n            \\n              /\\n            \\n            p\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(n/p\\\\;\\\\log(n/p)\\\\right)}\\n  . After that, the splitter elements have to be calculated in time \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            p\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            \\n              /\\n            \\n            p\\n            )\\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left(p\\\\,\\\\log(n/p)\\\\log(n)\\\\right)}\\n  . Finally, each group of \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   splits have to be merged in parallel by each processor with a running time of \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        (\\n        log\\n        \\u2061\\n        (\\n        p\\n        )\\n        \\n        n\\n        \\n          /\\n        \\n        p\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}(\\\\log(p)\\\\;n/p)}\\n   using a sequential p-way merge algorithm. Thus, the overall running time is given by\\n\\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                n\\n                p\\n              \\n            \\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                \\n                  n\\n                  p\\n                \\n              \\n              )\\n            \\n            +\\n            p\\n            log\\n            \\u2061\\n            \\n              (\\n              \\n                \\n                  n\\n                  p\\n                \\n              \\n              )\\n            \\n            log\\n            \\u2061\\n            (\\n            n\\n            )\\n            +\\n            \\n              \\n                n\\n                p\\n              \\n            \\n            log\\n            \\u2061\\n            (\\n            p\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}\\\\left({\\\\frac {n}{p}}\\\\log \\\\left({\\\\frac {n}{p}}\\\\right)+p\\\\log \\\\left({\\\\frac {n}{p}}\\\\right)\\\\log(n)+{\\\\frac {n}{p}}\\\\log(p)\\\\right)}\\n  .\\n\\nPractical adaption and application\\nThe multiway merge sort algorithm is very scalable through its high parallelization capability, which allows the use of many processors. This makes the algorithm a viable candidate for sorting large amounts of data, such as those processed in computer clusters. Also, since in such systems memory is usually not a limiting resource, the disadvantage of space complexity of merge sort is negligible. However, other factors become important in such systems, which are not taken into account when modelling on a PRAM. Here, the following aspects need to be considered: Memory hierarchy, when the data does not fit into the processors cache, or the communication overhead of exchanging data between processors, which could become a bottleneck when the data can no longer be accessed via the shared memory.\\nSanders et al. have presented in their paper a bulk synchronous parallel algorithm for multilevel multiway mergesort, which divides \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   processors into \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   groups of size \\n  \\n    \\n      \\n        \\n          p\\n          \\u2032\\n        \\n      \\n    \\n    {\\\\displaystyle p'}\\n  . All processors sort locally first. Unlike single level multiway mergesort, these sequences are then partitioned into \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   parts and assigned to the appropriate processor groups. These steps are repeated recursively in those groups. This reduces communication and especially avoids problems with many small messages. The hierarchical structure of the underlying real network can be used to define the processor groups (e.g. racks, clusters,...).\\n\\nFurther variants\\nMerge sort was one of the first sorting algorithms where optimal speed up was achieved, with Richard Cole using a clever subsampling algorithm to ensure O(1) merge. Other sophisticated parallel sorting algorithms can achieve the same or better time bounds with a lower constant. For example, in 1991 David Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW parallel random-access machine (PRAM) with n processors by performing partitioning implicitly. Powers further shows that a pipelined version of Batcher's Bitonic Mergesort at O((log n)2) time on a butterfly sorting network is in practice actually faster than his O(log n) sorts on a PRAM, and he provides detailed discussion of the hidden overheads in comparison, radix and parallel sorting.\\n\\nComparison with other sort algorithms\\nAlthough heapsort has the same time bounds as merge sort, it requires only \\u0398(1) auxiliary space instead of merge sort's \\u0398(n). On typical modern architectures, efficient quicksort implementations generally outperform merge sort for sorting RAM-based arrays. On the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a linked list: in this situation it is relatively easy to implement a merge sort in such a way that it requires only \\u0398(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.\\nAs of Perl 5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl). In Java, the Arrays.sort() methods use merge sort or a tuned quicksort depending on the datatypes and for implementation efficiency switch to insertion sort when fewer than seven array elements are being sorted. The Linux kernel uses merge sort for its linked lists. Python uses Timsort, another tuned hybrid of merge sort and insertion sort, that has become the standard sort algorithm in Java SE 7 (for arrays of non-primitive types), on the Android platform, and in GNU Octave.\\n\\nNotes\\nReferences\\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2009) [1990]. Introduction to Algorithms (3rd ed.). MIT Press and McGraw-Hill. ISBN 0-262-03384-4.\\nKatajainen, Jyrki; Pasanen, Tomi; Teuhola, Jukka (1996). \\\"Practical in-place mergesort\\\". Nordic Journal of Computing. 3 (1): 27\\u201340. CiteSeerX 10.1.1.22.8523. ISSN 1236-6064. Archived from the original on 2011-08-07. Retrieved 2009-04-04.. Also Practical In-Place Mergesort. Also [3]\\nKnuth, Donald (1998). \\\"Section 5.2.4: Sorting by Merging\\\". Sorting and Searching. The Art of Computer Programming. Vol. 3 (2nd ed.). Addison-Wesley. pp. 158\\u2013168. ISBN 0-201-89685-0.\\nKronrod, M. A. (1969). \\\"Optimal ordering algorithm without operational field\\\". Soviet Mathematics - Doklady. 10: 744.\\nLaMarca, A.; Ladner, R. E. (1997). \\\"The influence of caches on the performance of sorting\\\". Proc. 8th Ann. ACM-SIAM Symp. On Discrete Algorithms (SODA97): 370\\u2013379. CiteSeerX 10.1.1.31.1153.\\nSkiena, Steven S. (2008). \\\"4.5: Mergesort: Sorting by Divide-and-Conquer\\\". The Algorithm Design Manual (2nd ed.). Springer. pp. 120\\u2013125. ISBN 978-1-84800-069-8.\\nSun Microsystems. \\\"Arrays API (Java SE 6)\\\". Retrieved 2007-11-19.\\nOracle Corp. \\\"Arrays (Java SE 10 & JDK 10)\\\". Retrieved 2018-07-23.\\n\\nExternal links\\n\\nAnimated Sorting Algorithms: Merge Sort at the Wayback Machine (archived 6 March 2015) \\u2013 graphical demonstration\\nOpen Data Structures - Section 11.1.1 - Merge Sort, Pat Morin\"}, {\"Odd\\u2013even sort\": \"In computing, an odd\\u2013even sort or odd\\u2013even transposition sort (also known as brick sort or parity sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections.  It is a comparison sort related to bubble sort, with which it shares many characteristics.  It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched.  The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.\\n\\nSorting on processor arrays\\nOn parallel processors, with one value per processor and only local left\\u2013right neighbor connections, the processors all concurrently do a compare\\u2013exchange operation with their neighbors, alternating between odd\\u2013even and even\\u2013odd pairings.  This algorithm was originally presented, and shown to be efficient on such processors, by Habermann in 1972.The algorithm extends efficiently to the case of multiple items per processor.  In the Baudet\\u2013Stevenson odd\\u2013even merge-splitting algorithm, each processor sorts its own sublist at each step, using any efficient sort algorithm, and then performs a merge splitting, or transposition\\u2013merge, operation with its neighbor, with neighbor pairing alternating between odd\\u2013even and even\\u2013odd on each step.\\n\\nBatcher's odd\\u2013even mergesort\\nA related but more efficient sort algorithm is the Batcher odd\\u2013even mergesort, using compare\\u2013exchange operations and perfect-shuffle operations.\\nBatcher's method is efficient on parallel processors with long-range connections.\\n\\nAlgorithm\\nThe single-processor algorithm, like bubblesort, is simple but not very efficient. Here a zero-based index is assumed:\\n\\nProof of correctness\\nClaim:  Let \\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        ,\\n        \\n          a\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{1},...,a_{n}}\\n   be a sequence of data ordered by <.  The odd\\u2013even sort algorithm correctly sorts this data in \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   passes.  (A pass here is defined to be a full sequence of odd\\u2013even, or even\\u2013odd comparisons.  The passes occur in order pass 1: odd\\u2013even, pass 2: even\\u2013odd, etc.)\\nProof:\\nThis proof is based loosely on one by Thomas Worsch.Since the sorting algorithm only involves comparison-swap operations and is oblivious (the order of comparison-swap operations does not depend on the data), by Knuth's 0\\u20131 sorting principle, it suffices to check correctness when each \\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{i}}\\n   is either 0 or 1. Assume that there are \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n   1s.\\nObserve that the rightmost 1 can be either in an even or odd position, so it might not be moved by the first odd\\u2013even pass. But after the first odd\\u2013even pass, the rightmost 1 will be in an even position. It follows that it will be moved to the right by all remaining passes. Since the rightmost one starts in position greater than or equal to \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n  , it must be moved at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n      \\n    \\n    {\\\\displaystyle n-e}\\n   steps. It follows that it takes at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle n-e+1}\\n   passes to move the rightmost 1 to its correct position.\\nNow, consider the second rightmost 1. After two passes, the 1 to its right will have moved right by at least one step. It follows that, for all remaining passes, we can view the second rightmost 1 as the rightmost 1. The second rightmost 1 starts in position at least \\n  \\n    \\n      \\n        e\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle e-1}\\n   and must be moved to position at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n  , so it must be moved at most \\n  \\n    \\n      \\n        (\\n        n\\n        \\u2212\\n        1\\n        )\\n        \\u2212\\n        (\\n        e\\n        \\u2212\\n        1\\n        )\\n        =\\n        n\\n        \\u2212\\n        e\\n      \\n    \\n    {\\\\displaystyle (n-1)-(e-1)=n-e}\\n   steps. After at most 2 passes, the rightmost 1 will have already moved, so the entry to the right of the second rightmost 1 will be 0.  Hence, for all passes after the first two, the second rightmost 1 will move to the right. It thus takes at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        2\\n      \\n    \\n    {\\\\displaystyle n-e+2}\\n   passes to move the second rightmost 1 to its correct position.\\nContinuing in this manner, by induction it can be shown that the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th rightmost 1 is moved to its correct position in at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        i\\n      \\n    \\n    {\\\\displaystyle n-e+i}\\n   passes. Since \\n  \\n    \\n      \\n        i\\n        \\u2264\\n        e\\n      \\n    \\n    {\\\\displaystyle i\\\\leq e}\\n  , it follows that the \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n  -th rightmost 1 is moved to its correct position in at most \\n  \\n    \\n      \\n        n\\n        \\u2212\\n        e\\n        +\\n        e\\n        =\\n        n\\n      \\n    \\n    {\\\\displaystyle n-e+e=n}\\n   passes. The list is thus correctly sorted in \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   passes. QED.\\nWe remark that each pass takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   steps, so this algorithm has \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n   complexity.\\n\\n\\n== References ==\"}, {\"Oscillating merge sort\": \"Oscillating merge sort or oscillating sort is a variation of merge sort used with tape drives that can read backwards.  Instead of doing a complete distribution as is done in a tape merge, the distribution of the input and the merging of runs are interspersed.  The oscillating merge sort does not waste rewind time or have tape drives sit idle as in the conventional tape merge.\\nThe oscillating merge sort \\\"was designed for tapes that can be read backward and is more efficient generally than either the polyphase or cascade merges.\\\"\\n\\nReferences\\nBradley, James (1982), File and Data Base Techniques, Holt, Rinehart and Winston, ISBN 0-03-058673-9\\n\\nFurther reading\\nFlores, Ivan (1969), Computer Sorting, Prentice-Hall, ISBN 978-0-13165746-5\\nKnuth, D. E. (1975), Sorting and Searching, The Art of Computer Programming, vol. 3, Addison Wesley\\nLowden, B. G. T., \\\"A note on the oscillating sort\\\" (PDF), The Computer Journal, 20 (1): 92, doi:10.1093/comjnl/20.1.92\\nMartin, W. A. (1971), \\\"Sorting\\\", Computing Surveys, ACM, 3 (4): 147\\u2013174, doi:10.1145/356593.356594\\nSobel, Sheldon (July 1962), \\\"Oscillating Sort\\u2013A New Sort Merging Technique\\\", Journal of the ACM, New York, NY: ACM, 9 (3): 372\\u2013374, doi:10.1145/321127.321133, S2CID 11554742\\n\\nExternal links\\nMihaldinecz, Maximilian (2016), \\\"A variation of Oscillating Merge Sort implemented in Matlab\\\", GitHub\"}, {\"Pigeonhole sort\": \"Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number n of elements and the length N of the range of possible key values are approximately the same. It requires O(n + N) time.  It is similar to counting sort, but differs in that it \\\"moves items twice: once to the bucket array and again to the final destination [whereas] counting sort builds an auxiliary array then uses the array to compute each item's final destination and move the item there.\\\"The pigeonhole algorithm works as follows:\\n\\nGiven an array of values to be sorted, set up an auxiliary array of initially empty \\\"pigeonholes\\\", one pigeonhole for each key in the range of the keys in the original array.\\nGoing over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole eventually contains a list of all values with that key.\\nIterate over the pigeonhole array in increasing order of keys, and for each pigeonhole, put its elements into the original array in increasing order.\\n\\nSee also\\nPigeonhole principle\\nRadix sort\\nBucket queue, a related priority queue data structure\\n\\n\\n== References ==\"}, {\"Proxmap sort\": \"ProxmapSort, or Proxmap sort, is a sorting algorithm that works by partitioning an array of data items, or keys, into a number of \\\"subarrays\\\" (termed buckets, in similar sorts). The name is short for computing a \\\"proximity map,\\\" which indicates for each key K the beginning of a subarray where K will reside in the final sorted order. Keys are placed into each subarray using insertion sort. If keys are \\\"well distributed\\\" among the subarrays, sorting occurs in linear time. The computational complexity estimates involve the number of subarrays and the proximity mapping function, the \\\"map key,\\\" used. It is a form of bucket and radix sort.\\nOnce a ProxmapSort is complete, ProxmapSearch can be used to find keys in the sorted array in \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   time if the keys were well distributed during the sort.\\nBoth algorithms were invented in the late 1980s by Prof. Thomas A. Standish at the University of California, Irvine.\\n\\nOverview\\nBasic strategy\\nIn general:\\nGiven an array A with n keys:\\n\\nmap a key to a subarray of the destination array A2, by applying the map key function to each array item\\ndetermine how many keys will map to the same subarray, using an array of \\\"hit counts,\\\" H\\ndetermine where each subarray will begin in the destination array so that each bucket is exactly the right size to hold all the keys that will map to it, using an array of \\\"proxmaps,\\\" P\\nfor each key, compute the subarray it will map to, using an array of \\\"locations,\\\" L\\nfor each key, look up its location, place it into that cell of A2; if it collides with a key already in that position, insertion sort the key into place, moving keys greater than this key to the right by one to make a space for this key. Since the subarray is big enough to hold all the keys mapped to it, such movement will never cause the keys to overflow into the following subarray.Simplied version:\\nGiven an array A with n keys\\n\\nInitialize: Create and initialize 2 arrays of n size: hitCount, proxMap, and 2 arrays of A.length: location, and A2.\\nPartition: Using a carefully chosen mapKey function, divide the A2 into subarrays using the keys in A\\nDisperse: Read over A, dropping each key into its bucket in A2; insertion sorting as needed.\\nCollect: Visit the subarrays in order and put all the elements back into the original array, or simply use A2.Note: \\\"keys\\\" may also contain other data, for instance an array of Student objects that contain the key plus a student ID and name. This makes ProxMapSort suitable for organizing groups of objects, not just keys themselves.\\n\\nExample\\nConsider a full array: A[0 to n-1] with n keys. Let i be an index of A. Sort A's keys into array A2 of equal size.\\nThe map key function is defined as mapKey(key) = floor(K).\\n\\nPseudocode\\nHere A is the array to be sorted and the mapKey functions determines the number of subarrays to use. For example, floor(K) will simply assign as many subarrays as there are integers from the data in A. Dividing the key by a constant reduces the number of subarrays; different functions can be used to translate the range of elements in A to subarrays, such as converting the letters A\\u2013Z to 0\\u201325 or returning the first character (0\\u2013255) for sorting strings. Subarrays are sorted as the data comes in, not after all data has been placed into the subarray, as is typical in bucket sorting.\\n\\nProxmap searching\\nProxmapSearch uses the proxMap array generated by a previously done ProxmapSort to find keys in the sorted array A2 in constant time.\\n\\nBasic strategy\\nSort the keys using ProxmapSort, keeping  the MapKey function, and the P and A2 arrays\\nTo search for a key, go to P[MapKey(k)], the start of the subarray that contains the key, if that key is in the data set\\nSequentially search the subarray; if the key  is found, return it (and associated information); if find a value greater than the key, the key is not in the data set\\nComputing P[MapKey(k)] takes \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   time. If a map key that gives a good distribution of keys was used during the sort, each subarray is bounded above by a constant c, so at most c comparisons are needed to find the key or know it is not present; therefore ProxmapSearch is \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n  . If the worst map key was used, all keys are in the same subarray, so ProxmapSearch, in this worst case, will require \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   comparisons.\\n\\nPseudocode\\nfunction mapKey(key) is\\n    return floor(key)\\n\\n    proxMap \\u2190 previously generated proxmap array of size n\\n    A2 \\u2190 previously sorted array of size n\\nfunction proxmap-search(key) is\\n    for i = proxMap[mapKey(key)] to length(array) \\u2212 1 do\\n        if sortedArray[i].key == key then\\n            return sortedArray[i]\\n\\nAnalysis\\nPerformance\\nComputing H, P, and L all take \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n   time. Each is computed with one pass through an array, with constant time spent at each array location.\\n\\nWorst case: MapKey places all items into one subarray, resulting in a standard insertion sort, and time of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  .\\nBest case: MapKey delivers the same small number of items to each subarray in an order where the best case of insertion sort occurs. Each insertion sort is \\n  \\n    \\n      \\n        O\\n        (\\n        c\\n        )\\n      \\n    \\n    {\\\\displaystyle O(c)}\\n  , c the size of the subarrays; there are p subarrays thus p * c = n, so the insertion phase take O(n); thus, ProxmapSort is \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .\\nAverage case: Each subarray is at most size c, a constant; insertion sort for each subarray is then O(c^2) at worst \\u2013 a constant. (The actual time can be much better, since c items are not sorted until the last item is placed in the bucket). Total time is the number of buckets, (n/c), times \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          c\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(c^{2})}\\n   = \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n)}\\n  .Having a good MapKey function is imperative for avoiding the worst case. We must know something about the distribution of the data to come up with a good key.\\n\\nOptimizations\\nSave time: Save the MapKey(i) values so they don't have to be recomputed (as they are in the code above)\\nSave space: The proxMaps can be stored in the hitCount array, as the hit counts are not needed once the proxmap is computed; the data can be sorted back into A, instead of using A2, if one takes care to note which A values have been sorted so far, and which not.JavaScript code implementation:\\n\\nComparison with other sorting algorithms\\nSince ProxmapSort is not a comparison sort, the \\u03a9(n log n) lower bound is inapplicable. Its speed can be attributed to it not being comparison-based and using arrays instead of dynamically allocated objects and pointers that must be followed, such as is done with when using a binary search tree.\\nProxmapSort allows for the use of ProxmapSearch. Despite the O(n) build time, ProxMapSearch makes up for it with its \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   average access time, making it very appealing for large databases. If the data doesn't need to be updated often, the access time may make this function more favorable than other non-comparison sorting based sorts.\\n\\nGeneric bucket sort related to ProxmapSort\\nLike ProxmapSort, bucket sort generally operates on a list of n numeric inputs between zero and some maximum key or value M and divides the value range into n buckets each of size M/n. If each bucket is sorted using insertion sort, ProxmapSort and bucket sort can be shown to run in predicted linear time. However, the performance of this sort degrades with clustering (or too few buckets with too many keys); if many values occur close together, they will all fall into a single bucket and performance will be severely diminished. This behavior also holds for ProxmapSort: if the buckets are too large, its performance will degrade severely.\\n\\nReferences\\nThomas A. Standish. Data Structures in Java. Addison Wesley Longman, 1998. ISBN 0-201-30564-X. Section 10.6, pp. 394\\u2013405.\\nStandish, T. A.; Jacobson, N. (2005). \\\"Using O(n) Proxmap Sort and O(1) Proxmap Search to motivate CS2 students (Part I)\\\". ACM SIGCSE Bulletin. 37 (4). doi:10.1145/1113847.1113874.\\nStandish, T. A.; Jacobson, N. (2006). \\\"Using O(n) Proxmap Sort and O(1) Proxmap Search to motivate CS2 students, Part II\\\". ACM SIGCSE Bulletin. 38 (2). doi:10.1145/1138403.1138427.\\nNorman Jacobson \\\"A Synopsis of ProxmapSort & ProxmapSearch\\\" from Department of Computer Science, Donald Bren School of Information and Computer Sciences, UC Irvine.\\n\\nExternal links\\nhttp://www.cs.uah.edu/~rcoleman/CS221/Sorting/ProxMapSort.html\\nhttps://web.archive.org/web/20120712094020/http://www.valdosta.edu/~sfares/cs330/cs3410.a.sorting.1998.fa.html\\nhttps://web.archive.org/web/20120314220616/http://www.cs.uml.edu/~giam/91.102/Demos/ProxMapSort/ProxMapSort.c\"}, {\"Radix sort\": \"In computer science, radix sort is a non-comparative sorting algorithm. It avoids comparison by creating and distributing elements into buckets according to their radix. For elements with more than one significant digit, this bucketing process is repeated for each digit, while preserving the ordering of the prior step, until all digits have been considered. For this reason, radix sort has also been called bucket sort and digital sort.\\nRadix sort can be applied to data that can be sorted lexicographically, be they integers, words, punch cards, playing cards, or the mail.\\n\\nHistory\\nRadix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines. Radix sorting algorithms came into common use as a way to sort punched cards as early as 1923.The first memory-efficient computer algorithm for this sorting method was developed in 1954 at MIT by Harold H. Seward. Computerized radix sorts had previously been dismissed as impractical because of the perceived need for variable allocation of buckets of unknown size. Seward's innovation was to use a linear scan to determine the required bucket sizes and offsets beforehand, allowing for a single static allocation of auxiliary memory. The linear scan is closely related to Seward's other algorithm \\u2014 counting sort.\\nIn the modern era, radix sorts are most commonly applied to collections of binary strings and integers. It has been shown in some benchmarks to be faster than other more general-purpose sorting algorithms, sometimes 50% to three times faster.\\n\\nDigit order\\nRadix sorts can be implemented to start at either the most significant digit (MSD) or least significant digit (LSD). For example, with 1234, one could start with 1 (MSD) or 4 (LSD).\\nLSD radix sorts typically use the following sorting order: short keys come before longer keys, and then keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, like the sequence [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. LSD sorts are generally stable sorts.\\nMSD radix sorts are most suitable for sorting strings or fixed-length integer representations. A sequence like [b, c, e, d, f, g, ba] would be sorted as [b, ba, c, d, e, f, g]. If lexicographic ordering is used to sort variable-length integers in base 10, then numbers from 1 to 10 would be output as [1, 10, 2, 3, 4, 5, 6, 7, 8, 9], as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key. MSD sorts are not necessarily stable if the original ordering of duplicate keys must always be maintained.\\nOther than the traversal order, MSD and LSD sorts differ in their handling of variable length input.\\nLSD sorts can group by length, radix sort each group, then concatenate the groups in size order. MSD sorts must effectively 'extend' all shorter keys to the size of the largest key and sort them accordingly, which can be more complicated than the grouping required by LSD.\\nHowever, MSD sorts are more amenable to subdivision and recursion. Each bucket created by an MSD step can itself be radix sorted using the next most significant digit, without reference to any other buckets created in the previous step. Once the last digit is reached, concatenating the buckets is all that is required to complete the sort.\\n\\nExamples\\nLeast significant digit\\nInput list:\\n\\n[170, 45, 75, 90, 2, 802, 2, 66]Starting from the rightmost (last) digit, sort the numbers based on that digit:\\n\\n[{170, 90}, {2, 802, 2}, {45, 75}, {66}]Sorting by the next left digit:\\n\\n[{02, 802, 02}, {45}, {66}, {170, 75}, {90}]Notice that an implicit digit 0 is prepended for the two 2s so that 802 maintains its position between them.And finally by the leftmost digit:\\n\\n[{002, 002, 045, 066, 075, 090}, {170}, {802}]Notice that a 0 is prepended to all of the 1- or 2-digit numbers.Each step requires just a single pass over the data, since each item can be placed in its bucket without comparison with any other element.\\nSome radix sort implementations allocate space for buckets by first counting the number of keys that belong in each bucket before moving keys into those buckets. The number of times that each digit occurs is stored in an array.\\nAlthough it is always possible to pre-determine the bucket boundaries using counts, some implementations opt to use dynamic memory allocation instead.\\n\\nMost significant digit, forward recursive\\nInput list, fixed width numeric strings with leading zeros:\\n\\n[170, 045, 075, 025, 002, 024, 802, 066]First digit, with brackets indicating buckets:\\n\\n[{045, 075, 025, 002, 024, 066}, {170}, {802}]Notice that 170 and 802 are already complete because they are all that remain in their buckets, so no further recursion is neededNext digit:\\n\\n[{ {002}, {025, 024}, {045}, {066}, {075} }, 170, 802]Final digit:\\n\\n[ 002, { {024}, {025} }, 045, 066, 075 , 170, 802]All that remains is concatenation:\\n\\n[002, 024, 025, 045, 066, 075, 170, 802]\\n\\nComplexity and performance\\nRadix sort operates in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        w\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nw)}\\n   time, where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of keys, and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   is the key length. LSD variants can achieve a lower bound for \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   of 'average key length' when splitting variable length keys into groups as discussed above.\\nOptimized radix sorts can be very fast when working in a domain that suits them.\\nThey are constrained to lexicographic data, but for many practical applications this is not a limitation. Large key sizes can hinder LSD implementations when the induced number of passes becomes the bottleneck.\\n\\nSpecialized variants\\nIn-place MSD radix sort implementations\\nBinary MSD radix sort, also called binary quicksort, can be implemented in-place by splitting the input array into two bins - the 0s bin and the 1s bin. The 0s bin is grown from the beginning of the array, whereas the 1s bin is grown from the end of the array. The 0s bin boundary is placed before the first array element. The 1s bin boundary is placed after the last array element. The most significant bit of the first array element is examined. If this bit is a 1, then the first element is swapped with the element in front of the 1s bin boundary (the last element of the array), and the 1s bin is grown by one element by decrementing the 1s boundary array index. If this bit is a 0, then the first element remains at its current location, and the 0s bin is grown by one element. The next array element examined is the one in front of the 0s bin boundary (i.e. the first element that is not in the 0s bin or the 1s bin). This process continues until the 0s bin and the 1s bin reach each other. The 0s bin and the 1s bin are then sorted recursively based on the next bit of each array element. Recursive processing continues until the least significant bit has been used for sorting. Handling signed two's complement integers requires treating the most significant bit with the opposite sense, followed by unsigned treatment of the rest of the bits.\\nIn-place MSD binary-radix sort can be extended to larger radix and retain in-place capability. Counting sort is used to determine the size of each bin and their starting index. Swapping is used to place the current element into its bin, followed by expanding the bin boundary. As the array elements are scanned the bins are skipped over and only elements between bins are processed, until the entire array has been processed and all elements end up in their respective bins. The number of bins is the same as the radix used - e.g. 16 bins for 16-radix. Each pass is based on a single digit (e.g. 4-bits per digit in the case of 16-radix), starting from the most significant digit. Each bin is then processed recursively using the next digit, until all digits have been used for sorting.Neither in-place binary-radix sort nor n-bit-radix sort, discussed in paragraphs above, are stable algorithms.\\n\\nStable MSD radix sort implementations\\nMSD radix sort can be implemented as a stable algorithm, but requires the use of a memory buffer of the same size as the input array. This extra memory allows the input buffer to be scanned from the first array element to last, and move the array elements to the destination bins in the same order. Thus, equal elements will be placed in the memory buffer in the same order they were in the input array. The MSD-based algorithm uses the extra memory buffer as the output on the first level of recursion, but swaps the input and output on the next level of recursion, to avoid the overhead of copying the output result back to the input buffer. Each of the bins are recursively processed, as is done for the in-place MSD radix sort. After the sort by the last digit has been completed, the output buffer is checked to see if it is the original input array, and if it's not, then a single copy is performed. If the digit size is chosen such that the key size divided by the digit size is an even number, the copy at the end is avoided.\\n\\nHybrid approaches\\nRadix sort, such as the two-pass method where counting sort is used during the first pass of each level of recursion, has a large constant overhead. Thus, when the bins get small, other sorting algorithms should be used, such as insertion sort. A good implementation of insertion sort is fast for small arrays, stable, in-place, and can significantly speed up radix sort.\\n\\nApplication to parallel computing\\nThis recursive sorting algorithm has particular application to parallel computing, as each of the bins can be sorted independently. In this case, each bin is passed to the next available processor. A single processor would be used at the start (the most significant digit). By the second or third digit, all available processors would likely be engaged. Ideally, as each subdivision is fully sorted, fewer and fewer processors would be utilized. In the worst case, all of the keys will be identical or nearly identical to each other, with the result that there will be little to no advantage to using parallel computing to sort the keys.\\nIn the top level of recursion, opportunity for parallelism is in the counting sort portion of the algorithm. Counting is highly parallel, amenable to the parallel_reduce pattern, and splits the work well across multiple cores until reaching memory bandwidth limit. This portion of the algorithm has data-independent parallelism. Processing each bin in subsequent recursion levels is data-dependent, however. For example, if all keys were of the same value, then there would be only a single bin with any elements in it, and no parallelism would be available. For random inputs all bins would be near equally populated and a large amount of parallelism opportunity would be available.There are faster parallel sorting algorithms available, for example optimal complexity O(log(n)) are those of the Three Hungarians and Richard Cole and Batcher's bitonic merge sort has an algorithmic complexity of O(log2(n)), all of which have a lower algorithmic time complexity to radix sort on a CREW-PRAM. The fastest known PRAM sorts were described in 1991 by David Powers with a parallelized quicksort that can operate in O(log(n)) time on a CRCW-PRAM with n processors by performing partitioning implicitly, as well as a radixsort that operates using the same trick in O(k), where k is the maximum keylength. However, neither the PRAM architecture or a single sequential processor can actually be built in a way that will scale without the number of constant fan-out gate delays per cycle increasing as O(log(n)), so that in effect a pipelined version of Batcher's bitonic mergesort and the O(log(n)) PRAM sorts are all O(log2(n)) in terms of clock cycles, with Powers acknowledging that Batcher's would have lower constant in terms of gate delays than his Parallel quicksort and radix sort, or Cole's merge sort, for a keylength-independent sorting network of O(nlog2(n)).\\n\\nTree-based radix sort\\nRadix sorting can also be accomplished by building a tree (or radix tree) from the input set, and doing a pre-order traversal. This is similar to the relationship between heapsort and the heap data structure. This can be useful for certain data types, see burstsort.\\n\\nSee also\\nIBM 80 series Card Sorters\\nOther distribution sorts\\nKirkpatrick-Reisch sorting\\nPrefix sum\\n\\nReferences\\nExternal links\\n\\nExplanation, Pseudocode and implementation in C and Java\\nHigh Performance Implementation of LSD Radix sort in JavaScript\\nHigh Performance Implementation of LSD & MSD Radix sort in C# with source in GitHub\\nVideo tutorial of MSD Radix Sort\\nDemonstration and comparison of Radix sort with Bubble sort, Merge sort and Quicksort implemented in JavaScript\\nArticle about Radix sorting IEEE floating-point numbers with implementation.\\nFaster Floating Point Sorting and Multiple Histogramming with implementation in C++\\nPointers to radix sort visualizations\\nUSort library contains tuned implementations of radix sort for most numerical C types (C99)\\nDonald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Section 5.2.5: Sorting by Distribution, pp. 168\\u2013179.\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 8.3: Radix sort, pp. 170\\u2013173.\\nBRADSORT v1.50 source code\\nEfficient Trie-Based Sorting of Large Sets of Strings, by Ranjan Sinha and Justin Zobel. This paper describes a method of creating tries of buckets which figuratively burst into sub-tries when the buckets hold more than a predetermined capacity of strings, hence the name, \\\"Burstsort\\\".\\nOpen Data Structures - Java Edition - Section 11.2 - Counting Sort and Radix Sort, Pat Morin\\nOpen Data Structures - C++ Edition - Section 11.2 - Counting Sort and Radix Sort, Pat Morin\"}, {\"Timsort\": \"Timsort is a hybrid, stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. It was implemented by Tim Peters in 2002 for use in the Python programming language. The algorithm finds subsequences of the data that are already ordered (runs) and uses them to sort the remainder more efficiently. This is done by merging runs until certain criteria are fulfilled. Timsort has been Python's standard sorting algorithm since version 2.3. It is also used to sort arrays of non-primitive type in Java SE 7, on the Android platform, in GNU Octave, on V8, Swift, and Rust.It uses techniques from Peter McIlroy's 1993 paper \\\"Optimistic Sorting and Information Theoretic Complexity\\\".\\n\\nOperation\\nTimsort was designed to take advantage of runs of consecutive ordered elements that already exist in most real-world data, natural runs. It iterates over the data collecting elements into runs and simultaneously putting those runs in a stack. Whenever the runs on the top of the stack match a merge criterion, they are merged. This goes on until all data is traversed; then, all runs are merged two at a time and only one sorted run remains. The advantage of merging ordered runs instead of merging fixed size sub-lists (as done by traditional mergesort) is that it decreases the total number of comparisons needed to sort the entire list.\\nEach run has a minimum size, which is based on the size of the input and is defined at the start of the algorithm. If a run is smaller than this minimum run size, insertion sort is used to add more elements to the run until the minimum run size is reached.\\n\\nMerge criteria\\nTimsort is a stable sorting algorithm (order of elements with same key is kept) and strives to perform balanced merges (a merge thus merges runs of similar sizes).\\nIn order to achieve sorting stability, only consecutive runs are merged. Between two non-consecutive runs, there can be an element with the same key inside the runs. Merging those two runs would change the order of equal keys. Example of this situation ([] are ordered runs):  [1 2 2] 1 4 2 [0 1 2]\\nIn pursuit of balanced merges, Timsort considers three runs on the top of the stack, X, Y, Z, and maintains the invariants:\\n\\nIf any of these invariants is violated, Y is merged with the smaller of X or Z and the invariants are checked again. Once the invariants hold, the search for a new run in the data can start. These invariants maintain merges as being approximately balanced while maintaining a compromise between delaying merging for balance, exploiting fresh occurrence of runs in cache memory and making merge decisions relatively simple.\\n\\nMerge space overhead\\nThe original merge sort implementation is not in-place and it has a space overhead of N (data size). In-place merge sort implementations exist, but have a high time overhead. In order to achieve a middle term, Timsort performs a merge sort with a small time overhead and smaller space overhead than N.\\nFirst, Timsort performs a binary search to find the location where the first element of the second run would be inserted in the first ordered run, keeping it ordered. Then, it performs the same algorithm to find the location where the last element of the first run would be inserted in the second ordered run, keeping it ordered. Elements before and after these locations are already in their correct place and do not need to be merged. Then, the smaller of the remaining elements of the two runs is copied into temporary memory, and elements are merged with the larger run into the now free space.  If the first run is smaller, the merge starts at the beginning; if the second is smaller, the merge starts at the end. This optimization reduces the number of required element movements, the running time and the temporary space overhead in the general case.\\nExample: two runs [1, 2, 3, 6, 10] and [4, 5, 7, 9, 12, 14, 17] must be merged. Note that both runs are already sorted individually. The smallest element of the second run is 4 and it would have to be added at the fourth position of the first run in order to preserve its order (assuming that the first position of a run is 1). The largest element of the first run is 10 and it would have to be added at the fifth position of the second run in order to preserve its order. Therefore, [1, 2, 3] and [12, 14, 17] are already in their final positions and the runs in which elements movements are required are [6, 10] and [4, 5, 7, 9]. With this knowledge, we only need to allocate a temporary buffer of size 2 instead of 4.\\n\\nMerge direction\\nMerging can be done in both directions: left-to-right, as in the traditional mergesort, or right-to-left.\\n\\nGalloping mode during merge\\nAn individual merge of runs R1 and R2 keeps the count of consecutive elements selected from a run. When this number reaches the minimum galloping threshold (min_gallop), Timsort considers that it is likely that many consecutive elements may still be selected from that run and switches to the galloping mode. Let us assume that R1 is responsible for triggering it. In this mode, the algorithm performs an exponential search, also known as galloping search, for the next element x of the run R2 in the run R1. This is done in two stages: the first one finds the range (2k \\u2212 1, 2k+1 - 1) where x is. The second stage performs a binary search for the element x in the range found in the first stage. The galloping mode is an attempt to adapt the merge algorithm to the pattern of intervals between elements in runs.\\n\\nGalloping is not always efficient. In some cases galloping mode requires more comparisons than a simple linear search. According to benchmarks done by the developer, galloping is beneficial only when the initial element of one run is not one of the first seven elements of the other run. This implies an initial threshold of 7. To avoid the drawbacks of galloping mode, two actions are taken: (1) When galloping is found to be less efficient than binary search, galloping mode is exited. (2)\\nThe success or failure of galloping is used to adjust min_gallop. If the selected element is from the same array that returned an element previously, min_gallop is reduced by one, thus encouraging the return to galloping mode. Otherwise, the value is incremented by one, thus discouraging a return to galloping mode. In the case of random data, the value of min_gallop becomes so large that galloping mode never recurs.\\n\\nDescending runs\\nIn order to also take advantage of data sorted in descending order, Timsort reverses strictly descending runs when it finds them and adds them to the stack of runs. Since descending runs are later blindly reversed, excluding runs with equal elements maintains the algorithm's stability; i.e., equal elements won't be reversed.\\n\\nMinimum run size\\nBecause merging is most efficient when the number of runs is equal to, or slightly less than, a power of two, and notably less efficient when the number of runs is slightly more than a power of two, Timsort chooses minrun to try to ensure the former condition.Minrun is chosen from the range 32 to 64 inclusive, such that the size of the data, divided by minrun, is equal to, or slightly less than, a power of two.  The final algorithm takes the six most significant bits of the size of the array, adds one if any of the remaining bits are set, and uses that result as the minrun.  This algorithm works for all arrays, including those smaller than 64; for arrays of size 63 or less, this sets minrun equal to the array size and Timsort reduces to an insertion sort.\\n\\nAnalysis\\nIn the worst case, Timsort takes \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        log\\n        \\u2061\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle O(n\\\\log n)}\\n   comparisons to sort an array of n elements. In the best case, which occurs when the input is already sorted, it runs in linear time, meaning that it is an adaptive sorting algorithm.It is superior to Quicksort for sorting object references or pointers because these require expensive memory indirection to access data and perform comparisons and Quicksort's cache coherence benefits are greatly reduced.\\n\\nFormal verification\\nIn 2015, Dutch and German researchers in the EU FP7 ENVISAGE project found a bug in the standard implementation of Timsort. It was fixed in 2015 in Python, Java and Android.\\nSpecifically, the invariants on stacked run sizes ensure a tight upper bound on the maximum size of the required stack.  The implementation preallocated a stack sufficient to sort 264 bytes of input, and avoided further overflow checks.\\nHowever, the guarantee requires the invariants to apply to every group of three consecutive runs, but the implementation only checked it for the top three.  Using the KeY tool for formal verification of Java software, the researchers found that this check is not sufficient, and they were able to find run lengths (and inputs which generated those run lengths) which would result in the invariants being violated deeper in the stack after the top of the stack was merged.As a consequence, for certain inputs the allocated size is not sufficient to hold all unmerged runs. In Java, this generates for those inputs an array-out-of-bound exception. The smallest input that triggers this exception in Java and Android v7 is of size 67108864 (226). (Older Android versions already triggered this exception for certain inputs of size 65536 (216))\\nThe Java implementation was corrected by increasing the size of the preallocated stack based on an updated worst-case analysis.  The article also showed by formal methods how to establish the intended invariant by checking that the four topmost runs in the stack satisfy the two rules above.  This approach was adopted by Python and Android.\\n\\nReferences\\nFurther reading\\nAuger, Nicolas; Nicaud, Cyril; Pivoteau, Carine (2015). \\\"Merge Strategies: from Merge Sort to TimSort\\\". hal-01212839.\\nAuger, Jug\\u00e9, Nicaud, Pivoteau (2018). \\\"On the Worst-Case Complexity of TimSort\\\". ESA 2018.\\nSam Buss, Alexander Knop. \\\"Strategies for Stable Merge Sorting.\\\" SODA 2019.\\n\\nExternal links\\ntimsort.txt \\u2013 original explanation by Tim Peters\"}, {\"Category:String sorting algorithms\": \"\"}, {\"American flag sort\": \"An American flag sort is an efficient, in-place variant of radix sort that distributes items into buckets.  Non-comparative sorting algorithms such as radix sort and American flag sort are typically used to sort large objects such as strings, for which comparison is not a unit-time operation.\\nAmerican flag sort iterates through the bits of the objects, considering several bits of each object at a time.  For each set of bits, American flag sort makes two passes through the array of objects: first to count the number of objects that will fall in each bin, and second to place each object in its bucket.  This works especially well when sorting a byte at a time, using 256 buckets. With some optimizations, it is twice as fast as quicksort for large sets of strings.The name American flag sort comes by analogy with the Dutch national flag problem in the last step: efficiently partition the array into many \\\"stripes\\\".\\n\\nAlgorithm\\nSorting algorithms in general sort a list of objects according to some ordering scheme. In contrast to comparison-based sorting algorithms, such as quicksort, American flag sort  is based on directly comparing the bytes (numerical representation) of the underlying objects. In-place sorting algorithms, including American flag sort, run without allocating a significant amount of memory beyond that used by the original array. This is a significant advantage, both in memory savings and in time saved copying the array.\\nAmerican flag sort works by successively dividing a list of objects into buckets based on the first digit of their base-N representation (the base used is referred to as the radix). When N is 3, each object can be swapped into the correct bucket by using the Dutch national flag algorithm. When N is larger, however, objects cannot be immediately swapped into place, because it is unknown where each bucket should begin and end. American flag sort gets around this problem by making two passes through the array. The first pass counts the number of objects that belong in each of the N buckets. The beginning of each bucket is then computed as the sum of sizes of the preceding buckets. The second pass puts each object into the correct bucket.\\nAmerican flag sort is most efficient with a radix that is a power of 2, because bit-shifting operations can be used instead of expensive exponentiations to compute the value of each digit. When sorting strings using 8- or 7-bit encodings such as ASCII, it is typical to use a radix of 256 or 128, which amounts to sorting character-by-character.\\n\\nPerformance considerations\\nIt is worth noting that for pure English alphabet text, the counts histogram is always sparse. Depending on the hardware, it may be worth clearing the counts in correspondence with completing a bucket (as in the original paper.) Or it may be worth maintaining a max and min active bucket, or a more complex data structure suitable for sparse arrays. It is also important to use a more basic sorting method for very small data sets, except in pathological cases where keys may share very long prefixes.\\nMost critically, this algorithm follows a random permutation, and is thus particularly cache-unfriendly for large datasets. It is a suitable algorithm in conjunction with a k-way merge algorithm. (The original paper was written before cached memory was in common use.)\\n\\nPseudocode\\namerican_flag_sort(Array, Radix)\\n    for each digit D:\\n        # first pass: compute counts\\n        Counts <- zeros(Radix)\\n        for object X in Array:\\n            Counts[digit D of object X in base Radix] += 1\\n        # compute bucket offsets\\n        Offsets <- [ sum(Counts[0..i]) for i in 1..Radix]\\n        # swap objects into place\\n        for object X in Array:\\n            swap X to the bucket starting at Offsets[digit D of X in base Radix]\\n        for each Bucket:\\n            american_flag_sort(Bucket, Radix)\\n\\nSample implementation in Python\\nThis example written in the Python programming language will perform American flag sort for any radix of 2 or greater. Simplicity of exposition is chosen over clever programming, and so the log function is used instead of bit shifting techniques.\\n\\nSee also\\nBucket sort\\nMulti-key quicksort\\nRadixsort\\nDutch national flag problem\\n\\nReferences\\nGeneral\\n This article incorporates public domain material from Paul E. Black. \\\"American flag sort\\\". Dictionary of Algorithms and Data Structures. NIST.\"}, {\"Burstsort\": \"Burstsort and its variants are cache-efficient algorithms for sorting strings.  They are variants of the traditional radix sort but faster for large data sets of common strings, first published in 2003, with some optimizing versions published in later years.Burstsort algorithms use a trie to store prefixes of strings, with growable arrays of pointers as end nodes containing sorted, unique, suffixes (referred to as buckets). Some variants copy the string tails into the buckets. As the buckets grow beyond a predetermined threshold, the buckets are \\\"burst\\\" into tries, giving the sort its name. A more recent variant uses a bucket index with smaller sub-buckets to reduce memory usage. Most implementations delegate to multikey quicksort, an extension of three-way radix quicksort, to sort the contents of the buckets. By dividing the input into buckets with common prefixes, the sorting can be done in a cache-efficient manner.\\nBurstsort was introduced as a sort that is similar to MSD radix sort, but is faster due to being aware of caching and related radixes being stored closer to each other due to specifics of trie structure. It exploits specifics of strings that are usually encountered in real world. And although asymptotically it is the same as radix sort, with time complexity of O(wn) (w \\u2013 word length and n \\u2013 number of strings to be sorted), but due to better memory distribution it tends to be twice as fast on big data sets of strings.  It has been billed as the \\\"fastest known algorithm to sort large sets of strings\\\".\\n\\nReferences\\nA burstsort derivative (C-burstsort), faster than burstsort: Sinha, Ranjan; Zobel, Justin; Ring, David (January 2006). \\\"Cache-Efficient String Sorting Using Copying\\\" (PDF). Journal of Experimental Algorithmics. 11 (1.2): 1.2. CiteSeerX 10.1.1.85.3498. doi:10.1145/1187436.1187439. S2CID 3184411. Archived from the original (PDF) on 2007-10-01. Retrieved 2007-05-31.\\nThe data type used in burstsort: Heinz, Steffen; Zobel, Justin; Williams, Hugh E. (April 2002). \\\"Burst Tries: A Fast, Efficient Data Structure for String Keys\\\" (PDF). ACM Transactions on Information Systems. 20 (2): 192\\u2013223. CiteSeerX 10.1.1.18.3499. doi:10.1145/506309.506312. S2CID 14122377. Archived from the original (PDF) on 2013-12-05. Retrieved 2007-09-25.\\nSinha, Ranjan; Zobel, Justin (2003). \\\"Efficient Trie-Based Sorting of Large Sets of Strings\\\" (PDF). Proceedings of the 26th Australasian Computer Science Conference. Vol. 16. pp. 11\\u201318. CiteSeerX 10.1.1.12.2757. ISBN 978-0-909-92594-9. Archived from the original (PDF) on 2012-02-08. Retrieved 2007-09-25.\\nSinha, Ranjan; Wirth, Anthony (March 2010). \\\"Engineering Burstsort: Towards Fast In-Place String Sorting\\\" (PDF). ACM Journal of Experimental Algorithmics. 15 (2.5): 1\\u201324. doi:10.1145/1671970.1671978. S2CID 16410080.\\n\\nExternal links\\nA burstsort implementation in Java: burstsort4j\\nJudy arrays are a type of copy burstsort: C implementation\"}, {\"Multi-key quicksort\": \"Multi-key quicksort, also known as three-way radix quicksort, is an algorithm for sorting strings. This hybrid of quicksort and radix sort was originally suggested by P. Shackleton, as reported in one of C.A.R. Hoare's seminal papers on quicksort;:\\u200a14\\u200a its modern incarnation was developed by Jon Bentley and Robert Sedgewick in the mid-1990s. The algorithm is designed to exploit the property that in many problems, strings tend to have shared prefixes.\\nOne of the algorithm's uses is the construction of suffix arrays, for which it was one of the fastest algorithms as of 2004.\\n\\nDescription\\nThe three-way radix quicksort algorithm sorts an array of N (pointers to) strings in lexicographic order. It is assumed that all strings are of equal length K; if the strings are of varying length, they must be padded with extra elements that are less than any element in the strings. The pseudocode for the algorithm is then\\nalgorithm sort(a : array of string, d : integer) is\\n    if length(a) \\u2264 1 or d \\u2265 K then\\n        return\\n    p := pivot(a, d)\\n    i, j := partition(a, d, p)   (Note a simultaneous assignment of two variables.)\\n    sort(a[0:i), d)\\n    sort(a[i:j), d + 1)\\n    sort(a[j:length(a)), d)\\n\\nUnlike most string sorting algorithms that look at many bytes in a string to decide if a string is less than, the same as, or equal to some other string; and then turning its focus to some other pair of strings, the multi-key quicksort initially looks at only one byte of every string in the array, byte d, initially the first byte of every string.\\nThe recursive call uses a new value of d and passes a subarray where every string in the subarray has exactly the same initial part -- the characters before character d.\\nThe pivot function must return a single character. Bentley and Sedgewick suggest either picking the median of a[0][d], ..., a[length(a)\\u22121][d] or some random character in that range. The partition function is a variant of the one used in ordinary three-way quicksort: it rearranges a so that all of a[0], ..., a[i\\u22121] have an element at position d that is less than p, a[i], ..., a[j\\u22121] have p at position d, and strings from j onward have a d'th element larger than p. (The original partitioning function suggested by Bentley and Sedgewick may be slow in the case of repeated elements; a Dutch national flag partitioning can be used to alleviate this.)\\nPractical implementations of multi-key quicksort can benefit from the same optimizations typically applied to quicksort: median-of-three pivoting, switching to insertion sort for small arrays, etc.\\n\\nSee also\\nAmerican flag sort \\u2013  another radix sort variant that is fast for string sorting\\nTernary search tree \\u2013  three-way radix quicksort is isomorphic to this data structure in the same way that quicksort is isomorphic to binary search trees\\n\\nNotes\\n\\n\\n== References ==\"}, {\"Radix sort\": \"In computer science, radix sort is a non-comparative sorting algorithm. It avoids comparison by creating and distributing elements into buckets according to their radix. For elements with more than one significant digit, this bucketing process is repeated for each digit, while preserving the ordering of the prior step, until all digits have been considered. For this reason, radix sort has also been called bucket sort and digital sort.\\nRadix sort can be applied to data that can be sorted lexicographically, be they integers, words, punch cards, playing cards, or the mail.\\n\\nHistory\\nRadix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines. Radix sorting algorithms came into common use as a way to sort punched cards as early as 1923.The first memory-efficient computer algorithm for this sorting method was developed in 1954 at MIT by Harold H. Seward. Computerized radix sorts had previously been dismissed as impractical because of the perceived need for variable allocation of buckets of unknown size. Seward's innovation was to use a linear scan to determine the required bucket sizes and offsets beforehand, allowing for a single static allocation of auxiliary memory. The linear scan is closely related to Seward's other algorithm \\u2014 counting sort.\\nIn the modern era, radix sorts are most commonly applied to collections of binary strings and integers. It has been shown in some benchmarks to be faster than other more general-purpose sorting algorithms, sometimes 50% to three times faster.\\n\\nDigit order\\nRadix sorts can be implemented to start at either the most significant digit (MSD) or least significant digit (LSD). For example, with 1234, one could start with 1 (MSD) or 4 (LSD).\\nLSD radix sorts typically use the following sorting order: short keys come before longer keys, and then keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, like the sequence [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]. LSD sorts are generally stable sorts.\\nMSD radix sorts are most suitable for sorting strings or fixed-length integer representations. A sequence like [b, c, e, d, f, g, ba] would be sorted as [b, ba, c, d, e, f, g]. If lexicographic ordering is used to sort variable-length integers in base 10, then numbers from 1 to 10 would be output as [1, 10, 2, 3, 4, 5, 6, 7, 8, 9], as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key. MSD sorts are not necessarily stable if the original ordering of duplicate keys must always be maintained.\\nOther than the traversal order, MSD and LSD sorts differ in their handling of variable length input.\\nLSD sorts can group by length, radix sort each group, then concatenate the groups in size order. MSD sorts must effectively 'extend' all shorter keys to the size of the largest key and sort them accordingly, which can be more complicated than the grouping required by LSD.\\nHowever, MSD sorts are more amenable to subdivision and recursion. Each bucket created by an MSD step can itself be radix sorted using the next most significant digit, without reference to any other buckets created in the previous step. Once the last digit is reached, concatenating the buckets is all that is required to complete the sort.\\n\\nExamples\\nLeast significant digit\\nInput list:\\n\\n[170, 45, 75, 90, 2, 802, 2, 66]Starting from the rightmost (last) digit, sort the numbers based on that digit:\\n\\n[{170, 90}, {2, 802, 2}, {45, 75}, {66}]Sorting by the next left digit:\\n\\n[{02, 802, 02}, {45}, {66}, {170, 75}, {90}]Notice that an implicit digit 0 is prepended for the two 2s so that 802 maintains its position between them.And finally by the leftmost digit:\\n\\n[{002, 002, 045, 066, 075, 090}, {170}, {802}]Notice that a 0 is prepended to all of the 1- or 2-digit numbers.Each step requires just a single pass over the data, since each item can be placed in its bucket without comparison with any other element.\\nSome radix sort implementations allocate space for buckets by first counting the number of keys that belong in each bucket before moving keys into those buckets. The number of times that each digit occurs is stored in an array.\\nAlthough it is always possible to pre-determine the bucket boundaries using counts, some implementations opt to use dynamic memory allocation instead.\\n\\nMost significant digit, forward recursive\\nInput list, fixed width numeric strings with leading zeros:\\n\\n[170, 045, 075, 025, 002, 024, 802, 066]First digit, with brackets indicating buckets:\\n\\n[{045, 075, 025, 002, 024, 066}, {170}, {802}]Notice that 170 and 802 are already complete because they are all that remain in their buckets, so no further recursion is neededNext digit:\\n\\n[{ {002}, {025, 024}, {045}, {066}, {075} }, 170, 802]Final digit:\\n\\n[ 002, { {024}, {025} }, 045, 066, 075 , 170, 802]All that remains is concatenation:\\n\\n[002, 024, 025, 045, 066, 075, 170, 802]\\n\\nComplexity and performance\\nRadix sort operates in \\n  \\n    \\n      \\n        O\\n        (\\n        n\\n        w\\n        )\\n      \\n    \\n    {\\\\displaystyle O(nw)}\\n   time, where \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of keys, and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   is the key length. LSD variants can achieve a lower bound for \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   of 'average key length' when splitting variable length keys into groups as discussed above.\\nOptimized radix sorts can be very fast when working in a domain that suits them.\\nThey are constrained to lexicographic data, but for many practical applications this is not a limitation. Large key sizes can hinder LSD implementations when the induced number of passes becomes the bottleneck.\\n\\nSpecialized variants\\nIn-place MSD radix sort implementations\\nBinary MSD radix sort, also called binary quicksort, can be implemented in-place by splitting the input array into two bins - the 0s bin and the 1s bin. The 0s bin is grown from the beginning of the array, whereas the 1s bin is grown from the end of the array. The 0s bin boundary is placed before the first array element. The 1s bin boundary is placed after the last array element. The most significant bit of the first array element is examined. If this bit is a 1, then the first element is swapped with the element in front of the 1s bin boundary (the last element of the array), and the 1s bin is grown by one element by decrementing the 1s boundary array index. If this bit is a 0, then the first element remains at its current location, and the 0s bin is grown by one element. The next array element examined is the one in front of the 0s bin boundary (i.e. the first element that is not in the 0s bin or the 1s bin). This process continues until the 0s bin and the 1s bin reach each other. The 0s bin and the 1s bin are then sorted recursively based on the next bit of each array element. Recursive processing continues until the least significant bit has been used for sorting. Handling signed two's complement integers requires treating the most significant bit with the opposite sense, followed by unsigned treatment of the rest of the bits.\\nIn-place MSD binary-radix sort can be extended to larger radix and retain in-place capability. Counting sort is used to determine the size of each bin and their starting index. Swapping is used to place the current element into its bin, followed by expanding the bin boundary. As the array elements are scanned the bins are skipped over and only elements between bins are processed, until the entire array has been processed and all elements end up in their respective bins. The number of bins is the same as the radix used - e.g. 16 bins for 16-radix. Each pass is based on a single digit (e.g. 4-bits per digit in the case of 16-radix), starting from the most significant digit. Each bin is then processed recursively using the next digit, until all digits have been used for sorting.Neither in-place binary-radix sort nor n-bit-radix sort, discussed in paragraphs above, are stable algorithms.\\n\\nStable MSD radix sort implementations\\nMSD radix sort can be implemented as a stable algorithm, but requires the use of a memory buffer of the same size as the input array. This extra memory allows the input buffer to be scanned from the first array element to last, and move the array elements to the destination bins in the same order. Thus, equal elements will be placed in the memory buffer in the same order they were in the input array. The MSD-based algorithm uses the extra memory buffer as the output on the first level of recursion, but swaps the input and output on the next level of recursion, to avoid the overhead of copying the output result back to the input buffer. Each of the bins are recursively processed, as is done for the in-place MSD radix sort. After the sort by the last digit has been completed, the output buffer is checked to see if it is the original input array, and if it's not, then a single copy is performed. If the digit size is chosen such that the key size divided by the digit size is an even number, the copy at the end is avoided.\\n\\nHybrid approaches\\nRadix sort, such as the two-pass method where counting sort is used during the first pass of each level of recursion, has a large constant overhead. Thus, when the bins get small, other sorting algorithms should be used, such as insertion sort. A good implementation of insertion sort is fast for small arrays, stable, in-place, and can significantly speed up radix sort.\\n\\nApplication to parallel computing\\nThis recursive sorting algorithm has particular application to parallel computing, as each of the bins can be sorted independently. In this case, each bin is passed to the next available processor. A single processor would be used at the start (the most significant digit). By the second or third digit, all available processors would likely be engaged. Ideally, as each subdivision is fully sorted, fewer and fewer processors would be utilized. In the worst case, all of the keys will be identical or nearly identical to each other, with the result that there will be little to no advantage to using parallel computing to sort the keys.\\nIn the top level of recursion, opportunity for parallelism is in the counting sort portion of the algorithm. Counting is highly parallel, amenable to the parallel_reduce pattern, and splits the work well across multiple cores until reaching memory bandwidth limit. This portion of the algorithm has data-independent parallelism. Processing each bin in subsequent recursion levels is data-dependent, however. For example, if all keys were of the same value, then there would be only a single bin with any elements in it, and no parallelism would be available. For random inputs all bins would be near equally populated and a large amount of parallelism opportunity would be available.There are faster parallel sorting algorithms available, for example optimal complexity O(log(n)) are those of the Three Hungarians and Richard Cole and Batcher's bitonic merge sort has an algorithmic complexity of O(log2(n)), all of which have a lower algorithmic time complexity to radix sort on a CREW-PRAM. The fastest known PRAM sorts were described in 1991 by David Powers with a parallelized quicksort that can operate in O(log(n)) time on a CRCW-PRAM with n processors by performing partitioning implicitly, as well as a radixsort that operates using the same trick in O(k), where k is the maximum keylength. However, neither the PRAM architecture or a single sequential processor can actually be built in a way that will scale without the number of constant fan-out gate delays per cycle increasing as O(log(n)), so that in effect a pipelined version of Batcher's bitonic mergesort and the O(log(n)) PRAM sorts are all O(log2(n)) in terms of clock cycles, with Powers acknowledging that Batcher's would have lower constant in terms of gate delays than his Parallel quicksort and radix sort, or Cole's merge sort, for a keylength-independent sorting network of O(nlog2(n)).\\n\\nTree-based radix sort\\nRadix sorting can also be accomplished by building a tree (or radix tree) from the input set, and doing a pre-order traversal. This is similar to the relationship between heapsort and the heap data structure. This can be useful for certain data types, see burstsort.\\n\\nSee also\\nIBM 80 series Card Sorters\\nOther distribution sorts\\nKirkpatrick-Reisch sorting\\nPrefix sum\\n\\nReferences\\nExternal links\\n\\nExplanation, Pseudocode and implementation in C and Java\\nHigh Performance Implementation of LSD Radix sort in JavaScript\\nHigh Performance Implementation of LSD & MSD Radix sort in C# with source in GitHub\\nVideo tutorial of MSD Radix Sort\\nDemonstration and comparison of Radix sort with Bubble sort, Merge sort and Quicksort implemented in JavaScript\\nArticle about Radix sorting IEEE floating-point numbers with implementation.\\nFaster Floating Point Sorting and Multiple Histogramming with implementation in C++\\nPointers to radix sort visualizations\\nUSort library contains tuned implementations of radix sort for most numerical C types (C99)\\nDonald Knuth. The Art of Computer Programming, Volume 3: Sorting and Searching, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89685-0. Section 5.2.5: Sorting by Distribution, pp. 168\\u2013179.\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7. Section 8.3: Radix sort, pp. 170\\u2013173.\\nBRADSORT v1.50 source code\\nEfficient Trie-Based Sorting of Large Sets of Strings, by Ranjan Sinha and Justin Zobel. This paper describes a method of creating tries of buckets which figuratively burst into sub-tries when the buckets hold more than a predetermined capacity of strings, hence the name, \\\"Burstsort\\\".\\nOpen Data Structures - Java Edition - Section 11.2 - Counting Sort and Radix Sort, Pat Morin\\nOpen Data Structures - C++ Edition - Section 11.2 - Counting Sort and Radix Sort, Pat Morin\"}]"